nohup: ignoring input
05/05/2022 18:24:54 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
05/05/2022 18:24:54 - INFO - transformers.configuration_utils -   loading configuration file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/config.json
05/05/2022 18:24:54 - INFO - transformers.configuration_utils -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 9,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

05/05/2022 18:24:54 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 18:24:54 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 18:24:54 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 18:24:54 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 18:24:54 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 18:24:54 - INFO - transformers.modeling_utils -   loading weights file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/pytorch_model.bin
05/05/2022 18:24:57 - INFO - transformers.configuration_utils -   loading configuration file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/config.json
05/05/2022 18:24:57 - INFO - transformers.configuration_utils -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 9,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

05/05/2022 18:24:57 - INFO - transformers.modeling_utils -   loading weights file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/pytorch_model.bin
05/05/2022 18:24:59 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForTokenClassification: ['bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']
05/05/2022 18:25:03 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/home/hs3228/TextBrewer/data/conll', device=device(type='cuda'), do_distill=True, do_eval=True, do_lower_case=False, do_predict=True, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, labels='', learning_rate=0.0001, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='/home/hs3228/TextBrewer/output_model/conll_distill/teacher', model_name_or_path_student=None, model_type='bert', n_gpu=1, no_cuda=False, num_hidden_layers=9, num_train_epochs=20.0, output_dir='/home/hs3228/TextBrewer/output_model/conll_distill/student_layer9', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=32, save_steps=750, seed=42, server_ip='', server_port='', tokenizer_name='', warmup_steps=0.1, weight_decay=0.0)
05/05/2022 18:25:03 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_train_teacher_128
05/05/2022 18:25:04 - INFO - __main__ -   ***** Running training *****
05/05/2022 18:25:04 - INFO - __main__ -     Num examples = 14041
05/05/2022 18:25:04 - INFO - __main__ -     Num Epochs = 20
05/05/2022 18:25:04 - INFO - __main__ -     Instantaneous batch size per GPU = 32
05/05/2022 18:25:04 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32
05/05/2022 18:25:04 - INFO - __main__ -     Gradient Accumulation steps = 1
05/05/2022 18:25:04 - INFO - __main__ -     Total optimization steps = 8780
05/05/2022 18:25:04 - INFO - Distillation -   Training steps per epoch: 439
05/05/2022 18:25:04 - INFO - Distillation -   Checkpoints(step): [0]
05/05/2022 18:25:04 - INFO - Distillation -   Epoch 1
05/05/2022 18:25:04 - INFO - Distillation -   Length of current epoch in forward batch: 439
/home/hs3228/miniconda3/envs/prac/lib/python3.6/site-packages/transformers/optimization.py:146: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
05/05/2022 18:25:10 - INFO - Distillation -   Global step: 21, epoch step:21
05/05/2022 18:25:15 - INFO - Distillation -   Global step: 42, epoch step:42
05/05/2022 18:25:20 - INFO - Distillation -   Global step: 63, epoch step:63
05/05/2022 18:25:26 - INFO - Distillation -   Global step: 84, epoch step:84
05/05/2022 18:25:31 - INFO - Distillation -   Global step: 105, epoch step:105
05/05/2022 18:25:37 - INFO - Distillation -   Global step: 126, epoch step:126
05/05/2022 18:25:42 - INFO - Distillation -   Global step: 147, epoch step:147
05/05/2022 18:25:48 - INFO - Distillation -   Global step: 168, epoch step:168
05/05/2022 18:25:53 - INFO - Distillation -   Global step: 189, epoch step:189
05/05/2022 18:25:58 - INFO - Distillation -   Global step: 210, epoch step:210
05/05/2022 18:26:04 - INFO - Distillation -   Global step: 231, epoch step:231
05/05/2022 18:26:09 - INFO - Distillation -   Global step: 252, epoch step:252
05/05/2022 18:26:15 - INFO - Distillation -   Global step: 273, epoch step:273
05/05/2022 18:26:20 - INFO - Distillation -   Global step: 294, epoch step:294
05/05/2022 18:26:25 - INFO - Distillation -   Global step: 315, epoch step:315
05/05/2022 18:26:31 - INFO - Distillation -   Global step: 336, epoch step:336
05/05/2022 18:26:36 - INFO - Distillation -   Global step: 357, epoch step:357
05/05/2022 18:26:42 - INFO - Distillation -   Global step: 378, epoch step:378
05/05/2022 18:26:47 - INFO - Distillation -   Global step: 399, epoch step:399
05/05/2022 18:26:52 - INFO - Distillation -   Global step: 420, epoch step:420
05/05/2022 18:26:57 - INFO - Distillation -   Saving at global step 439, epoch step 439 epoch 1
05/05/2022 18:26:58 - INFO - Distillation -   Running callback function...
05/05/2022 18:26:58 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 18:26:58 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 18:26:58 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 18:26:58 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 18:26:58 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 18:26:58 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 18:26:58 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 18:26:58 - INFO - __main__ -     Num examples = 3453
05/05/2022 18:26:58 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   1%|▏         | 6/432 [00:00<00:07, 59.60it/s]Evaluating:   3%|▎         | 13/432 [00:00<00:07, 59.83it/s]Evaluating:   4%|▍         | 19/432 [00:00<00:06, 59.61it/s]Evaluating:   6%|▌         | 25/432 [00:00<00:06, 59.39it/s]Evaluating:   7%|▋         | 31/432 [00:00<00:06, 59.27it/s]Evaluating:   9%|▊         | 37/432 [00:00<00:06, 59.13it/s]Evaluating:  10%|▉         | 43/432 [00:00<00:06, 59.14it/s]Evaluating:  11%|█▏        | 49/432 [00:00<00:06, 58.97it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:06, 58.75it/s]Evaluating:  14%|█▍        | 61/432 [00:01<00:06, 58.68it/s]Evaluating:  16%|█▌        | 67/432 [00:01<00:06, 58.46it/s]Evaluating:  17%|█▋        | 73/432 [00:01<00:06, 58.19it/s]Evaluating:  18%|█▊        | 79/432 [00:01<00:06, 57.87it/s]Evaluating:  20%|█▉        | 85/432 [00:01<00:06, 57.76it/s]Evaluating:  21%|██        | 91/432 [00:01<00:05, 57.55it/s]Evaluating:  22%|██▏       | 97/432 [00:01<00:05, 57.43it/s]Evaluating:  24%|██▍       | 103/432 [00:01<00:05, 57.22it/s]Evaluating:  25%|██▌       | 109/432 [00:01<00:05, 57.11it/s]Evaluating:  27%|██▋       | 115/432 [00:01<00:05, 57.05it/s]Evaluating:  28%|██▊       | 121/432 [00:02<00:05, 57.02it/s]Evaluating:  29%|██▉       | 127/432 [00:02<00:05, 56.74it/s]Evaluating:  31%|███       | 133/432 [00:02<00:05, 56.59it/s]Evaluating:  32%|███▏      | 139/432 [00:02<00:05, 56.47it/s]Evaluating:  34%|███▎      | 145/432 [00:02<00:05, 56.42it/s]Evaluating:  35%|███▍      | 151/432 [00:02<00:04, 56.34it/s]Evaluating:  36%|███▋      | 157/432 [00:02<00:04, 56.35it/s]Evaluating:  38%|███▊      | 163/432 [00:02<00:04, 56.30it/s]Evaluating:  39%|███▉      | 169/432 [00:02<00:04, 56.11it/s]Evaluating:  41%|████      | 175/432 [00:03<00:04, 56.08it/s]Evaluating:  42%|████▏     | 181/432 [00:03<00:04, 56.07it/s]Evaluating:  43%|████▎     | 187/432 [00:03<00:04, 55.92it/s]Evaluating:  45%|████▍     | 193/432 [00:03<00:04, 55.72it/s]Evaluating:  46%|████▌     | 199/432 [00:03<00:04, 55.65it/s]Evaluating:  47%|████▋     | 205/432 [00:03<00:04, 55.30it/s]Evaluating:  49%|████▉     | 211/432 [00:03<00:04, 54.90it/s]Evaluating:  50%|█████     | 217/432 [00:03<00:03, 54.61it/s]Evaluating:  52%|█████▏    | 223/432 [00:03<00:03, 54.34it/s]Evaluating:  53%|█████▎    | 229/432 [00:04<00:03, 54.16it/s]Evaluating:  54%|█████▍    | 235/432 [00:04<00:03, 54.04it/s]Evaluating:  56%|█████▌    | 241/432 [00:04<00:03, 53.79it/s]Evaluating:  57%|█████▋    | 247/432 [00:04<00:03, 53.43it/s]Evaluating:  59%|█████▊    | 253/432 [00:04<00:03, 53.13it/s]Evaluating:  60%|█████▉    | 259/432 [00:04<00:03, 52.98it/s]Evaluating:  61%|██████▏   | 265/432 [00:04<00:03, 52.74it/s]Evaluating:  63%|██████▎   | 271/432 [00:04<00:03, 52.70it/s]Evaluating:  64%|██████▍   | 277/432 [00:04<00:02, 52.71it/s]Evaluating:  66%|██████▌   | 283/432 [00:05<00:02, 52.72it/s]Evaluating:  67%|██████▋   | 289/432 [00:05<00:02, 52.61it/s]Evaluating:  68%|██████▊   | 295/432 [00:05<00:02, 52.38it/s]Evaluating:  70%|██████▉   | 301/432 [00:05<00:02, 52.12it/s]Evaluating:  71%|███████   | 307/432 [00:05<00:02, 52.01it/s]Evaluating:  72%|███████▏  | 313/432 [00:05<00:02, 51.83it/s]Evaluating:  74%|███████▍  | 319/432 [00:05<00:02, 51.75it/s]Evaluating:  75%|███████▌  | 325/432 [00:05<00:02, 51.70it/s]Evaluating:  77%|███████▋  | 331/432 [00:05<00:01, 51.50it/s]Evaluating:  78%|███████▊  | 337/432 [00:06<00:01, 50.92it/s]Evaluating:  79%|███████▉  | 343/432 [00:06<00:01, 50.84it/s]Evaluating:  81%|████████  | 349/432 [00:06<00:01, 50.68it/s]Evaluating:  82%|████████▏ | 355/432 [00:06<00:01, 50.60it/s]Evaluating:  84%|████████▎ | 361/432 [00:06<00:01, 50.51it/s]Evaluating:  85%|████████▍ | 367/432 [00:06<00:01, 50.45it/s]Evaluating:  86%|████████▋ | 373/432 [00:06<00:01, 50.32it/s]Evaluating:  88%|████████▊ | 379/432 [00:06<00:01, 50.26it/s]Evaluating:  89%|████████▉ | 385/432 [00:07<00:00, 50.21it/s]Evaluating:  91%|█████████ | 391/432 [00:07<00:00, 50.08it/s]Evaluating:  92%|█████████▏| 397/432 [00:07<00:00, 49.87it/s]Evaluating:  93%|█████████▎| 402/432 [00:07<00:00, 49.63it/s]Evaluating:  94%|█████████▍| 407/432 [00:07<00:00, 49.40it/s]Evaluating:  95%|█████████▌| 412/432 [00:07<00:00, 49.29it/s]Evaluating:  97%|█████████▋| 417/432 [00:07<00:00, 49.25it/s]Evaluating:  98%|█████████▊| 422/432 [00:07<00:00, 49.21it/s]Evaluating:  99%|█████████▉| 427/432 [00:07<00:00, 49.06it/s]Evaluating: 100%|██████████| 432/432 [00:08<00:00, 53.92it/s]
05/05/2022 18:27:07 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 18:27:07 - INFO - __main__ -     f1 = 0.9069971870604782
05/05/2022 18:27:07 - INFO - __main__ -     loss = 0.17822743932217439
05/05/2022 18:27:07 - INFO - __main__ -     precision = 0.9000348918353105
05/05/2022 18:27:07 - INFO - __main__ -     recall = 0.9140680368532955
05/05/2022 18:27:07 - INFO - Distillation -   Epoch 1 finished
05/05/2022 18:27:07 - INFO - Distillation -   Epoch 2
05/05/2022 18:27:07 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 18:27:07 - INFO - Distillation -   Global step: 441, epoch step:2
05/05/2022 18:27:13 - INFO - Distillation -   Global step: 462, epoch step:23
05/05/2022 18:27:18 - INFO - Distillation -   Global step: 483, epoch step:44
05/05/2022 18:27:24 - INFO - Distillation -   Global step: 504, epoch step:65
05/05/2022 18:27:29 - INFO - Distillation -   Global step: 525, epoch step:86
05/05/2022 18:27:35 - INFO - Distillation -   Global step: 546, epoch step:107
05/05/2022 18:27:40 - INFO - Distillation -   Global step: 567, epoch step:128
05/05/2022 18:27:46 - INFO - Distillation -   Global step: 588, epoch step:149
05/05/2022 18:27:51 - INFO - Distillation -   Global step: 609, epoch step:170
05/05/2022 18:27:57 - INFO - Distillation -   Global step: 630, epoch step:191
05/05/2022 18:28:02 - INFO - Distillation -   Global step: 651, epoch step:212
05/05/2022 18:28:07 - INFO - Distillation -   Global step: 672, epoch step:233
05/05/2022 18:28:13 - INFO - Distillation -   Global step: 693, epoch step:254
05/05/2022 18:28:18 - INFO - Distillation -   Global step: 714, epoch step:275
05/05/2022 18:28:24 - INFO - Distillation -   Global step: 735, epoch step:296
05/05/2022 18:28:29 - INFO - Distillation -   Global step: 756, epoch step:317
05/05/2022 18:28:35 - INFO - Distillation -   Global step: 777, epoch step:338
05/05/2022 18:28:40 - INFO - Distillation -   Global step: 798, epoch step:359
05/05/2022 18:28:46 - INFO - Distillation -   Global step: 819, epoch step:380
05/05/2022 18:28:51 - INFO - Distillation -   Global step: 840, epoch step:401
05/05/2022 18:28:57 - INFO - Distillation -   Global step: 861, epoch step:422
05/05/2022 18:29:01 - INFO - Distillation -   Saving at global step 878, epoch step 439 epoch 2
05/05/2022 18:29:02 - INFO - Distillation -   Running callback function...
05/05/2022 18:29:02 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 18:29:02 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 18:29:02 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 18:29:02 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 18:29:02 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 18:29:02 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 18:29:02 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 18:29:02 - INFO - __main__ -     Num examples = 3453
05/05/2022 18:29:02 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   2%|▏         | 7/432 [00:00<00:07, 60.27it/s]Evaluating:   3%|▎         | 14/432 [00:00<00:06, 59.93it/s]Evaluating:   5%|▍         | 20/432 [00:00<00:06, 59.72it/s]Evaluating:   6%|▌         | 26/432 [00:00<00:06, 59.57it/s]Evaluating:   7%|▋         | 32/432 [00:00<00:06, 59.41it/s]Evaluating:   9%|▉         | 38/432 [00:00<00:06, 59.30it/s]Evaluating:  10%|█         | 44/432 [00:00<00:06, 59.03it/s]Evaluating:  12%|█▏        | 50/432 [00:00<00:06, 58.98it/s]Evaluating:  13%|█▎        | 56/432 [00:00<00:06, 58.90it/s]Evaluating:  14%|█▍        | 62/432 [00:01<00:06, 58.32it/s]Evaluating:  16%|█▌        | 68/432 [00:01<00:06, 57.91it/s]Evaluating:  17%|█▋        | 74/432 [00:01<00:06, 57.60it/s]Evaluating:  19%|█▊        | 80/432 [00:01<00:06, 57.62it/s]Evaluating:  20%|█▉        | 86/432 [00:01<00:05, 57.70it/s]Evaluating:  21%|██▏       | 92/432 [00:01<00:05, 57.61it/s]Evaluating:  23%|██▎       | 98/432 [00:01<00:05, 57.53it/s]Evaluating:  24%|██▍       | 104/432 [00:01<00:05, 57.41it/s]Evaluating:  25%|██▌       | 110/432 [00:01<00:05, 57.30it/s]Evaluating:  27%|██▋       | 116/432 [00:01<00:05, 57.16it/s]Evaluating:  28%|██▊       | 122/432 [00:02<00:05, 56.93it/s]Evaluating:  30%|██▉       | 128/432 [00:02<00:05, 56.82it/s]Evaluating:  31%|███       | 134/432 [00:02<00:05, 56.66it/s]Evaluating:  32%|███▏      | 140/432 [00:02<00:05, 56.44it/s]Evaluating:  34%|███▍      | 146/432 [00:02<00:05, 56.12it/s]Evaluating:  35%|███▌      | 152/432 [00:02<00:04, 56.01it/s]Evaluating:  37%|███▋      | 158/432 [00:02<00:04, 56.06it/s]Evaluating:  38%|███▊      | 164/432 [00:02<00:04, 56.12it/s]Evaluating:  39%|███▉      | 170/432 [00:02<00:04, 55.98it/s]Evaluating:  41%|████      | 176/432 [00:03<00:04, 55.59it/s]Evaluating:  42%|████▏     | 182/432 [00:03<00:04, 55.21it/s]Evaluating:  44%|████▎     | 188/432 [00:03<00:04, 54.99it/s]Evaluating:  45%|████▍     | 194/432 [00:03<00:04, 54.84it/s]Evaluating:  46%|████▋     | 200/432 [00:03<00:04, 54.56it/s]Evaluating:  48%|████▊     | 206/432 [00:03<00:04, 54.38it/s]Evaluating:  49%|████▉     | 212/432 [00:03<00:04, 54.20it/s]Evaluating:  50%|█████     | 218/432 [00:03<00:03, 54.10it/s]Evaluating:  52%|█████▏    | 224/432 [00:03<00:03, 53.93it/s]Evaluating:  53%|█████▎    | 230/432 [00:04<00:03, 53.62it/s]Evaluating:  55%|█████▍    | 236/432 [00:04<00:03, 53.56it/s]Evaluating:  56%|█████▌    | 242/432 [00:04<00:03, 53.59it/s]Evaluating:  57%|█████▋    | 248/432 [00:04<00:03, 53.51it/s]Evaluating:  59%|█████▉    | 254/432 [00:04<00:03, 53.29it/s]Evaluating:  60%|██████    | 260/432 [00:04<00:03, 52.97it/s]Evaluating:  62%|██████▏   | 266/432 [00:04<00:03, 52.80it/s]Evaluating:  63%|██████▎   | 272/432 [00:04<00:03, 52.75it/s]Evaluating:  64%|██████▍   | 278/432 [00:04<00:02, 52.63it/s]Evaluating:  66%|██████▌   | 284/432 [00:05<00:02, 52.45it/s]Evaluating:  67%|██████▋   | 290/432 [00:05<00:02, 52.26it/s]Evaluating:  69%|██████▊   | 296/432 [00:05<00:02, 52.06it/s]Evaluating:  70%|██████▉   | 302/432 [00:05<00:02, 51.84it/s]Evaluating:  71%|███████▏  | 308/432 [00:05<00:02, 51.61it/s]Evaluating:  73%|███████▎  | 314/432 [00:05<00:02, 51.61it/s]Evaluating:  74%|███████▍  | 320/432 [00:05<00:02, 51.65it/s]Evaluating:  75%|███████▌  | 326/432 [00:05<00:02, 51.71it/s]Evaluating:  77%|███████▋  | 332/432 [00:06<00:01, 51.45it/s]Evaluating:  78%|███████▊  | 338/432 [00:06<00:01, 51.18it/s]Evaluating:  80%|███████▉  | 344/432 [00:06<00:01, 51.08it/s]Evaluating:  81%|████████  | 350/432 [00:06<00:01, 50.85it/s]Evaluating:  82%|████████▏ | 356/432 [00:06<00:01, 50.56it/s]Evaluating:  84%|████████▍ | 362/432 [00:06<00:01, 50.16it/s]Evaluating:  85%|████████▌ | 368/432 [00:06<00:01, 49.95it/s]Evaluating:  86%|████████▋ | 373/432 [00:06<00:01, 49.87it/s]Evaluating:  88%|████████▊ | 378/432 [00:06<00:01, 49.64it/s]Evaluating:  89%|████████▊ | 383/432 [00:07<00:00, 49.54it/s]Evaluating:  90%|████████▉ | 388/432 [00:07<00:00, 49.53it/s]Evaluating:  91%|█████████ | 393/432 [00:07<00:00, 49.41it/s]Evaluating:  92%|█████████▏| 398/432 [00:07<00:00, 49.34it/s]Evaluating:  93%|█████████▎| 403/432 [00:07<00:00, 49.18it/s]Evaluating:  94%|█████████▍| 408/432 [00:07<00:00, 49.02it/s]Evaluating:  96%|█████████▌| 413/432 [00:07<00:00, 48.78it/s]Evaluating:  97%|█████████▋| 418/432 [00:07<00:00, 48.72it/s]Evaluating:  98%|█████████▊| 423/432 [00:07<00:00, 48.68it/s]Evaluating:  99%|█████████▉| 428/432 [00:07<00:00, 48.60it/s]Evaluating: 100%|██████████| 432/432 [00:08<00:00, 53.68it/s]
05/05/2022 18:29:11 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 18:29:11 - INFO - __main__ -     f1 = 0.8971534326253636
05/05/2022 18:29:11 - INFO - __main__ -     loss = 0.21359379212155874
05/05/2022 18:29:11 - INFO - __main__ -     precision = 0.8925127126073996
05/05/2022 18:29:11 - INFO - __main__ -     recall = 0.9018426647767541
05/05/2022 18:29:11 - INFO - Distillation -   Epoch 2 finished
05/05/2022 18:29:11 - INFO - Distillation -   Epoch 3
05/05/2022 18:29:11 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 18:29:12 - INFO - Distillation -   Global step: 882, epoch step:4
05/05/2022 18:29:17 - INFO - Distillation -   Global step: 903, epoch step:25
05/05/2022 18:29:23 - INFO - Distillation -   Global step: 924, epoch step:46
05/05/2022 18:29:28 - INFO - Distillation -   Global step: 945, epoch step:67
05/05/2022 18:29:34 - INFO - Distillation -   Global step: 966, epoch step:88
05/05/2022 18:29:39 - INFO - Distillation -   Global step: 987, epoch step:109
05/05/2022 18:29:45 - INFO - Distillation -   Global step: 1008, epoch step:130
05/05/2022 18:29:50 - INFO - Distillation -   Global step: 1029, epoch step:151
05/05/2022 18:29:56 - INFO - Distillation -   Global step: 1050, epoch step:172
05/05/2022 18:30:01 - INFO - Distillation -   Global step: 1071, epoch step:193
05/05/2022 18:30:07 - INFO - Distillation -   Global step: 1092, epoch step:214
05/05/2022 18:30:12 - INFO - Distillation -   Global step: 1113, epoch step:235
05/05/2022 18:30:18 - INFO - Distillation -   Global step: 1134, epoch step:256
05/05/2022 18:30:23 - INFO - Distillation -   Global step: 1155, epoch step:277
05/05/2022 18:30:29 - INFO - Distillation -   Global step: 1176, epoch step:298
05/05/2022 18:30:34 - INFO - Distillation -   Global step: 1197, epoch step:319
05/05/2022 18:30:39 - INFO - Distillation -   Global step: 1218, epoch step:340
05/05/2022 18:30:45 - INFO - Distillation -   Global step: 1239, epoch step:361
05/05/2022 18:30:50 - INFO - Distillation -   Global step: 1260, epoch step:382
05/05/2022 18:30:56 - INFO - Distillation -   Global step: 1281, epoch step:403
05/05/2022 18:31:01 - INFO - Distillation -   Global step: 1302, epoch step:424
05/05/2022 18:31:05 - INFO - Distillation -   Saving at global step 1317, epoch step 439 epoch 3
05/05/2022 18:31:06 - INFO - Distillation -   Running callback function...
05/05/2022 18:31:06 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 18:31:06 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 18:31:06 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 18:31:06 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 18:31:06 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 18:31:06 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 18:31:06 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 18:31:06 - INFO - __main__ -     Num examples = 3453
05/05/2022 18:31:06 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   2%|▏         | 7/432 [00:00<00:07, 60.13it/s]Evaluating:   3%|▎         | 14/432 [00:00<00:06, 59.94it/s]Evaluating:   5%|▍         | 20/432 [00:00<00:06, 59.73it/s]Evaluating:   6%|▌         | 26/432 [00:00<00:06, 59.65it/s]Evaluating:   7%|▋         | 32/432 [00:00<00:06, 59.55it/s]Evaluating:   9%|▉         | 38/432 [00:00<00:06, 59.31it/s]Evaluating:  10%|█         | 44/432 [00:00<00:06, 59.13it/s]Evaluating:  12%|█▏        | 50/432 [00:00<00:06, 58.95it/s]Evaluating:  13%|█▎        | 56/432 [00:00<00:06, 58.78it/s]Evaluating:  14%|█▍        | 62/432 [00:01<00:06, 58.71it/s]Evaluating:  16%|█▌        | 68/432 [00:01<00:06, 58.55it/s]Evaluating:  17%|█▋        | 74/432 [00:01<00:06, 58.40it/s]Evaluating:  19%|█▊        | 80/432 [00:01<00:06, 58.11it/s]Evaluating:  20%|█▉        | 86/432 [00:01<00:05, 57.93it/s]Evaluating:  21%|██▏       | 92/432 [00:01<00:05, 57.66it/s]Evaluating:  23%|██▎       | 98/432 [00:01<00:05, 57.56it/s]Evaluating:  24%|██▍       | 104/432 [00:01<00:05, 57.34it/s]Evaluating:  25%|██▌       | 110/432 [00:01<00:05, 57.25it/s]Evaluating:  27%|██▋       | 116/432 [00:01<00:05, 57.25it/s]Evaluating:  28%|██▊       | 122/432 [00:02<00:05, 56.91it/s]Evaluating:  30%|██▉       | 128/432 [00:02<00:05, 56.67it/s]Evaluating:  31%|███       | 134/432 [00:02<00:05, 56.37it/s]Evaluating:  32%|███▏      | 140/432 [00:02<00:05, 56.22it/s]Evaluating:  34%|███▍      | 146/432 [00:02<00:05, 56.06it/s]Evaluating:  35%|███▌      | 152/432 [00:02<00:05, 55.97it/s]Evaluating:  37%|███▋      | 158/432 [00:02<00:04, 55.73it/s]Evaluating:  38%|███▊      | 164/432 [00:02<00:04, 55.50it/s]Evaluating:  39%|███▉      | 170/432 [00:02<00:04, 55.23it/s]Evaluating:  41%|████      | 176/432 [00:03<00:04, 55.05it/s]Evaluating:  42%|████▏     | 182/432 [00:03<00:04, 54.96it/s]Evaluating:  44%|████▎     | 188/432 [00:03<00:04, 54.72it/s]Evaluating:  45%|████▍     | 194/432 [00:03<00:04, 54.62it/s]Evaluating:  46%|████▋     | 200/432 [00:03<00:04, 54.56it/s]Evaluating:  48%|████▊     | 206/432 [00:03<00:04, 54.32it/s]Evaluating:  49%|████▉     | 212/432 [00:03<00:04, 54.06it/s]Evaluating:  50%|█████     | 218/432 [00:03<00:03, 53.99it/s]Evaluating:  52%|█████▏    | 224/432 [00:03<00:03, 53.85it/s]Evaluating:  53%|█████▎    | 230/432 [00:04<00:03, 53.72it/s]Evaluating:  55%|█████▍    | 236/432 [00:04<00:03, 53.41it/s]Evaluating:  56%|█████▌    | 242/432 [00:04<00:03, 53.28it/s]Evaluating:  57%|█████▋    | 248/432 [00:04<00:03, 53.08it/s]Evaluating:  59%|█████▉    | 254/432 [00:04<00:03, 53.01it/s]Evaluating:  60%|██████    | 260/432 [00:04<00:03, 53.04it/s]Evaluating:  62%|██████▏   | 266/432 [00:04<00:03, 53.00it/s]Evaluating:  63%|██████▎   | 272/432 [00:04<00:03, 52.97it/s]Evaluating:  64%|██████▍   | 278/432 [00:04<00:02, 52.87it/s]Evaluating:  66%|██████▌   | 284/432 [00:05<00:02, 52.62it/s]Evaluating:  67%|██████▋   | 290/432 [00:05<00:02, 52.42it/s]Evaluating:  69%|██████▊   | 296/432 [00:05<00:02, 52.25it/s]Evaluating:  70%|██████▉   | 302/432 [00:05<00:02, 52.09it/s]Evaluating:  71%|███████▏  | 308/432 [00:05<00:02, 51.89it/s]Evaluating:  73%|███████▎  | 314/432 [00:05<00:02, 51.62it/s]Evaluating:  74%|███████▍  | 320/432 [00:05<00:02, 51.37it/s]Evaluating:  75%|███████▌  | 326/432 [00:05<00:02, 51.23it/s]Evaluating:  77%|███████▋  | 332/432 [00:06<00:01, 51.16it/s]Evaluating:  78%|███████▊  | 338/432 [00:06<00:01, 51.10it/s]Evaluating:  80%|███████▉  | 344/432 [00:06<00:01, 51.05it/s]Evaluating:  81%|████████  | 350/432 [00:06<00:01, 50.95it/s]Evaluating:  82%|████████▏ | 356/432 [00:06<00:01, 50.77it/s]Evaluating:  84%|████████▍ | 362/432 [00:06<00:01, 50.57it/s]Evaluating:  85%|████████▌ | 368/432 [00:06<00:01, 50.38it/s]Evaluating:  87%|████████▋ | 374/432 [00:06<00:01, 50.22it/s]Evaluating:  88%|████████▊ | 380/432 [00:06<00:01, 50.08it/s]Evaluating:  89%|████████▉ | 386/432 [00:07<00:00, 49.87it/s]Evaluating:  91%|█████████ | 391/432 [00:07<00:00, 49.76it/s]Evaluating:  92%|█████████▏| 396/432 [00:07<00:00, 49.62it/s]Evaluating:  93%|█████████▎| 401/432 [00:07<00:00, 49.38it/s]Evaluating:  94%|█████████▍| 406/432 [00:07<00:00, 49.34it/s]Evaluating:  95%|█████████▌| 411/432 [00:07<00:00, 49.37it/s]Evaluating:  96%|█████████▋| 416/432 [00:07<00:00, 49.24it/s]Evaluating:  97%|█████████▋| 421/432 [00:07<00:00, 49.18it/s]Evaluating:  99%|█████████▊| 426/432 [00:07<00:00, 48.97it/s]Evaluating: 100%|█████████▉| 431/432 [00:08<00:00, 48.83it/s]Evaluating: 100%|██████████| 432/432 [00:08<00:00, 53.73it/s]
05/05/2022 18:31:15 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 18:31:15 - INFO - __main__ -     f1 = 0.9013064971751412
05/05/2022 18:31:15 - INFO - __main__ -     loss = 0.2123586076520296
05/05/2022 18:31:15 - INFO - __main__ -     precision = 0.8981351161154116
05/05/2022 18:31:15 - INFO - __main__ -     recall = 0.9045003543586109
05/05/2022 18:31:15 - INFO - Distillation -   Epoch 3 finished
05/05/2022 18:31:15 - INFO - Distillation -   Epoch 4
05/05/2022 18:31:15 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 18:31:16 - INFO - Distillation -   Global step: 1323, epoch step:6
05/05/2022 18:31:22 - INFO - Distillation -   Global step: 1344, epoch step:27
05/05/2022 18:31:27 - INFO - Distillation -   Global step: 1365, epoch step:48
05/05/2022 18:31:33 - INFO - Distillation -   Global step: 1386, epoch step:69
05/05/2022 18:31:38 - INFO - Distillation -   Global step: 1407, epoch step:90
05/05/2022 18:31:44 - INFO - Distillation -   Global step: 1428, epoch step:111
05/05/2022 18:31:49 - INFO - Distillation -   Global step: 1449, epoch step:132
05/05/2022 18:31:55 - INFO - Distillation -   Global step: 1470, epoch step:153
05/05/2022 18:32:00 - INFO - Distillation -   Global step: 1491, epoch step:174
05/05/2022 18:32:06 - INFO - Distillation -   Global step: 1512, epoch step:195
05/05/2022 18:32:11 - INFO - Distillation -   Global step: 1533, epoch step:216
05/05/2022 18:32:17 - INFO - Distillation -   Global step: 1554, epoch step:237
05/05/2022 18:32:22 - INFO - Distillation -   Global step: 1575, epoch step:258
05/05/2022 18:32:28 - INFO - Distillation -   Global step: 1596, epoch step:279
05/05/2022 18:32:33 - INFO - Distillation -   Global step: 1617, epoch step:300
05/05/2022 18:32:39 - INFO - Distillation -   Global step: 1638, epoch step:321
05/05/2022 18:32:44 - INFO - Distillation -   Global step: 1659, epoch step:342
05/05/2022 18:32:50 - INFO - Distillation -   Global step: 1680, epoch step:363
05/05/2022 18:32:55 - INFO - Distillation -   Global step: 1701, epoch step:384
05/05/2022 18:33:01 - INFO - Distillation -   Global step: 1722, epoch step:405
05/05/2022 18:33:06 - INFO - Distillation -   Global step: 1743, epoch step:426
05/05/2022 18:33:09 - INFO - Distillation -   Saving at global step 1756, epoch step 439 epoch 4
05/05/2022 18:33:10 - INFO - Distillation -   Running callback function...
05/05/2022 18:33:10 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 18:33:10 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 18:33:10 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 18:33:10 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 18:33:10 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 18:33:10 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 18:33:10 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 18:33:10 - INFO - __main__ -     Num examples = 3453
05/05/2022 18:33:10 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   1%|▏         | 6/432 [00:00<00:07, 59.87it/s]Evaluating:   3%|▎         | 12/432 [00:00<00:07, 59.87it/s]Evaluating:   4%|▍         | 18/432 [00:00<00:06, 59.80it/s]Evaluating:   6%|▌         | 24/432 [00:00<00:06, 59.67it/s]Evaluating:   7%|▋         | 30/432 [00:00<00:06, 59.50it/s]Evaluating:   8%|▊         | 36/432 [00:00<00:06, 59.31it/s]Evaluating:  10%|▉         | 42/432 [00:00<00:06, 59.17it/s]Evaluating:  11%|█         | 48/432 [00:00<00:06, 58.99it/s]Evaluating:  12%|█▎        | 54/432 [00:00<00:06, 58.83it/s]Evaluating:  14%|█▍        | 60/432 [00:01<00:06, 58.60it/s]Evaluating:  15%|█▌        | 66/432 [00:01<00:06, 58.38it/s]Evaluating:  17%|█▋        | 72/432 [00:01<00:06, 58.27it/s]Evaluating:  18%|█▊        | 78/432 [00:01<00:06, 58.23it/s]Evaluating:  19%|█▉        | 84/432 [00:01<00:06, 57.93it/s]Evaluating:  21%|██        | 90/432 [00:01<00:05, 57.79it/s]Evaluating:  22%|██▏       | 96/432 [00:01<00:05, 57.73it/s]Evaluating:  24%|██▎       | 102/432 [00:01<00:05, 57.77it/s]Evaluating:  25%|██▌       | 108/432 [00:01<00:05, 57.44it/s]Evaluating:  26%|██▋       | 114/432 [00:01<00:05, 57.18it/s]Evaluating:  28%|██▊       | 120/432 [00:02<00:05, 56.99it/s]Evaluating:  29%|██▉       | 126/432 [00:02<00:05, 56.66it/s]Evaluating:  31%|███       | 132/432 [00:02<00:05, 56.61it/s]Evaluating:  32%|███▏      | 138/432 [00:02<00:05, 56.59it/s]Evaluating:  33%|███▎      | 144/432 [00:02<00:05, 56.43it/s]Evaluating:  35%|███▍      | 150/432 [00:02<00:05, 56.29it/s]Evaluating:  36%|███▌      | 156/432 [00:02<00:04, 56.21it/s]Evaluating:  38%|███▊      | 162/432 [00:02<00:04, 55.94it/s]Evaluating:  39%|███▉      | 168/432 [00:02<00:04, 55.58it/s]Evaluating:  40%|████      | 174/432 [00:03<00:04, 55.25it/s]Evaluating:  42%|████▏     | 180/432 [00:03<00:04, 55.01it/s]Evaluating:  43%|████▎     | 186/432 [00:03<00:04, 54.80it/s]Evaluating:  44%|████▍     | 192/432 [00:03<00:04, 54.80it/s]Evaluating:  46%|████▌     | 198/432 [00:03<00:04, 54.62it/s]Evaluating:  47%|████▋     | 204/432 [00:03<00:04, 54.45it/s]Evaluating:  49%|████▊     | 210/432 [00:03<00:04, 54.23it/s]Evaluating:  50%|█████     | 216/432 [00:03<00:04, 53.97it/s]Evaluating:  51%|█████▏    | 222/432 [00:03<00:03, 53.72it/s]Evaluating:  53%|█████▎    | 228/432 [00:04<00:03, 53.52it/s]Evaluating:  54%|█████▍    | 234/432 [00:04<00:03, 53.19it/s]Evaluating:  56%|█████▌    | 240/432 [00:04<00:03, 53.07it/s]Evaluating:  57%|█████▋    | 246/432 [00:04<00:03, 52.97it/s]Evaluating:  58%|█████▊    | 252/432 [00:04<00:03, 52.88it/s]Evaluating:  60%|█████▉    | 258/432 [00:04<00:03, 52.78it/s]Evaluating:  61%|██████    | 264/432 [00:04<00:03, 52.67it/s]Evaluating:  62%|██████▎   | 270/432 [00:04<00:03, 52.40it/s]Evaluating:  64%|██████▍   | 276/432 [00:04<00:02, 52.31it/s]Evaluating:  65%|██████▌   | 282/432 [00:05<00:02, 52.26it/s]Evaluating:  67%|██████▋   | 288/432 [00:05<00:02, 52.17it/s]Evaluating:  68%|██████▊   | 294/432 [00:05<00:02, 52.11it/s]Evaluating:  69%|██████▉   | 300/432 [00:05<00:02, 51.76it/s]Evaluating:  71%|███████   | 306/432 [00:05<00:02, 51.51it/s]Evaluating:  72%|███████▏  | 312/432 [00:05<00:02, 51.38it/s]Evaluating:  74%|███████▎  | 318/432 [00:05<00:02, 51.35it/s]Evaluating:  75%|███████▌  | 324/432 [00:05<00:02, 51.22it/s]Evaluating:  76%|███████▋  | 330/432 [00:05<00:01, 51.13it/s]Evaluating:  78%|███████▊  | 336/432 [00:06<00:01, 50.86it/s]Evaluating:  79%|███████▉  | 342/432 [00:06<00:01, 50.61it/s]Evaluating:  81%|████████  | 348/432 [00:06<00:01, 50.49it/s]Evaluating:  82%|████████▏ | 354/432 [00:06<00:01, 50.34it/s]Evaluating:  83%|████████▎ | 360/432 [00:06<00:01, 50.23it/s]Evaluating:  85%|████████▍ | 366/432 [00:06<00:01, 50.17it/s]Evaluating:  86%|████████▌ | 372/432 [00:06<00:01, 50.04it/s]Evaluating:  88%|████████▊ | 378/432 [00:06<00:01, 49.83it/s]Evaluating:  89%|████████▊ | 383/432 [00:07<00:00, 49.69it/s]Evaluating:  90%|████████▉ | 388/432 [00:07<00:00, 49.56it/s]Evaluating:  91%|█████████ | 393/432 [00:07<00:00, 49.42it/s]Evaluating:  92%|█████████▏| 398/432 [00:07<00:00, 49.39it/s]Evaluating:  93%|█████████▎| 403/432 [00:07<00:00, 49.29it/s]Evaluating:  94%|█████████▍| 408/432 [00:07<00:00, 49.22it/s]Evaluating:  96%|█████████▌| 413/432 [00:07<00:00, 49.11it/s]Evaluating:  97%|█████████▋| 418/432 [00:07<00:00, 48.94it/s]Evaluating:  98%|█████████▊| 423/432 [00:07<00:00, 48.70it/s]Evaluating:  99%|█████████▉| 428/432 [00:07<00:00, 48.59it/s]Evaluating: 100%|██████████| 432/432 [00:08<00:00, 53.59it/s]
05/05/2022 18:33:19 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 18:33:19 - INFO - __main__ -     f1 = 0.9005754758742807
05/05/2022 18:33:19 - INFO - __main__ -     loss = 0.22606273172675312
05/05/2022 18:33:19 - INFO - __main__ -     precision = 0.9000176959830118
05/05/2022 18:33:19 - INFO - __main__ -     recall = 0.9011339475549256
05/05/2022 18:33:19 - INFO - Distillation -   Epoch 4 finished
05/05/2022 18:33:19 - INFO - Distillation -   Epoch 5
05/05/2022 18:33:19 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 18:33:21 - INFO - Distillation -   Global step: 1764, epoch step:8
05/05/2022 18:33:27 - INFO - Distillation -   Global step: 1785, epoch step:29
05/05/2022 18:33:32 - INFO - Distillation -   Global step: 1806, epoch step:50
05/05/2022 18:33:38 - INFO - Distillation -   Global step: 1827, epoch step:71
05/05/2022 18:33:43 - INFO - Distillation -   Global step: 1848, epoch step:92
05/05/2022 18:33:49 - INFO - Distillation -   Global step: 1869, epoch step:113
05/05/2022 18:33:54 - INFO - Distillation -   Global step: 1890, epoch step:134
05/05/2022 18:34:00 - INFO - Distillation -   Global step: 1911, epoch step:155
05/05/2022 18:34:05 - INFO - Distillation -   Global step: 1932, epoch step:176
05/05/2022 18:34:11 - INFO - Distillation -   Global step: 1953, epoch step:197
05/05/2022 18:34:16 - INFO - Distillation -   Global step: 1974, epoch step:218
05/05/2022 18:34:21 - INFO - Distillation -   Global step: 1995, epoch step:239
05/05/2022 18:34:27 - INFO - Distillation -   Global step: 2016, epoch step:260
05/05/2022 18:34:32 - INFO - Distillation -   Global step: 2037, epoch step:281
05/05/2022 18:34:38 - INFO - Distillation -   Global step: 2058, epoch step:302
05/05/2022 18:34:43 - INFO - Distillation -   Global step: 2079, epoch step:323
05/05/2022 18:34:49 - INFO - Distillation -   Global step: 2100, epoch step:344
05/05/2022 18:34:54 - INFO - Distillation -   Global step: 2121, epoch step:365
05/05/2022 18:35:00 - INFO - Distillation -   Global step: 2142, epoch step:386
05/05/2022 18:35:05 - INFO - Distillation -   Global step: 2163, epoch step:407
05/05/2022 18:35:11 - INFO - Distillation -   Global step: 2184, epoch step:428
05/05/2022 18:35:14 - INFO - Distillation -   Saving at global step 2195, epoch step 439 epoch 5
05/05/2022 18:35:14 - INFO - Distillation -   Running callback function...
05/05/2022 18:35:14 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 18:35:14 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 18:35:14 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 18:35:14 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 18:35:14 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 18:35:14 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 18:35:15 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 18:35:15 - INFO - __main__ -     Num examples = 3453
05/05/2022 18:35:15 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   1%|▏         | 6/432 [00:00<00:07, 59.89it/s]Evaluating:   3%|▎         | 12/432 [00:00<00:07, 59.82it/s]Evaluating:   4%|▍         | 18/432 [00:00<00:06, 59.51it/s]Evaluating:   6%|▌         | 24/432 [00:00<00:06, 59.39it/s]Evaluating:   7%|▋         | 30/432 [00:00<00:06, 59.24it/s]Evaluating:   8%|▊         | 36/432 [00:00<00:06, 59.04it/s]Evaluating:  10%|▉         | 42/432 [00:00<00:06, 58.88it/s]Evaluating:  11%|█         | 48/432 [00:00<00:06, 58.64it/s]Evaluating:  12%|█▎        | 54/432 [00:00<00:06, 58.53it/s]Evaluating:  14%|█▍        | 60/432 [00:01<00:06, 58.22it/s]Evaluating:  15%|█▌        | 66/432 [00:01<00:06, 57.93it/s]Evaluating:  17%|█▋        | 72/432 [00:01<00:06, 57.82it/s]Evaluating:  18%|█▊        | 78/432 [00:01<00:06, 57.62it/s]Evaluating:  19%|█▉        | 84/432 [00:01<00:06, 57.38it/s]Evaluating:  21%|██        | 90/432 [00:01<00:05, 57.32it/s]Evaluating:  22%|██▏       | 96/432 [00:01<00:05, 57.20it/s]Evaluating:  24%|██▎       | 102/432 [00:01<00:05, 57.03it/s]Evaluating:  25%|██▌       | 108/432 [00:01<00:05, 56.92it/s]Evaluating:  26%|██▋       | 114/432 [00:01<00:05, 56.89it/s]Evaluating:  28%|██▊       | 120/432 [00:02<00:05, 56.65it/s]Evaluating:  29%|██▉       | 126/432 [00:02<00:05, 56.47it/s]Evaluating:  31%|███       | 132/432 [00:02<00:05, 56.23it/s]Evaluating:  32%|███▏      | 138/432 [00:02<00:05, 56.08it/s]Evaluating:  33%|███▎      | 144/432 [00:02<00:05, 55.90it/s]Evaluating:  35%|███▍      | 150/432 [00:02<00:05, 55.70it/s]Evaluating:  36%|███▌      | 156/432 [00:02<00:04, 55.39it/s]Evaluating:  38%|███▊      | 162/432 [00:02<00:04, 55.36it/s]Evaluating:  39%|███▉      | 168/432 [00:02<00:04, 55.23it/s]Evaluating:  40%|████      | 174/432 [00:03<00:04, 55.20it/s]Evaluating:  42%|████▏     | 180/432 [00:03<00:04, 55.06it/s]Evaluating:  43%|████▎     | 186/432 [00:03<00:04, 54.86it/s]Evaluating:  44%|████▍     | 192/432 [00:03<00:04, 54.70it/s]Evaluating:  46%|████▌     | 198/432 [00:03<00:04, 54.57it/s]Evaluating:  47%|████▋     | 204/432 [00:03<00:04, 54.44it/s]Evaluating:  49%|████▊     | 210/432 [00:03<00:04, 54.11it/s]Evaluating:  50%|█████     | 216/432 [00:03<00:04, 53.87it/s]Evaluating:  51%|█████▏    | 222/432 [00:03<00:03, 53.71it/s]Evaluating:  53%|█████▎    | 228/432 [00:04<00:03, 53.51it/s]Evaluating:  54%|█████▍    | 234/432 [00:04<00:03, 53.27it/s]Evaluating:  56%|█████▌    | 240/432 [00:04<00:03, 53.11it/s]Evaluating:  57%|█████▋    | 246/432 [00:04<00:03, 52.99it/s]Evaluating:  58%|█████▊    | 252/432 [00:04<00:03, 52.87it/s]Evaluating:  60%|█████▉    | 258/432 [00:04<00:03, 52.73it/s]Evaluating:  61%|██████    | 264/432 [00:04<00:03, 52.64it/s]Evaluating:  62%|██████▎   | 270/432 [00:04<00:03, 52.39it/s]Evaluating:  64%|██████▍   | 276/432 [00:04<00:02, 52.25it/s]Evaluating:  65%|██████▌   | 282/432 [00:05<00:02, 52.21it/s]Evaluating:  67%|██████▋   | 288/432 [00:05<00:02, 52.16it/s]Evaluating:  68%|██████▊   | 294/432 [00:05<00:02, 51.95it/s]Evaluating:  69%|██████▉   | 300/432 [00:05<00:02, 51.79it/s]Evaluating:  71%|███████   | 306/432 [00:05<00:02, 51.59it/s]Evaluating:  72%|███████▏  | 312/432 [00:05<00:02, 51.37it/s]Evaluating:  74%|███████▎  | 318/432 [00:05<00:02, 51.18it/s]Evaluating:  75%|███████▌  | 324/432 [00:05<00:02, 50.95it/s]Evaluating:  76%|███████▋  | 330/432 [00:06<00:02, 50.82it/s]Evaluating:  78%|███████▊  | 336/432 [00:06<00:01, 50.77it/s]Evaluating:  79%|███████▉  | 342/432 [00:06<00:01, 50.76it/s]Evaluating:  81%|████████  | 348/432 [00:06<00:01, 50.65it/s]Evaluating:  82%|████████▏ | 354/432 [00:06<00:01, 50.38it/s]Evaluating:  83%|████████▎ | 360/432 [00:06<00:01, 50.05it/s]Evaluating:  85%|████████▍ | 366/432 [00:06<00:01, 49.71it/s]Evaluating:  86%|████████▌ | 371/432 [00:06<00:01, 49.47it/s]Evaluating:  87%|████████▋ | 376/432 [00:06<00:01, 49.25it/s]Evaluating:  88%|████████▊ | 381/432 [00:07<00:01, 49.07it/s]Evaluating:  89%|████████▉ | 386/432 [00:07<00:00, 48.94it/s]Evaluating:  91%|█████████ | 391/432 [00:07<00:00, 48.82it/s]Evaluating:  92%|█████████▏| 396/432 [00:07<00:00, 48.74it/s]Evaluating:  93%|█████████▎| 401/432 [00:07<00:00, 48.67it/s]Evaluating:  94%|█████████▍| 406/432 [00:07<00:00, 48.41it/s]Evaluating:  95%|█████████▌| 411/432 [00:07<00:00, 48.31it/s]Evaluating:  96%|█████████▋| 416/432 [00:07<00:00, 48.25it/s]Evaluating:  97%|█████████▋| 421/432 [00:07<00:00, 48.28it/s]Evaluating:  99%|█████████▊| 426/432 [00:07<00:00, 48.15it/s]Evaluating: 100%|█████████▉| 431/432 [00:08<00:00, 48.09it/s]Evaluating: 100%|██████████| 432/432 [00:08<00:00, 53.30it/s]
05/05/2022 18:35:24 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 18:35:24 - INFO - __main__ -     f1 = 0.8999736078120877
05/05/2022 18:35:24 - INFO - __main__ -     loss = 0.22844407965953906
05/05/2022 18:35:24 - INFO - __main__ -     precision = 0.8937620129302813
05/05/2022 18:35:24 - INFO - __main__ -     recall = 0.9062721474131822
05/05/2022 18:35:24 - INFO - Distillation -   Epoch 5 finished
05/05/2022 18:35:24 - INFO - Distillation -   Epoch 6
05/05/2022 18:35:24 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 18:35:26 - INFO - Distillation -   Global step: 2205, epoch step:10
05/05/2022 18:35:32 - INFO - Distillation -   Global step: 2226, epoch step:31
05/05/2022 18:35:37 - INFO - Distillation -   Global step: 2247, epoch step:52
05/05/2022 18:35:43 - INFO - Distillation -   Global step: 2268, epoch step:73
05/05/2022 18:35:48 - INFO - Distillation -   Global step: 2289, epoch step:94
05/05/2022 18:35:54 - INFO - Distillation -   Global step: 2310, epoch step:115
05/05/2022 18:35:59 - INFO - Distillation -   Global step: 2331, epoch step:136
05/05/2022 18:36:05 - INFO - Distillation -   Global step: 2352, epoch step:157
05/05/2022 18:36:10 - INFO - Distillation -   Global step: 2373, epoch step:178
05/05/2022 18:36:15 - INFO - Distillation -   Global step: 2394, epoch step:199
05/05/2022 18:36:21 - INFO - Distillation -   Global step: 2415, epoch step:220
05/05/2022 18:36:26 - INFO - Distillation -   Global step: 2436, epoch step:241
05/05/2022 18:36:32 - INFO - Distillation -   Global step: 2457, epoch step:262
05/05/2022 18:36:37 - INFO - Distillation -   Global step: 2478, epoch step:283
05/05/2022 18:36:43 - INFO - Distillation -   Global step: 2499, epoch step:304
05/05/2022 18:36:48 - INFO - Distillation -   Global step: 2520, epoch step:325
05/05/2022 18:36:54 - INFO - Distillation -   Global step: 2541, epoch step:346
05/05/2022 18:36:59 - INFO - Distillation -   Global step: 2562, epoch step:367
05/05/2022 18:37:05 - INFO - Distillation -   Global step: 2583, epoch step:388
05/05/2022 18:37:10 - INFO - Distillation -   Global step: 2604, epoch step:409
05/05/2022 18:37:16 - INFO - Distillation -   Global step: 2625, epoch step:430
05/05/2022 18:37:18 - INFO - Distillation -   Saving at global step 2634, epoch step 439 epoch 6
05/05/2022 18:37:19 - INFO - Distillation -   Running callback function...
05/05/2022 18:37:19 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 18:37:19 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 18:37:19 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 18:37:19 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 18:37:19 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 18:37:19 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 18:37:19 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 18:37:19 - INFO - __main__ -     Num examples = 3453
05/05/2022 18:37:19 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   2%|▏         | 7/432 [00:00<00:07, 60.05it/s]Evaluating:   3%|▎         | 14/432 [00:00<00:06, 59.96it/s]Evaluating:   5%|▍         | 20/432 [00:00<00:06, 59.87it/s]Evaluating:   6%|▌         | 26/432 [00:00<00:06, 59.64it/s]Evaluating:   7%|▋         | 32/432 [00:00<00:06, 59.37it/s]Evaluating:   9%|▉         | 38/432 [00:00<00:06, 59.25it/s]Evaluating:  10%|█         | 44/432 [00:00<00:06, 59.03it/s]Evaluating:  12%|█▏        | 50/432 [00:00<00:06, 58.72it/s]Evaluating:  13%|█▎        | 56/432 [00:00<00:06, 58.65it/s]Evaluating:  14%|█▍        | 62/432 [00:01<00:06, 58.53it/s]Evaluating:  16%|█▌        | 68/432 [00:01<00:06, 58.42it/s]Evaluating:  17%|█▋        | 74/432 [00:01<00:06, 58.45it/s]Evaluating:  19%|█▊        | 80/432 [00:01<00:06, 58.26it/s]Evaluating:  20%|█▉        | 86/432 [00:01<00:05, 57.79it/s]Evaluating:  21%|██▏       | 92/432 [00:01<00:05, 57.67it/s]Evaluating:  23%|██▎       | 98/432 [00:01<00:05, 57.66it/s]Evaluating:  24%|██▍       | 104/432 [00:01<00:05, 57.50it/s]Evaluating:  25%|██▌       | 110/432 [00:01<00:05, 57.37it/s]Evaluating:  27%|██▋       | 116/432 [00:01<00:05, 57.24it/s]Evaluating:  28%|██▊       | 122/432 [00:02<00:05, 57.14it/s]Evaluating:  30%|██▉       | 128/432 [00:02<00:05, 56.85it/s]Evaluating:  31%|███       | 134/432 [00:02<00:05, 56.60it/s]Evaluating:  32%|███▏      | 140/432 [00:02<00:05, 56.58it/s]Evaluating:  34%|███▍      | 146/432 [00:02<00:05, 56.26it/s]Evaluating:  35%|███▌      | 152/432 [00:02<00:05, 55.99it/s]Evaluating:  37%|███▋      | 158/432 [00:02<00:04, 55.78it/s]Evaluating:  38%|███▊      | 164/432 [00:02<00:04, 55.59it/s]Evaluating:  39%|███▉      | 170/432 [00:02<00:04, 55.47it/s]Evaluating:  41%|████      | 176/432 [00:03<00:04, 55.35it/s]Evaluating:  42%|████▏     | 182/432 [00:03<00:04, 54.94it/s]Evaluating:  44%|████▎     | 188/432 [00:03<00:04, 54.58it/s]Evaluating:  45%|████▍     | 194/432 [00:03<00:04, 54.33it/s]Evaluating:  46%|████▋     | 200/432 [00:03<00:04, 54.23it/s]Evaluating:  48%|████▊     | 206/432 [00:03<00:04, 54.14it/s]Evaluating:  49%|████▉     | 212/432 [00:03<00:04, 53.98it/s]Evaluating:  50%|█████     | 218/432 [00:03<00:03, 53.77it/s]Evaluating:  52%|█████▏    | 224/432 [00:03<00:03, 53.63it/s]Evaluating:  53%|█████▎    | 230/432 [00:04<00:03, 53.50it/s]Evaluating:  55%|█████▍    | 236/432 [00:04<00:03, 53.42it/s]Evaluating:  56%|█████▌    | 242/432 [00:04<00:03, 53.32it/s]Evaluating:  57%|█████▋    | 248/432 [00:04<00:03, 53.17it/s]Evaluating:  59%|█████▉    | 254/432 [00:04<00:03, 53.03it/s]Evaluating:  60%|██████    | 260/432 [00:04<00:03, 53.02it/s]Evaluating:  62%|██████▏   | 266/432 [00:04<00:03, 52.87it/s]Evaluating:  63%|██████▎   | 272/432 [00:04<00:03, 52.74it/s]Evaluating:  64%|██████▍   | 278/432 [00:04<00:02, 52.62it/s]Evaluating:  66%|██████▌   | 284/432 [00:05<00:02, 52.36it/s]Evaluating:  67%|██████▋   | 290/432 [00:05<00:02, 52.14it/s]Evaluating:  69%|██████▊   | 296/432 [00:05<00:02, 51.98it/s]Evaluating:  70%|██████▉   | 302/432 [00:05<00:02, 51.82it/s]Evaluating:  71%|███████▏  | 308/432 [00:05<00:02, 51.64it/s]Evaluating:  73%|███████▎  | 314/432 [00:05<00:02, 51.45it/s]Evaluating:  74%|███████▍  | 320/432 [00:05<00:02, 51.14it/s]Evaluating:  75%|███████▌  | 326/432 [00:05<00:02, 50.97it/s]Evaluating:  77%|███████▋  | 332/432 [00:06<00:01, 50.94it/s]Evaluating:  78%|███████▊  | 338/432 [00:06<00:01, 50.86it/s]Evaluating:  80%|███████▉  | 344/432 [00:06<00:01, 50.77it/s]Evaluating:  81%|████████  | 350/432 [00:06<00:01, 50.69it/s]Evaluating:  82%|████████▏ | 356/432 [00:06<00:01, 50.59it/s]Evaluating:  84%|████████▍ | 362/432 [00:06<00:01, 50.54it/s]Evaluating:  85%|████████▌ | 368/432 [00:06<00:01, 50.42it/s]Evaluating:  87%|████████▋ | 374/432 [00:06<00:01, 50.19it/s]Evaluating:  88%|████████▊ | 380/432 [00:06<00:01, 50.05it/s]Evaluating:  89%|████████▉ | 386/432 [00:07<00:00, 49.86it/s]Evaluating:  91%|█████████ | 391/432 [00:07<00:00, 49.77it/s]Evaluating:  92%|█████████▏| 396/432 [00:07<00:00, 49.50it/s]Evaluating:  93%|█████████▎| 401/432 [00:07<00:00, 49.34it/s]Evaluating:  94%|█████████▍| 406/432 [00:07<00:00, 49.27it/s]Evaluating:  95%|█████████▌| 411/432 [00:07<00:00, 49.18it/s]Evaluating:  96%|█████████▋| 416/432 [00:07<00:00, 49.06it/s]Evaluating:  97%|█████████▋| 421/432 [00:07<00:00, 48.95it/s]Evaluating:  99%|█████████▊| 426/432 [00:07<00:00, 48.80it/s]Evaluating: 100%|█████████▉| 431/432 [00:08<00:00, 48.73it/s]Evaluating: 100%|██████████| 432/432 [00:08<00:00, 53.66it/s]
05/05/2022 18:37:28 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 18:37:28 - INFO - __main__ -     f1 = 0.906186924089513
05/05/2022 18:37:28 - INFO - __main__ -     loss = 0.22500074065751652
05/05/2022 18:37:28 - INFO - __main__ -     precision = 0.8977569118414189
05/05/2022 18:37:28 - INFO - __main__ -     recall = 0.914776754075124
05/05/2022 18:37:28 - INFO - Distillation -   Epoch 6 finished
05/05/2022 18:37:28 - INFO - Distillation -   Epoch 7
05/05/2022 18:37:28 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 18:37:31 - INFO - Distillation -   Global step: 2646, epoch step:12
05/05/2022 18:37:36 - INFO - Distillation -   Global step: 2667, epoch step:33
05/05/2022 18:37:42 - INFO - Distillation -   Global step: 2688, epoch step:54
05/05/2022 18:37:47 - INFO - Distillation -   Global step: 2709, epoch step:75
05/05/2022 18:37:53 - INFO - Distillation -   Global step: 2730, epoch step:96
05/05/2022 18:37:58 - INFO - Distillation -   Global step: 2751, epoch step:117
05/05/2022 18:38:04 - INFO - Distillation -   Global step: 2772, epoch step:138
05/05/2022 18:38:09 - INFO - Distillation -   Global step: 2793, epoch step:159
05/05/2022 18:38:15 - INFO - Distillation -   Global step: 2814, epoch step:180
05/05/2022 18:38:20 - INFO - Distillation -   Global step: 2835, epoch step:201
05/05/2022 18:38:26 - INFO - Distillation -   Global step: 2856, epoch step:222
05/05/2022 18:38:31 - INFO - Distillation -   Global step: 2877, epoch step:243
05/05/2022 18:38:37 - INFO - Distillation -   Global step: 2898, epoch step:264
05/05/2022 18:38:42 - INFO - Distillation -   Global step: 2919, epoch step:285
05/05/2022 18:38:48 - INFO - Distillation -   Global step: 2940, epoch step:306
05/05/2022 18:38:53 - INFO - Distillation -   Global step: 2961, epoch step:327
05/05/2022 18:38:59 - INFO - Distillation -   Global step: 2982, epoch step:348
05/05/2022 18:39:04 - INFO - Distillation -   Global step: 3003, epoch step:369
05/05/2022 18:39:10 - INFO - Distillation -   Global step: 3024, epoch step:390
05/05/2022 18:39:15 - INFO - Distillation -   Global step: 3045, epoch step:411
05/05/2022 18:39:21 - INFO - Distillation -   Global step: 3066, epoch step:432
05/05/2022 18:39:22 - INFO - Distillation -   Saving at global step 3073, epoch step 439 epoch 7
05/05/2022 18:39:23 - INFO - Distillation -   Running callback function...
05/05/2022 18:39:23 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 18:39:23 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 18:39:23 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 18:39:23 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 18:39:23 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 18:39:23 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 18:39:23 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 18:39:23 - INFO - __main__ -     Num examples = 3453
05/05/2022 18:39:23 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   1%|▏         | 6/432 [00:00<00:07, 59.93it/s]Evaluating:   3%|▎         | 12/432 [00:00<00:07, 59.59it/s]Evaluating:   4%|▍         | 18/432 [00:00<00:07, 59.13it/s]Evaluating:   6%|▌         | 24/432 [00:00<00:06, 58.96it/s]Evaluating:   7%|▋         | 30/432 [00:00<00:06, 58.92it/s]Evaluating:   8%|▊         | 36/432 [00:00<00:06, 58.80it/s]Evaluating:  10%|▉         | 42/432 [00:00<00:06, 58.67it/s]Evaluating:  11%|█         | 48/432 [00:00<00:06, 58.64it/s]Evaluating:  12%|█▎        | 54/432 [00:00<00:06, 58.61it/s]Evaluating:  14%|█▍        | 60/432 [00:01<00:06, 58.56it/s]Evaluating:  15%|█▌        | 66/432 [00:01<00:06, 58.48it/s]Evaluating:  17%|█▋        | 72/432 [00:01<00:06, 58.26it/s]Evaluating:  18%|█▊        | 78/432 [00:01<00:06, 58.13it/s]Evaluating:  19%|█▉        | 84/432 [00:01<00:06, 57.92it/s]Evaluating:  21%|██        | 90/432 [00:01<00:05, 57.88it/s]Evaluating:  22%|██▏       | 96/432 [00:01<00:05, 57.82it/s]Evaluating:  24%|██▎       | 102/432 [00:01<00:05, 57.54it/s]Evaluating:  25%|██▌       | 108/432 [00:01<00:05, 57.41it/s]Evaluating:  26%|██▋       | 114/432 [00:01<00:05, 57.37it/s]Evaluating:  28%|██▊       | 120/432 [00:02<00:05, 57.43it/s]Evaluating:  29%|██▉       | 126/432 [00:02<00:05, 57.15it/s]Evaluating:  31%|███       | 132/432 [00:02<00:05, 56.90it/s]Evaluating:  32%|███▏      | 138/432 [00:02<00:05, 56.65it/s]Evaluating:  33%|███▎      | 144/432 [00:02<00:05, 56.38it/s]Evaluating:  35%|███▍      | 150/432 [00:02<00:05, 56.28it/s]Evaluating:  36%|███▌      | 156/432 [00:02<00:04, 56.23it/s]Evaluating:  38%|███▊      | 162/432 [00:02<00:04, 55.98it/s]Evaluating:  39%|███▉      | 168/432 [00:02<00:04, 55.60it/s]Evaluating:  40%|████      | 174/432 [00:03<00:04, 55.44it/s]Evaluating:  42%|████▏     | 180/432 [00:03<00:04, 55.04it/s]Evaluating:  43%|████▎     | 186/432 [00:03<00:04, 54.81it/s]Evaluating:  44%|████▍     | 192/432 [00:03<00:04, 54.76it/s]Evaluating:  46%|████▌     | 198/432 [00:03<00:04, 54.74it/s]Evaluating:  47%|████▋     | 204/432 [00:03<00:04, 54.56it/s]Evaluating:  49%|████▊     | 210/432 [00:03<00:04, 54.37it/s]Evaluating:  50%|█████     | 216/432 [00:03<00:03, 54.11it/s]Evaluating:  51%|█████▏    | 222/432 [00:03<00:03, 53.96it/s]Evaluating:  53%|█████▎    | 228/432 [00:04<00:03, 53.77it/s]Evaluating:  54%|█████▍    | 234/432 [00:04<00:03, 53.52it/s]Evaluating:  56%|█████▌    | 240/432 [00:04<00:03, 53.32it/s]Evaluating:  57%|█████▋    | 246/432 [00:04<00:03, 53.18it/s]Evaluating:  58%|█████▊    | 252/432 [00:04<00:03, 52.92it/s]Evaluating:  60%|█████▉    | 258/432 [00:04<00:03, 52.77it/s]Evaluating:  61%|██████    | 264/432 [00:04<00:03, 52.70it/s]Evaluating:  62%|██████▎   | 270/432 [00:04<00:03, 52.56it/s]Evaluating:  64%|██████▍   | 276/432 [00:04<00:02, 52.40it/s]Evaluating:  65%|██████▌   | 282/432 [00:05<00:02, 52.37it/s]Evaluating:  67%|██████▋   | 288/432 [00:05<00:02, 52.25it/s]Evaluating:  68%|██████▊   | 294/432 [00:05<00:02, 52.02it/s]Evaluating:  69%|██████▉   | 300/432 [00:05<00:02, 51.80it/s]Evaluating:  71%|███████   | 306/432 [00:05<00:02, 51.61it/s]Evaluating:  72%|███████▏  | 312/432 [00:05<00:02, 51.51it/s]Evaluating:  74%|███████▎  | 318/432 [00:05<00:02, 51.43it/s]Evaluating:  75%|███████▌  | 324/432 [00:05<00:02, 51.26it/s]Evaluating:  76%|███████▋  | 330/432 [00:05<00:02, 50.89it/s]Evaluating:  78%|███████▊  | 336/432 [00:06<00:01, 50.56it/s]Evaluating:  79%|███████▉  | 342/432 [00:06<00:01, 50.52it/s]Evaluating:  81%|████████  | 348/432 [00:06<00:01, 50.35it/s]Evaluating:  82%|████████▏ | 354/432 [00:06<00:01, 50.18it/s]Evaluating:  83%|████████▎ | 360/432 [00:06<00:01, 50.04it/s]Evaluating:  85%|████████▍ | 366/432 [00:06<00:01, 49.94it/s]Evaluating:  86%|████████▌ | 371/432 [00:06<00:01, 49.87it/s]Evaluating:  87%|████████▋ | 376/432 [00:06<00:01, 49.62it/s]Evaluating:  88%|████████▊ | 381/432 [00:07<00:01, 49.44it/s]Evaluating:  89%|████████▉ | 386/432 [00:07<00:00, 49.25it/s]Evaluating:  91%|█████████ | 391/432 [00:07<00:00, 49.10it/s]Evaluating:  92%|█████████▏| 396/432 [00:07<00:00, 49.02it/s]Evaluating:  93%|█████████▎| 401/432 [00:07<00:00, 48.89it/s]Evaluating:  94%|█████████▍| 406/432 [00:07<00:00, 48.76it/s]Evaluating:  95%|█████████▌| 411/432 [00:07<00:00, 48.79it/s]Evaluating:  96%|█████████▋| 416/432 [00:07<00:00, 48.96it/s]Evaluating:  97%|█████████▋| 421/432 [00:07<00:00, 48.99it/s]Evaluating:  99%|█████████▊| 426/432 [00:07<00:00, 48.80it/s]Evaluating: 100%|█████████▉| 431/432 [00:08<00:00, 48.67it/s]Evaluating: 100%|██████████| 432/432 [00:08<00:00, 53.57it/s]
05/05/2022 18:39:32 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 18:39:32 - INFO - __main__ -     f1 = 0.8980880547272407
05/05/2022 18:39:32 - INFO - __main__ -     loss = 0.22834125445014888
05/05/2022 18:39:32 - INFO - __main__ -     precision = 0.8891976380687738
05/05/2022 18:39:32 - INFO - __main__ -     recall = 0.9071580439404677
05/05/2022 18:39:32 - INFO - Distillation -   Epoch 7 finished
05/05/2022 18:39:32 - INFO - Distillation -   Epoch 8
05/05/2022 18:39:32 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 18:39:36 - INFO - Distillation -   Global step: 3087, epoch step:14
05/05/2022 18:39:41 - INFO - Distillation -   Global step: 3108, epoch step:35
05/05/2022 18:39:47 - INFO - Distillation -   Global step: 3129, epoch step:56
05/05/2022 18:39:52 - INFO - Distillation -   Global step: 3150, epoch step:77
05/05/2022 18:39:58 - INFO - Distillation -   Global step: 3171, epoch step:98
05/05/2022 18:40:03 - INFO - Distillation -   Global step: 3192, epoch step:119
05/05/2022 18:40:09 - INFO - Distillation -   Global step: 3213, epoch step:140
05/05/2022 18:40:14 - INFO - Distillation -   Global step: 3234, epoch step:161
05/05/2022 18:40:20 - INFO - Distillation -   Global step: 3255, epoch step:182
05/05/2022 18:40:25 - INFO - Distillation -   Global step: 3276, epoch step:203
05/05/2022 18:40:31 - INFO - Distillation -   Global step: 3297, epoch step:224
05/05/2022 18:40:36 - INFO - Distillation -   Global step: 3318, epoch step:245
05/05/2022 18:40:42 - INFO - Distillation -   Global step: 3339, epoch step:266
05/05/2022 18:40:47 - INFO - Distillation -   Global step: 3360, epoch step:287
05/05/2022 18:40:52 - INFO - Distillation -   Global step: 3381, epoch step:308
05/05/2022 18:40:58 - INFO - Distillation -   Global step: 3402, epoch step:329
05/05/2022 18:41:03 - INFO - Distillation -   Global step: 3423, epoch step:350
05/05/2022 18:41:09 - INFO - Distillation -   Global step: 3444, epoch step:371
05/05/2022 18:41:14 - INFO - Distillation -   Global step: 3465, epoch step:392
05/05/2022 18:41:20 - INFO - Distillation -   Global step: 3486, epoch step:413
05/05/2022 18:41:25 - INFO - Distillation -   Global step: 3507, epoch step:434
05/05/2022 18:41:27 - INFO - Distillation -   Saving at global step 3512, epoch step 439 epoch 8
05/05/2022 18:41:27 - INFO - Distillation -   Running callback function...
05/05/2022 18:41:27 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 18:41:27 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 18:41:27 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 18:41:27 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 18:41:27 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 18:41:27 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 18:41:28 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 18:41:28 - INFO - __main__ -     Num examples = 3453
05/05/2022 18:41:28 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   1%|▏         | 6/432 [00:00<00:07, 59.80it/s]Evaluating:   3%|▎         | 12/432 [00:00<00:07, 59.69it/s]Evaluating:   4%|▍         | 18/432 [00:00<00:06, 59.44it/s]Evaluating:   6%|▌         | 24/432 [00:00<00:06, 59.29it/s]Evaluating:   7%|▋         | 30/432 [00:00<00:06, 59.25it/s]Evaluating:   8%|▊         | 36/432 [00:00<00:06, 59.02it/s]Evaluating:  10%|▉         | 42/432 [00:00<00:06, 58.80it/s]Evaluating:  11%|█         | 48/432 [00:00<00:06, 58.47it/s]Evaluating:  12%|█▎        | 54/432 [00:00<00:06, 58.47it/s]Evaluating:  14%|█▍        | 60/432 [00:01<00:06, 58.27it/s]Evaluating:  15%|█▌        | 66/432 [00:01<00:06, 58.32it/s]Evaluating:  17%|█▋        | 72/432 [00:01<00:06, 58.37it/s]Evaluating:  18%|█▊        | 78/432 [00:01<00:06, 58.11it/s]Evaluating:  19%|█▉        | 84/432 [00:01<00:05, 58.10it/s]Evaluating:  21%|██        | 90/432 [00:01<00:05, 58.06it/s]Evaluating:  22%|██▏       | 96/432 [00:01<00:05, 57.67it/s]Evaluating:  24%|██▎       | 102/432 [00:01<00:05, 57.31it/s]Evaluating:  25%|██▌       | 108/432 [00:01<00:05, 56.97it/s]Evaluating:  26%|██▋       | 114/432 [00:01<00:05, 56.70it/s]Evaluating:  28%|██▊       | 120/432 [00:02<00:05, 56.46it/s]Evaluating:  29%|██▉       | 126/432 [00:02<00:05, 56.30it/s]Evaluating:  31%|███       | 132/432 [00:02<00:05, 56.15it/s]Evaluating:  32%|███▏      | 138/432 [00:02<00:05, 55.90it/s]Evaluating:  33%|███▎      | 144/432 [00:02<00:05, 55.75it/s]Evaluating:  35%|███▍      | 150/432 [00:02<00:05, 55.62it/s]Evaluating:  36%|███▌      | 156/432 [00:02<00:04, 55.57it/s]Evaluating:  38%|███▊      | 162/432 [00:02<00:04, 55.43it/s]Evaluating:  39%|███▉      | 168/432 [00:02<00:04, 55.23it/s]Evaluating:  40%|████      | 174/432 [00:03<00:04, 55.03it/s]Evaluating:  42%|████▏     | 180/432 [00:03<00:04, 54.71it/s]Evaluating:  43%|████▎     | 186/432 [00:03<00:04, 54.70it/s]Evaluating:  44%|████▍     | 192/432 [00:03<00:04, 54.52it/s]Evaluating:  46%|████▌     | 198/432 [00:03<00:04, 54.29it/s]Evaluating:  47%|████▋     | 204/432 [00:03<00:04, 54.10it/s]Evaluating:  49%|████▊     | 210/432 [00:03<00:04, 53.84it/s]Evaluating:  50%|█████     | 216/432 [00:03<00:04, 53.65it/s]Evaluating:  51%|█████▏    | 222/432 [00:03<00:03, 53.54it/s]Evaluating:  53%|█████▎    | 228/432 [00:04<00:03, 53.39it/s]Evaluating:  54%|█████▍    | 234/432 [00:04<00:03, 53.19it/s]Evaluating:  56%|█████▌    | 240/432 [00:04<00:03, 53.08it/s]Evaluating:  57%|█████▋    | 246/432 [00:04<00:03, 52.75it/s]Evaluating:  58%|█████▊    | 252/432 [00:04<00:03, 52.48it/s]Evaluating:  60%|█████▉    | 258/432 [00:04<00:03, 52.36it/s]Evaluating:  61%|██████    | 264/432 [00:04<00:03, 52.35it/s]Evaluating:  62%|██████▎   | 270/432 [00:04<00:03, 52.08it/s]Evaluating:  64%|██████▍   | 276/432 [00:04<00:03, 51.78it/s]Evaluating:  65%|██████▌   | 282/432 [00:05<00:02, 51.61it/s]Evaluating:  67%|██████▋   | 288/432 [00:05<00:02, 51.69it/s]Evaluating:  68%|██████▊   | 294/432 [00:05<00:02, 51.74it/s]Evaluating:  69%|██████▉   | 300/432 [00:05<00:02, 51.50it/s]Evaluating:  71%|███████   | 306/432 [00:05<00:02, 51.35it/s]Evaluating:  72%|███████▏  | 312/432 [00:05<00:02, 51.21it/s]Evaluating:  74%|███████▎  | 318/432 [00:05<00:02, 51.06it/s]Evaluating:  75%|███████▌  | 324/432 [00:05<00:02, 50.96it/s]Evaluating:  76%|███████▋  | 330/432 [00:06<00:02, 50.71it/s]Evaluating:  78%|███████▊  | 336/432 [00:06<00:01, 50.49it/s]Evaluating:  79%|███████▉  | 342/432 [00:06<00:01, 50.35it/s]Evaluating:  81%|████████  | 348/432 [00:06<00:01, 50.22it/s]Evaluating:  82%|████████▏ | 354/432 [00:06<00:01, 50.01it/s]Evaluating:  83%|████████▎ | 360/432 [00:06<00:01, 49.88it/s]Evaluating:  84%|████████▍ | 365/432 [00:06<00:01, 49.82it/s]Evaluating:  86%|████████▌ | 370/432 [00:06<00:01, 49.72it/s]Evaluating:  87%|████████▋ | 375/432 [00:06<00:01, 49.59it/s]Evaluating:  88%|████████▊ | 380/432 [00:07<00:01, 49.64it/s]Evaluating:  89%|████████▉ | 385/432 [00:07<00:00, 49.60it/s]Evaluating:  90%|█████████ | 390/432 [00:07<00:00, 49.59it/s]Evaluating:  91%|█████████▏| 395/432 [00:07<00:00, 49.52it/s]Evaluating:  93%|█████████▎| 400/432 [00:07<00:00, 49.33it/s]Evaluating:  94%|█████████▍| 405/432 [00:07<00:00, 49.10it/s]Evaluating:  95%|█████████▍| 410/432 [00:07<00:00, 48.76it/s]Evaluating:  96%|█████████▌| 415/432 [00:07<00:00, 48.43it/s]Evaluating:  97%|█████████▋| 420/432 [00:07<00:00, 48.17it/s]Evaluating:  98%|█████████▊| 425/432 [00:07<00:00, 48.03it/s]Evaluating: 100%|█████████▉| 430/432 [00:08<00:00, 47.95it/s]Evaluating: 100%|██████████| 432/432 [00:08<00:00, 53.27it/s]
05/05/2022 18:41:36 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 18:41:36 - INFO - __main__ -     f1 = 0.9062059238363892
05/05/2022 18:41:36 - INFO - __main__ -     loss = 0.2201177468125374
05/05/2022 18:41:36 - INFO - __main__ -     precision = 0.9017543859649123
05/05/2022 18:41:36 - INFO - __main__ -     recall = 0.9107016300496102
05/05/2022 18:41:36 - INFO - Distillation -   Epoch 8 finished
05/05/2022 18:41:36 - INFO - Distillation -   Epoch 9
05/05/2022 18:41:36 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 18:41:41 - INFO - Distillation -   Global step: 3528, epoch step:16
05/05/2022 18:41:46 - INFO - Distillation -   Global step: 3549, epoch step:37
05/05/2022 18:41:52 - INFO - Distillation -   Global step: 3570, epoch step:58
05/05/2022 18:41:57 - INFO - Distillation -   Global step: 3591, epoch step:79
05/05/2022 18:42:03 - INFO - Distillation -   Global step: 3612, epoch step:100
05/05/2022 18:42:08 - INFO - Distillation -   Global step: 3633, epoch step:121
05/05/2022 18:42:13 - INFO - Distillation -   Global step: 3654, epoch step:142
05/05/2022 18:42:19 - INFO - Distillation -   Global step: 3675, epoch step:163
05/05/2022 18:42:24 - INFO - Distillation -   Global step: 3696, epoch step:184
05/05/2022 18:42:30 - INFO - Distillation -   Global step: 3717, epoch step:205
05/05/2022 18:42:35 - INFO - Distillation -   Global step: 3738, epoch step:226
05/05/2022 18:42:41 - INFO - Distillation -   Global step: 3759, epoch step:247
05/05/2022 18:42:46 - INFO - Distillation -   Global step: 3780, epoch step:268
05/05/2022 18:42:52 - INFO - Distillation -   Global step: 3801, epoch step:289
05/05/2022 18:42:57 - INFO - Distillation -   Global step: 3822, epoch step:310
05/05/2022 18:43:03 - INFO - Distillation -   Global step: 3843, epoch step:331
05/05/2022 18:43:08 - INFO - Distillation -   Global step: 3864, epoch step:352
05/05/2022 18:43:14 - INFO - Distillation -   Global step: 3885, epoch step:373
05/05/2022 18:43:19 - INFO - Distillation -   Global step: 3906, epoch step:394
05/05/2022 18:43:25 - INFO - Distillation -   Global step: 3927, epoch step:415
05/05/2022 18:43:30 - INFO - Distillation -   Global step: 3948, epoch step:436
05/05/2022 18:43:31 - INFO - Distillation -   Saving at global step 3951, epoch step 439 epoch 9
05/05/2022 18:43:32 - INFO - Distillation -   Running callback function...
05/05/2022 18:43:32 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 18:43:32 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 18:43:32 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 18:43:32 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 18:43:32 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 18:43:32 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 18:43:32 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 18:43:32 - INFO - __main__ -     Num examples = 3453
05/05/2022 18:43:32 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   1%|▏         | 6/432 [00:00<00:07, 59.96it/s]Evaluating:   3%|▎         | 12/432 [00:00<00:07, 59.63it/s]Evaluating:   4%|▍         | 18/432 [00:00<00:06, 59.44it/s]Evaluating:   6%|▌         | 24/432 [00:00<00:06, 59.28it/s]Evaluating:   7%|▋         | 30/432 [00:00<00:06, 59.29it/s]Evaluating:   8%|▊         | 36/432 [00:00<00:06, 58.91it/s]Evaluating:  10%|▉         | 42/432 [00:00<00:06, 58.85it/s]Evaluating:  11%|█         | 48/432 [00:00<00:06, 58.94it/s]Evaluating:  12%|█▎        | 54/432 [00:00<00:06, 58.99it/s]Evaluating:  14%|█▍        | 60/432 [00:01<00:06, 58.88it/s]Evaluating:  15%|█▌        | 66/432 [00:01<00:06, 58.82it/s]Evaluating:  17%|█▋        | 72/432 [00:01<00:06, 58.54it/s]Evaluating:  18%|█▊        | 78/432 [00:01<00:06, 58.30it/s]Evaluating:  19%|█▉        | 84/432 [00:01<00:05, 58.05it/s]Evaluating:  21%|██        | 90/432 [00:01<00:05, 57.87it/s]Evaluating:  22%|██▏       | 96/432 [00:01<00:05, 57.79it/s]Evaluating:  24%|██▎       | 102/432 [00:01<00:05, 57.61it/s]Evaluating:  25%|██▌       | 108/432 [00:01<00:05, 57.19it/s]Evaluating:  26%|██▋       | 114/432 [00:01<00:05, 56.98it/s]Evaluating:  28%|██▊       | 120/432 [00:02<00:05, 56.71it/s]Evaluating:  29%|██▉       | 126/432 [00:02<00:05, 56.56it/s]Evaluating:  31%|███       | 132/432 [00:02<00:05, 56.44it/s]Evaluating:  32%|███▏      | 138/432 [00:02<00:05, 56.30it/s]Evaluating:  33%|███▎      | 144/432 [00:02<00:05, 56.12it/s]Evaluating:  35%|███▍      | 150/432 [00:02<00:05, 56.23it/s]Evaluating:  36%|███▌      | 156/432 [00:02<00:04, 56.14it/s]Evaluating:  38%|███▊      | 162/432 [00:02<00:04, 55.89it/s]Evaluating:  39%|███▉      | 168/432 [00:02<00:04, 55.56it/s]Evaluating:  40%|████      | 174/432 [00:03<00:04, 55.32it/s]Evaluating:  42%|████▏     | 180/432 [00:03<00:04, 55.23it/s]Evaluating:  43%|████▎     | 186/432 [00:03<00:04, 54.91it/s]Evaluating:  44%|████▍     | 192/432 [00:03<00:04, 54.67it/s]Evaluating:  46%|████▌     | 198/432 [00:03<00:04, 54.36it/s]Evaluating:  47%|████▋     | 204/432 [00:03<00:04, 54.15it/s]Evaluating:  49%|████▊     | 210/432 [00:03<00:04, 53.97it/s]Evaluating:  50%|█████     | 216/432 [00:03<00:04, 53.80it/s]Evaluating:  51%|█████▏    | 222/432 [00:03<00:03, 53.72it/s]Evaluating:  53%|█████▎    | 228/432 [00:04<00:03, 53.48it/s]Evaluating:  54%|█████▍    | 234/432 [00:04<00:03, 53.33it/s]Evaluating:  56%|█████▌    | 240/432 [00:04<00:03, 53.21it/s]Evaluating:  57%|█████▋    | 246/432 [00:04<00:03, 53.12it/s]Evaluating:  58%|█████▊    | 252/432 [00:04<00:03, 52.94it/s]Evaluating:  60%|█████▉    | 258/432 [00:04<00:03, 52.85it/s]Evaluating:  61%|██████    | 264/432 [00:04<00:03, 52.55it/s]Evaluating:  62%|██████▎   | 270/432 [00:04<00:03, 52.30it/s]Evaluating:  64%|██████▍   | 276/432 [00:04<00:02, 52.23it/s]Evaluating:  65%|██████▌   | 282/432 [00:05<00:02, 52.22it/s]Evaluating:  67%|██████▋   | 288/432 [00:05<00:02, 52.06it/s]Evaluating:  68%|██████▊   | 294/432 [00:05<00:02, 51.68it/s]Evaluating:  69%|██████▉   | 300/432 [00:05<00:02, 51.68it/s]Evaluating:  71%|███████   | 306/432 [00:05<00:02, 51.43it/s]Evaluating:  72%|███████▏  | 312/432 [00:05<00:02, 51.38it/s]Evaluating:  74%|███████▎  | 318/432 [00:05<00:02, 51.44it/s]Evaluating:  75%|███████▌  | 324/432 [00:05<00:02, 51.34it/s]Evaluating:  76%|███████▋  | 330/432 [00:06<00:01, 51.22it/s]Evaluating:  78%|███████▊  | 336/432 [00:06<00:01, 50.78it/s]Evaluating:  79%|███████▉  | 342/432 [00:06<00:01, 50.61it/s]Evaluating:  81%|████████  | 348/432 [00:06<00:01, 50.51it/s]Evaluating:  82%|████████▏ | 354/432 [00:06<00:01, 50.28it/s]Evaluating:  83%|████████▎ | 360/432 [00:06<00:01, 50.16it/s]Evaluating:  85%|████████▍ | 366/432 [00:06<00:01, 50.11it/s]Evaluating:  86%|████████▌ | 372/432 [00:06<00:01, 49.96it/s]Evaluating:  87%|████████▋ | 377/432 [00:06<00:01, 49.89it/s]Evaluating:  88%|████████▊ | 382/432 [00:07<00:01, 49.76it/s]Evaluating:  90%|████████▉ | 387/432 [00:07<00:00, 49.65it/s]Evaluating:  91%|█████████ | 392/432 [00:07<00:00, 49.42it/s]Evaluating:  92%|█████████▏| 397/432 [00:07<00:00, 49.19it/s]Evaluating:  93%|█████████▎| 402/432 [00:07<00:00, 48.84it/s]Evaluating:  94%|█████████▍| 407/432 [00:07<00:00, 48.72it/s]Evaluating:  95%|█████████▌| 412/432 [00:07<00:00, 48.41it/s]Evaluating:  97%|█████████▋| 417/432 [00:07<00:00, 48.22it/s]Evaluating:  98%|█████████▊| 422/432 [00:07<00:00, 48.16it/s]Evaluating:  99%|█████████▉| 427/432 [00:07<00:00, 48.10it/s]Evaluating: 100%|██████████| 432/432 [00:08<00:00, 53.48it/s]
05/05/2022 18:43:41 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 18:43:41 - INFO - __main__ -     f1 = 0.8957528957528959
05/05/2022 18:43:41 - INFO - __main__ -     loss = 0.24484447113278374
05/05/2022 18:43:41 - INFO - __main__ -     precision = 0.8873435326842837
05/05/2022 18:43:41 - INFO - __main__ -     recall = 0.9043231750531537
05/05/2022 18:43:41 - INFO - Distillation -   Epoch 9 finished
05/05/2022 18:43:41 - INFO - Distillation -   Epoch 10
05/05/2022 18:43:41 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 18:43:45 - INFO - Distillation -   Global step: 3969, epoch step:18
05/05/2022 18:43:51 - INFO - Distillation -   Global step: 3990, epoch step:39
05/05/2022 18:43:56 - INFO - Distillation -   Global step: 4011, epoch step:60
05/05/2022 18:44:02 - INFO - Distillation -   Global step: 4032, epoch step:81
05/05/2022 18:44:07 - INFO - Distillation -   Global step: 4053, epoch step:102
05/05/2022 18:44:13 - INFO - Distillation -   Global step: 4074, epoch step:123
05/05/2022 18:44:18 - INFO - Distillation -   Global step: 4095, epoch step:144
05/05/2022 18:44:24 - INFO - Distillation -   Global step: 4116, epoch step:165
05/05/2022 18:44:29 - INFO - Distillation -   Global step: 4137, epoch step:186
05/05/2022 18:44:35 - INFO - Distillation -   Global step: 4158, epoch step:207
05/05/2022 18:44:40 - INFO - Distillation -   Global step: 4179, epoch step:228
05/05/2022 18:44:46 - INFO - Distillation -   Global step: 4200, epoch step:249
05/05/2022 18:44:51 - INFO - Distillation -   Global step: 4221, epoch step:270
05/05/2022 18:44:57 - INFO - Distillation -   Global step: 4242, epoch step:291
05/05/2022 18:45:02 - INFO - Distillation -   Global step: 4263, epoch step:312
05/05/2022 18:45:08 - INFO - Distillation -   Global step: 4284, epoch step:333
05/05/2022 18:45:13 - INFO - Distillation -   Global step: 4305, epoch step:354
05/05/2022 18:45:19 - INFO - Distillation -   Global step: 4326, epoch step:375
05/05/2022 18:45:24 - INFO - Distillation -   Global step: 4347, epoch step:396
05/05/2022 18:45:30 - INFO - Distillation -   Global step: 4368, epoch step:417
05/05/2022 18:45:35 - INFO - Distillation -   Global step: 4389, epoch step:438
05/05/2022 18:45:35 - INFO - Distillation -   Saving at global step 4390, epoch step 439 epoch 10
05/05/2022 18:45:36 - INFO - Distillation -   Running callback function...
05/05/2022 18:45:36 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 18:45:36 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 18:45:36 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 18:45:36 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 18:45:36 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 18:45:36 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 18:45:36 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 18:45:36 - INFO - __main__ -     Num examples = 3453
05/05/2022 18:45:36 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   1%|▏         | 6/432 [00:00<00:07, 59.88it/s]Evaluating:   3%|▎         | 12/432 [00:00<00:07, 59.85it/s]Evaluating:   4%|▍         | 18/432 [00:00<00:06, 59.74it/s]Evaluating:   6%|▌         | 24/432 [00:00<00:06, 59.77it/s]Evaluating:   7%|▋         | 30/432 [00:00<00:06, 59.59it/s]Evaluating:   8%|▊         | 36/432 [00:00<00:06, 59.40it/s]Evaluating:  10%|▉         | 42/432 [00:00<00:06, 59.21it/s]Evaluating:  11%|█         | 48/432 [00:00<00:06, 58.98it/s]Evaluating:  12%|█▎        | 54/432 [00:00<00:06, 58.74it/s]Evaluating:  14%|█▍        | 60/432 [00:01<00:06, 58.57it/s]Evaluating:  15%|█▌        | 66/432 [00:01<00:06, 58.38it/s]Evaluating:  17%|█▋        | 72/432 [00:01<00:06, 58.24it/s]Evaluating:  18%|█▊        | 78/432 [00:01<00:06, 57.97it/s]Evaluating:  19%|█▉        | 84/432 [00:01<00:06, 57.76it/s]Evaluating:  21%|██        | 90/432 [00:01<00:05, 57.51it/s]Evaluating:  22%|██▏       | 96/432 [00:01<00:05, 57.58it/s]Evaluating:  24%|██▎       | 102/432 [00:01<00:05, 57.48it/s]Evaluating:  25%|██▌       | 108/432 [00:01<00:05, 57.23it/s]Evaluating:  26%|██▋       | 114/432 [00:01<00:05, 57.23it/s]Evaluating:  28%|██▊       | 120/432 [00:02<00:05, 57.13it/s]Evaluating:  29%|██▉       | 126/432 [00:02<00:05, 56.92it/s]Evaluating:  31%|███       | 132/432 [00:02<00:05, 56.70it/s]Evaluating:  32%|███▏      | 138/432 [00:02<00:05, 56.49it/s]Evaluating:  33%|███▎      | 144/432 [00:02<00:05, 56.32it/s]Evaluating:  35%|███▍      | 150/432 [00:02<00:05, 56.11it/s]Evaluating:  36%|███▌      | 156/432 [00:02<00:04, 56.05it/s]Evaluating:  38%|███▊      | 162/432 [00:02<00:04, 55.67it/s]Evaluating:  39%|███▉      | 168/432 [00:02<00:04, 55.33it/s]Evaluating:  40%|████      | 174/432 [00:03<00:04, 55.12it/s]Evaluating:  42%|████▏     | 180/432 [00:03<00:04, 54.95it/s]Evaluating:  43%|████▎     | 186/432 [00:03<00:04, 54.70it/s]Evaluating:  44%|████▍     | 192/432 [00:03<00:04, 54.60it/s]Evaluating:  46%|████▌     | 198/432 [00:03<00:04, 54.58it/s]Evaluating:  47%|████▋     | 204/432 [00:03<00:04, 54.46it/s]Evaluating:  49%|████▊     | 210/432 [00:03<00:04, 54.24it/s]Evaluating:  50%|█████     | 216/432 [00:03<00:04, 53.88it/s]Evaluating:  51%|█████▏    | 222/432 [00:03<00:03, 53.70it/s]Evaluating:  53%|█████▎    | 228/432 [00:04<00:03, 53.34it/s]Evaluating:  54%|█████▍    | 234/432 [00:04<00:03, 53.16it/s]Evaluating:  56%|█████▌    | 240/432 [00:04<00:03, 53.00it/s]Evaluating:  57%|█████▋    | 246/432 [00:04<00:03, 52.83it/s]Evaluating:  58%|█████▊    | 252/432 [00:04<00:03, 52.76it/s]Evaluating:  60%|█████▉    | 258/432 [00:04<00:03, 52.69it/s]Evaluating:  61%|██████    | 264/432 [00:04<00:03, 52.62it/s]Evaluating:  62%|██████▎   | 270/432 [00:04<00:03, 52.40it/s]Evaluating:  64%|██████▍   | 276/432 [00:04<00:02, 52.27it/s]Evaluating:  65%|██████▌   | 282/432 [00:05<00:02, 52.32it/s]Evaluating:  67%|██████▋   | 288/432 [00:05<00:02, 52.08it/s]Evaluating:  68%|██████▊   | 294/432 [00:05<00:02, 51.97it/s]Evaluating:  69%|██████▉   | 300/432 [00:05<00:02, 51.90it/s]Evaluating:  71%|███████   | 306/432 [00:05<00:02, 51.68it/s]Evaluating:  72%|███████▏  | 312/432 [00:05<00:02, 51.51it/s]Evaluating:  74%|███████▎  | 318/432 [00:05<00:02, 51.39it/s]Evaluating:  75%|███████▌  | 324/432 [00:05<00:02, 51.12it/s]Evaluating:  76%|███████▋  | 330/432 [00:06<00:02, 50.92it/s]Evaluating:  78%|███████▊  | 336/432 [00:06<00:01, 50.84it/s]Evaluating:  79%|███████▉  | 342/432 [00:06<00:01, 50.64it/s]Evaluating:  81%|████████  | 348/432 [00:06<00:01, 50.57it/s]Evaluating:  82%|████████▏ | 354/432 [00:06<00:01, 50.41it/s]Evaluating:  83%|████████▎ | 360/432 [00:06<00:01, 50.33it/s]Evaluating:  85%|████████▍ | 366/432 [00:06<00:01, 50.24it/s]Evaluating:  86%|████████▌ | 372/432 [00:06<00:01, 50.18it/s]Evaluating:  88%|████████▊ | 378/432 [00:06<00:01, 49.95it/s]Evaluating:  89%|████████▊ | 383/432 [00:07<00:00, 49.81it/s]Evaluating:  90%|████████▉ | 388/432 [00:07<00:00, 49.64it/s]Evaluating:  91%|█████████ | 393/432 [00:07<00:00, 49.49it/s]Evaluating:  92%|█████████▏| 398/432 [00:07<00:00, 49.41it/s]Evaluating:  93%|█████████▎| 403/432 [00:07<00:00, 49.33it/s]Evaluating:  94%|█████████▍| 408/432 [00:07<00:00, 49.17it/s]Evaluating:  96%|█████████▌| 413/432 [00:07<00:00, 48.97it/s]Evaluating:  97%|█████████▋| 418/432 [00:07<00:00, 48.85it/s]Evaluating:  98%|█████████▊| 423/432 [00:07<00:00, 48.68it/s]Evaluating:  99%|█████████▉| 428/432 [00:07<00:00, 48.46it/s]Evaluating: 100%|██████████| 432/432 [00:08<00:00, 53.56it/s]
05/05/2022 18:45:45 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 18:45:45 - INFO - __main__ -     f1 = 0.9065420560747665
05/05/2022 18:45:45 - INFO - __main__ -     loss = 0.21959349890080923
05/05/2022 18:45:45 - INFO - __main__ -     precision = 0.9022464022464023
05/05/2022 18:45:45 - INFO - __main__ -     recall = 0.9108788093550674
05/05/2022 18:45:45 - INFO - Distillation -   Epoch 10 finished
05/05/2022 18:45:45 - INFO - Distillation -   Epoch 11
05/05/2022 18:45:45 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 18:45:50 - INFO - Distillation -   Global step: 4410, epoch step:20
05/05/2022 18:45:56 - INFO - Distillation -   Global step: 4431, epoch step:41
05/05/2022 18:46:01 - INFO - Distillation -   Global step: 4452, epoch step:62
05/05/2022 18:46:07 - INFO - Distillation -   Global step: 4473, epoch step:83
05/05/2022 18:46:12 - INFO - Distillation -   Global step: 4494, epoch step:104
05/05/2022 18:46:18 - INFO - Distillation -   Global step: 4515, epoch step:125
05/05/2022 18:46:23 - INFO - Distillation -   Global step: 4536, epoch step:146
05/05/2022 18:46:29 - INFO - Distillation -   Global step: 4557, epoch step:167
05/05/2022 18:46:34 - INFO - Distillation -   Global step: 4578, epoch step:188
05/05/2022 18:46:40 - INFO - Distillation -   Global step: 4599, epoch step:209
05/05/2022 18:46:45 - INFO - Distillation -   Global step: 4620, epoch step:230
05/05/2022 18:46:51 - INFO - Distillation -   Global step: 4641, epoch step:251
05/05/2022 18:46:56 - INFO - Distillation -   Global step: 4662, epoch step:272
05/05/2022 18:47:02 - INFO - Distillation -   Global step: 4683, epoch step:293
05/05/2022 18:47:07 - INFO - Distillation -   Global step: 4704, epoch step:314
05/05/2022 18:47:13 - INFO - Distillation -   Global step: 4725, epoch step:335
05/05/2022 18:47:18 - INFO - Distillation -   Global step: 4746, epoch step:356
05/05/2022 18:47:23 - INFO - Distillation -   Global step: 4767, epoch step:377
05/05/2022 18:47:29 - INFO - Distillation -   Global step: 4788, epoch step:398
05/05/2022 18:47:34 - INFO - Distillation -   Global step: 4809, epoch step:419
05/05/2022 18:47:40 - INFO - Distillation -   Saving at global step 4829, epoch step 439 epoch 11
05/05/2022 18:47:40 - INFO - Distillation -   Running callback function...
05/05/2022 18:47:40 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 18:47:40 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 18:47:40 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 18:47:40 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 18:47:40 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 18:47:40 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 18:47:41 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 18:47:41 - INFO - __main__ -     Num examples = 3453
05/05/2022 18:47:41 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   1%|▏         | 6/432 [00:00<00:07, 59.58it/s]Evaluating:   3%|▎         | 12/432 [00:00<00:07, 59.56it/s]Evaluating:   4%|▍         | 18/432 [00:00<00:06, 59.53it/s]Evaluating:   6%|▌         | 24/432 [00:00<00:06, 59.49it/s]Evaluating:   7%|▋         | 30/432 [00:00<00:06, 59.24it/s]Evaluating:   8%|▊         | 36/432 [00:00<00:06, 59.22it/s]Evaluating:  10%|▉         | 42/432 [00:00<00:06, 59.09it/s]Evaluating:  11%|█         | 48/432 [00:00<00:06, 58.98it/s]Evaluating:  12%|█▎        | 54/432 [00:00<00:06, 58.97it/s]Evaluating:  14%|█▍        | 60/432 [00:01<00:06, 58.82it/s]Evaluating:  15%|█▌        | 66/432 [00:01<00:06, 58.71it/s]Evaluating:  17%|█▋        | 72/432 [00:01<00:06, 58.39it/s]Evaluating:  18%|█▊        | 78/432 [00:01<00:06, 58.21it/s]Evaluating:  19%|█▉        | 84/432 [00:01<00:05, 58.12it/s]Evaluating:  21%|██        | 90/432 [00:01<00:05, 57.93it/s]Evaluating:  22%|██▏       | 96/432 [00:01<00:05, 57.82it/s]Evaluating:  24%|██▎       | 102/432 [00:01<00:05, 57.47it/s]Evaluating:  25%|██▌       | 108/432 [00:01<00:05, 57.34it/s]Evaluating:  26%|██▋       | 114/432 [00:01<00:05, 57.23it/s]Evaluating:  28%|██▊       | 120/432 [00:02<00:05, 56.98it/s]Evaluating:  29%|██▉       | 126/432 [00:02<00:05, 56.85it/s]Evaluating:  31%|███       | 132/432 [00:02<00:05, 56.77it/s]Evaluating:  32%|███▏      | 138/432 [00:02<00:05, 56.74it/s]Evaluating:  33%|███▎      | 144/432 [00:02<00:05, 56.68it/s]Evaluating:  35%|███▍      | 150/432 [00:02<00:05, 56.37it/s]Evaluating:  36%|███▌      | 156/432 [00:02<00:04, 56.06it/s]Evaluating:  38%|███▊      | 162/432 [00:02<00:04, 55.89it/s]Evaluating:  39%|███▉      | 168/432 [00:02<00:04, 55.60it/s]Evaluating:  40%|████      | 174/432 [00:03<00:04, 55.37it/s]Evaluating:  42%|████▏     | 180/432 [00:03<00:04, 55.20it/s]Evaluating:  43%|████▎     | 186/432 [00:03<00:04, 54.92it/s]Evaluating:  44%|████▍     | 192/432 [00:03<00:04, 54.76it/s]Evaluating:  46%|████▌     | 198/432 [00:03<00:04, 54.64it/s]Evaluating:  47%|████▋     | 204/432 [00:03<00:04, 54.42it/s]Evaluating:  49%|████▊     | 210/432 [00:03<00:04, 54.14it/s]Evaluating:  50%|█████     | 216/432 [00:03<00:04, 53.89it/s]Evaluating:  51%|█████▏    | 222/432 [00:03<00:03, 53.87it/s]Evaluating:  53%|█████▎    | 228/432 [00:04<00:03, 53.78it/s]Evaluating:  54%|█████▍    | 234/432 [00:04<00:03, 53.54it/s]Evaluating:  56%|█████▌    | 240/432 [00:04<00:03, 53.37it/s]Evaluating:  57%|█████▋    | 246/432 [00:04<00:03, 53.11it/s]Evaluating:  58%|█████▊    | 252/432 [00:04<00:03, 52.94it/s]Evaluating:  60%|█████▉    | 258/432 [00:04<00:03, 52.72it/s]Evaluating:  61%|██████    | 264/432 [00:04<00:03, 52.58it/s]Evaluating:  62%|██████▎   | 270/432 [00:04<00:03, 52.44it/s]Evaluating:  64%|██████▍   | 276/432 [00:04<00:02, 52.13it/s]Evaluating:  65%|██████▌   | 282/432 [00:05<00:02, 52.02it/s]Evaluating:  67%|██████▋   | 288/432 [00:05<00:02, 51.93it/s]Evaluating:  68%|██████▊   | 294/432 [00:05<00:02, 51.84it/s]Evaluating:  69%|██████▉   | 300/432 [00:05<00:02, 51.81it/s]Evaluating:  71%|███████   | 306/432 [00:05<00:02, 51.69it/s]Evaluating:  72%|███████▏  | 312/432 [00:05<00:02, 51.48it/s]Evaluating:  74%|███████▎  | 318/432 [00:05<00:02, 51.16it/s]Evaluating:  75%|███████▌  | 324/432 [00:05<00:02, 50.97it/s]Evaluating:  76%|███████▋  | 330/432 [00:05<00:02, 50.95it/s]Evaluating:  78%|███████▊  | 336/432 [00:06<00:01, 50.98it/s]Evaluating:  79%|███████▉  | 342/432 [00:06<00:01, 50.84it/s]Evaluating:  81%|████████  | 348/432 [00:06<00:01, 50.62it/s]Evaluating:  82%|████████▏ | 354/432 [00:06<00:01, 50.41it/s]Evaluating:  83%|████████▎ | 360/432 [00:06<00:01, 50.24it/s]Evaluating:  85%|████████▍ | 366/432 [00:06<00:01, 49.98it/s]Evaluating:  86%|████████▌ | 371/432 [00:06<00:01, 49.74it/s]Evaluating:  87%|████████▋ | 376/432 [00:06<00:01, 49.66it/s]Evaluating:  88%|████████▊ | 381/432 [00:07<00:01, 49.62it/s]Evaluating:  89%|████████▉ | 386/432 [00:07<00:00, 49.51it/s]Evaluating:  91%|█████████ | 391/432 [00:07<00:00, 49.41it/s]Evaluating:  92%|█████████▏| 396/432 [00:07<00:00, 49.17it/s]Evaluating:  93%|█████████▎| 401/432 [00:07<00:00, 49.05it/s]Evaluating:  94%|█████████▍| 406/432 [00:07<00:00, 49.06it/s]Evaluating:  95%|█████████▌| 411/432 [00:07<00:00, 49.04it/s]Evaluating:  96%|█████████▋| 416/432 [00:07<00:00, 48.93it/s]Evaluating:  97%|█████████▋| 421/432 [00:07<00:00, 48.80it/s]Evaluating:  99%|█████████▊| 426/432 [00:07<00:00, 48.76it/s]Evaluating: 100%|█████████▉| 431/432 [00:08<00:00, 48.62it/s]Evaluating: 100%|██████████| 432/432 [00:08<00:00, 53.59it/s]
05/05/2022 18:47:49 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 18:47:49 - INFO - __main__ -     f1 = 0.9082108598081492
05/05/2022 18:47:49 - INFO - __main__ -     loss = 0.20662965606136177
05/05/2022 18:47:49 - INFO - __main__ -     precision = 0.9022556390977443
05/05/2022 18:47:49 - INFO - __main__ -     recall = 0.9142452161587526
05/05/2022 18:47:49 - INFO - Distillation -   Epoch 11 finished
05/05/2022 18:47:49 - INFO - Distillation -   Epoch 12
05/05/2022 18:47:49 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 18:47:50 - INFO - Distillation -   Global step: 4830, epoch step:1
05/05/2022 18:47:55 - INFO - Distillation -   Global step: 4851, epoch step:22
05/05/2022 18:48:01 - INFO - Distillation -   Global step: 4872, epoch step:43
05/05/2022 18:48:06 - INFO - Distillation -   Global step: 4893, epoch step:64
05/05/2022 18:48:12 - INFO - Distillation -   Global step: 4914, epoch step:85
05/05/2022 18:48:17 - INFO - Distillation -   Global step: 4935, epoch step:106
05/05/2022 18:48:23 - INFO - Distillation -   Global step: 4956, epoch step:127
05/05/2022 18:48:28 - INFO - Distillation -   Global step: 4977, epoch step:148
05/05/2022 18:48:33 - INFO - Distillation -   Global step: 4998, epoch step:169
05/05/2022 18:48:39 - INFO - Distillation -   Global step: 5019, epoch step:190
05/05/2022 18:48:44 - INFO - Distillation -   Global step: 5040, epoch step:211
05/05/2022 18:48:50 - INFO - Distillation -   Global step: 5061, epoch step:232
05/05/2022 18:48:55 - INFO - Distillation -   Global step: 5082, epoch step:253
05/05/2022 18:49:01 - INFO - Distillation -   Global step: 5103, epoch step:274
05/05/2022 18:49:06 - INFO - Distillation -   Global step: 5124, epoch step:295
05/05/2022 18:49:12 - INFO - Distillation -   Global step: 5145, epoch step:316
05/05/2022 18:49:17 - INFO - Distillation -   Global step: 5166, epoch step:337
05/05/2022 18:49:23 - INFO - Distillation -   Global step: 5187, epoch step:358
05/05/2022 18:49:28 - INFO - Distillation -   Global step: 5208, epoch step:379
05/05/2022 18:49:34 - INFO - Distillation -   Global step: 5229, epoch step:400
05/05/2022 18:49:39 - INFO - Distillation -   Global step: 5250, epoch step:421
05/05/2022 18:49:44 - INFO - Distillation -   Saving at global step 5268, epoch step 439 epoch 12
05/05/2022 18:49:45 - INFO - Distillation -   Running callback function...
05/05/2022 18:49:45 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 18:49:45 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 18:49:45 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 18:49:45 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 18:49:45 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 18:49:45 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 18:49:45 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 18:49:45 - INFO - __main__ -     Num examples = 3453
05/05/2022 18:49:45 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   1%|▏         | 6/432 [00:00<00:07, 59.90it/s]Evaluating:   3%|▎         | 12/432 [00:00<00:07, 59.70it/s]Evaluating:   4%|▍         | 18/432 [00:00<00:06, 59.62it/s]Evaluating:   6%|▌         | 24/432 [00:00<00:06, 59.46it/s]Evaluating:   7%|▋         | 30/432 [00:00<00:06, 59.24it/s]Evaluating:   8%|▊         | 36/432 [00:00<00:06, 58.98it/s]Evaluating:  10%|▉         | 42/432 [00:00<00:06, 58.78it/s]Evaluating:  11%|█         | 48/432 [00:00<00:06, 58.75it/s]Evaluating:  12%|█▎        | 54/432 [00:00<00:06, 58.64it/s]Evaluating:  14%|█▍        | 60/432 [00:01<00:06, 58.59it/s]Evaluating:  15%|█▌        | 66/432 [00:01<00:06, 58.61it/s]Evaluating:  17%|█▋        | 72/432 [00:01<00:06, 58.57it/s]Evaluating:  18%|█▊        | 78/432 [00:01<00:06, 58.46it/s]Evaluating:  19%|█▉        | 84/432 [00:01<00:05, 58.24it/s]Evaluating:  21%|██        | 90/432 [00:01<00:05, 57.91it/s]Evaluating:  22%|██▏       | 96/432 [00:01<00:05, 57.54it/s]Evaluating:  24%|██▎       | 102/432 [00:01<00:05, 57.47it/s]Evaluating:  25%|██▌       | 108/432 [00:01<00:05, 57.33it/s]Evaluating:  26%|██▋       | 114/432 [00:01<00:05, 57.27it/s]Evaluating:  28%|██▊       | 120/432 [00:02<00:05, 57.16it/s]Evaluating:  29%|██▉       | 126/432 [00:02<00:05, 57.02it/s]Evaluating:  31%|███       | 132/432 [00:02<00:05, 56.83it/s]Evaluating:  32%|███▏      | 138/432 [00:02<00:05, 56.37it/s]Evaluating:  33%|███▎      | 144/432 [00:02<00:05, 56.24it/s]Evaluating:  35%|███▍      | 150/432 [00:02<00:05, 56.00it/s]Evaluating:  36%|███▌      | 156/432 [00:02<00:04, 55.79it/s]Evaluating:  38%|███▊      | 162/432 [00:02<00:04, 55.76it/s]Evaluating:  39%|███▉      | 168/432 [00:02<00:04, 55.50it/s]Evaluating:  40%|████      | 174/432 [00:03<00:04, 55.26it/s]Evaluating:  42%|████▏     | 180/432 [00:03<00:04, 55.03it/s]Evaluating:  43%|████▎     | 186/432 [00:03<00:04, 54.91it/s]Evaluating:  44%|████▍     | 192/432 [00:03<00:04, 54.82it/s]Evaluating:  46%|████▌     | 198/432 [00:03<00:04, 54.63it/s]Evaluating:  47%|████▋     | 204/432 [00:03<00:04, 54.45it/s]Evaluating:  49%|████▊     | 210/432 [00:03<00:04, 54.25it/s]Evaluating:  50%|█████     | 216/432 [00:03<00:03, 54.00it/s]Evaluating:  51%|█████▏    | 222/432 [00:03<00:03, 53.88it/s]Evaluating:  53%|█████▎    | 228/432 [00:04<00:03, 53.80it/s]Evaluating:  54%|█████▍    | 234/432 [00:04<00:03, 53.59it/s]Evaluating:  56%|█████▌    | 240/432 [00:04<00:03, 53.50it/s]Evaluating:  57%|█████▋    | 246/432 [00:04<00:03, 53.33it/s]Evaluating:  58%|█████▊    | 252/432 [00:04<00:03, 53.08it/s]Evaluating:  60%|█████▉    | 258/432 [00:04<00:03, 52.88it/s]Evaluating:  61%|██████    | 264/432 [00:04<00:03, 52.58it/s]Evaluating:  62%|██████▎   | 270/432 [00:04<00:03, 52.44it/s]Evaluating:  64%|██████▍   | 276/432 [00:04<00:02, 52.12it/s]Evaluating:  65%|██████▌   | 282/432 [00:05<00:02, 51.94it/s]Evaluating:  67%|██████▋   | 288/432 [00:05<00:02, 51.73it/s]Evaluating:  68%|██████▊   | 294/432 [00:05<00:02, 51.51it/s]Evaluating:  69%|██████▉   | 300/432 [00:05<00:02, 51.42it/s]Evaluating:  71%|███████   | 306/432 [00:05<00:02, 51.29it/s]Evaluating:  72%|███████▏  | 312/432 [00:05<00:02, 51.18it/s]Evaluating:  74%|███████▎  | 318/432 [00:05<00:02, 51.05it/s]Evaluating:  75%|███████▌  | 324/432 [00:05<00:02, 50.86it/s]Evaluating:  76%|███████▋  | 330/432 [00:06<00:02, 50.69it/s]Evaluating:  78%|███████▊  | 336/432 [00:06<00:01, 50.51it/s]Evaluating:  79%|███████▉  | 342/432 [00:06<00:01, 50.22it/s]Evaluating:  81%|████████  | 348/432 [00:06<00:01, 50.02it/s]Evaluating:  82%|████████▏ | 354/432 [00:06<00:01, 49.87it/s]Evaluating:  83%|████████▎ | 360/432 [00:06<00:01, 49.94it/s]Evaluating:  84%|████████▍ | 365/432 [00:06<00:01, 49.82it/s]Evaluating:  86%|████████▌ | 370/432 [00:06<00:01, 49.75it/s]Evaluating:  87%|████████▋ | 376/432 [00:06<00:01, 49.89it/s]Evaluating:  88%|████████▊ | 381/432 [00:07<00:01, 49.91it/s]Evaluating:  89%|████████▉ | 386/432 [00:07<00:00, 49.85it/s]Evaluating:  91%|█████████ | 391/432 [00:07<00:00, 49.86it/s]Evaluating:  92%|█████████▏| 396/432 [00:07<00:00, 49.67it/s]Evaluating:  93%|█████████▎| 401/432 [00:07<00:00, 49.51it/s]Evaluating:  94%|█████████▍| 406/432 [00:07<00:00, 49.32it/s]Evaluating:  95%|█████████▌| 411/432 [00:07<00:00, 49.19it/s]Evaluating:  96%|█████████▋| 416/432 [00:07<00:00, 48.99it/s]Evaluating:  97%|█████████▋| 421/432 [00:07<00:00, 48.88it/s]Evaluating:  99%|█████████▊| 426/432 [00:07<00:00, 48.70it/s]Evaluating: 100%|█████████▉| 431/432 [00:08<00:00, 48.50it/s]Evaluating: 100%|██████████| 432/432 [00:08<00:00, 53.53it/s]
05/05/2022 18:49:54 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 18:49:54 - INFO - __main__ -     f1 = 0.9090748898678415
05/05/2022 18:49:54 - INFO - __main__ -     loss = 0.2020378537337148
05/05/2022 18:49:54 - INFO - __main__ -     precision = 0.9041359971959341
05/05/2022 18:49:54 - INFO - __main__ -     recall = 0.9140680368532955
05/05/2022 18:49:54 - INFO - Distillation -   Epoch 12 finished
05/05/2022 18:49:54 - INFO - Distillation -   Epoch 13
05/05/2022 18:49:54 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 18:49:55 - INFO - Distillation -   Global step: 5271, epoch step:3
05/05/2022 18:50:00 - INFO - Distillation -   Global step: 5292, epoch step:24
05/05/2022 18:50:06 - INFO - Distillation -   Global step: 5313, epoch step:45
05/05/2022 18:50:11 - INFO - Distillation -   Global step: 5334, epoch step:66
05/05/2022 18:50:16 - INFO - Distillation -   Global step: 5355, epoch step:87
05/05/2022 18:50:22 - INFO - Distillation -   Global step: 5376, epoch step:108
05/05/2022 18:50:27 - INFO - Distillation -   Global step: 5397, epoch step:129
05/05/2022 18:50:33 - INFO - Distillation -   Global step: 5418, epoch step:150
05/05/2022 18:50:38 - INFO - Distillation -   Global step: 5439, epoch step:171
05/05/2022 18:50:44 - INFO - Distillation -   Global step: 5460, epoch step:192
05/05/2022 18:50:49 - INFO - Distillation -   Global step: 5481, epoch step:213
05/05/2022 18:50:55 - INFO - Distillation -   Global step: 5502, epoch step:234
05/05/2022 18:51:00 - INFO - Distillation -   Global step: 5523, epoch step:255
05/05/2022 18:51:06 - INFO - Distillation -   Global step: 5544, epoch step:276
05/05/2022 18:51:11 - INFO - Distillation -   Global step: 5565, epoch step:297
05/05/2022 18:51:17 - INFO - Distillation -   Global step: 5586, epoch step:318
05/05/2022 18:51:22 - INFO - Distillation -   Global step: 5607, epoch step:339
05/05/2022 18:51:28 - INFO - Distillation -   Global step: 5628, epoch step:360
05/05/2022 18:51:33 - INFO - Distillation -   Global step: 5649, epoch step:381
05/05/2022 18:51:39 - INFO - Distillation -   Global step: 5670, epoch step:402
05/05/2022 18:51:44 - INFO - Distillation -   Global step: 5691, epoch step:423
05/05/2022 18:51:48 - INFO - Distillation -   Saving at global step 5707, epoch step 439 epoch 13
05/05/2022 18:51:49 - INFO - Distillation -   Running callback function...
05/05/2022 18:51:49 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 18:51:49 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 18:51:49 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 18:51:49 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 18:51:49 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 18:51:49 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 18:51:49 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 18:51:49 - INFO - __main__ -     Num examples = 3453
05/05/2022 18:51:49 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   2%|▏         | 7/432 [00:00<00:07, 60.25it/s]Evaluating:   3%|▎         | 14/432 [00:00<00:06, 59.90it/s]Evaluating:   5%|▍         | 20/432 [00:00<00:06, 59.68it/s]Evaluating:   6%|▌         | 26/432 [00:00<00:06, 59.51it/s]Evaluating:   7%|▋         | 32/432 [00:00<00:06, 59.28it/s]Evaluating:   9%|▉         | 38/432 [00:00<00:06, 59.05it/s]Evaluating:  10%|█         | 44/432 [00:00<00:06, 58.88it/s]Evaluating:  12%|█▏        | 50/432 [00:00<00:06, 58.87it/s]Evaluating:  13%|█▎        | 56/432 [00:00<00:06, 58.74it/s]Evaluating:  14%|█▍        | 62/432 [00:01<00:06, 58.51it/s]Evaluating:  16%|█▌        | 68/432 [00:01<00:06, 58.38it/s]Evaluating:  17%|█▋        | 74/432 [00:01<00:06, 58.44it/s]Evaluating:  19%|█▊        | 80/432 [00:01<00:06, 58.31it/s]Evaluating:  20%|█▉        | 86/432 [00:01<00:05, 58.36it/s]Evaluating:  21%|██▏       | 92/432 [00:01<00:05, 58.00it/s]Evaluating:  23%|██▎       | 98/432 [00:01<00:05, 57.60it/s]Evaluating:  24%|██▍       | 104/432 [00:01<00:05, 57.50it/s]Evaluating:  25%|██▌       | 110/432 [00:01<00:05, 57.44it/s]Evaluating:  27%|██▋       | 116/432 [00:01<00:05, 57.33it/s]Evaluating:  28%|██▊       | 122/432 [00:02<00:05, 57.04it/s]Evaluating:  30%|██▉       | 128/432 [00:02<00:05, 56.88it/s]Evaluating:  31%|███       | 134/432 [00:02<00:05, 56.67it/s]Evaluating:  32%|███▏      | 140/432 [00:02<00:05, 56.62it/s]Evaluating:  34%|███▍      | 146/432 [00:02<00:05, 56.38it/s]Evaluating:  35%|███▌      | 152/432 [00:02<00:04, 56.10it/s]Evaluating:  37%|███▋      | 158/432 [00:02<00:04, 56.02it/s]Evaluating:  38%|███▊      | 164/432 [00:02<00:04, 55.92it/s]Evaluating:  39%|███▉      | 170/432 [00:02<00:04, 55.58it/s]Evaluating:  41%|████      | 176/432 [00:03<00:04, 55.31it/s]Evaluating:  42%|████▏     | 182/432 [00:03<00:04, 55.12it/s]Evaluating:  44%|████▎     | 188/432 [00:03<00:04, 54.75it/s]Evaluating:  45%|████▍     | 194/432 [00:03<00:04, 54.52it/s]Evaluating:  46%|████▋     | 200/432 [00:03<00:04, 54.37it/s]Evaluating:  48%|████▊     | 206/432 [00:03<00:04, 54.10it/s]Evaluating:  49%|████▉     | 212/432 [00:03<00:04, 53.88it/s]Evaluating:  50%|█████     | 218/432 [00:03<00:03, 53.83it/s]Evaluating:  52%|█████▏    | 224/432 [00:03<00:03, 53.63it/s]Evaluating:  53%|█████▎    | 230/432 [00:04<00:03, 53.39it/s]Evaluating:  55%|█████▍    | 236/432 [00:04<00:03, 53.33it/s]Evaluating:  56%|█████▌    | 242/432 [00:04<00:03, 53.36it/s]Evaluating:  57%|█████▋    | 248/432 [00:04<00:03, 53.15it/s]Evaluating:  59%|█████▉    | 254/432 [00:04<00:03, 52.90it/s]Evaluating:  60%|██████    | 260/432 [00:04<00:03, 52.71it/s]Evaluating:  62%|██████▏   | 266/432 [00:04<00:03, 52.55it/s]Evaluating:  63%|██████▎   | 272/432 [00:04<00:03, 52.45it/s]Evaluating:  64%|██████▍   | 278/432 [00:04<00:02, 52.29it/s]Evaluating:  66%|██████▌   | 284/432 [00:05<00:02, 52.12it/s]Evaluating:  67%|██████▋   | 290/432 [00:05<00:02, 51.81it/s]Evaluating:  69%|██████▊   | 296/432 [00:05<00:02, 51.75it/s]Evaluating:  70%|██████▉   | 302/432 [00:05<00:02, 51.62it/s]Evaluating:  71%|███████▏  | 308/432 [00:05<00:02, 51.51it/s]Evaluating:  73%|███████▎  | 314/432 [00:05<00:02, 51.36it/s]Evaluating:  74%|███████▍  | 320/432 [00:05<00:02, 51.00it/s]Evaluating:  75%|███████▌  | 326/432 [00:05<00:02, 50.92it/s]Evaluating:  77%|███████▋  | 332/432 [00:06<00:01, 50.74it/s]Evaluating:  78%|███████▊  | 338/432 [00:06<00:01, 50.45it/s]Evaluating:  80%|███████▉  | 344/432 [00:06<00:01, 50.29it/s]Evaluating:  81%|████████  | 350/432 [00:06<00:01, 50.24it/s]Evaluating:  82%|████████▏ | 356/432 [00:06<00:01, 50.12it/s]Evaluating:  84%|████████▍ | 362/432 [00:06<00:01, 49.98it/s]Evaluating:  85%|████████▍ | 367/432 [00:06<00:01, 49.84it/s]Evaluating:  86%|████████▌ | 372/432 [00:06<00:01, 49.70it/s]Evaluating:  87%|████████▋ | 377/432 [00:06<00:01, 49.57it/s]Evaluating:  88%|████████▊ | 382/432 [00:07<00:01, 49.64it/s]Evaluating:  90%|████████▉ | 387/432 [00:07<00:00, 49.52it/s]Evaluating:  91%|█████████ | 392/432 [00:07<00:00, 49.61it/s]Evaluating:  92%|█████████▏| 397/432 [00:07<00:00, 49.66it/s]Evaluating:  93%|█████████▎| 402/432 [00:07<00:00, 49.55it/s]Evaluating:  94%|█████████▍| 407/432 [00:07<00:00, 49.38it/s]Evaluating:  95%|█████████▌| 412/432 [00:07<00:00, 49.22it/s]Evaluating:  97%|█████████▋| 417/432 [00:07<00:00, 48.86it/s]Evaluating:  98%|█████████▊| 422/432 [00:07<00:00, 48.76it/s]Evaluating:  99%|█████████▉| 427/432 [00:07<00:00, 48.56it/s]Evaluating: 100%|██████████| 432/432 [00:08<00:00, 53.56it/s]
05/05/2022 18:51:58 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 18:51:58 - INFO - __main__ -     f1 = 0.9114303340089892
05/05/2022 18:51:58 - INFO - __main__ -     loss = 0.20236871594547706
05/05/2022 18:51:58 - INFO - __main__ -     precision = 0.9067157636331755
05/05/2022 18:51:58 - INFO - __main__ -     recall = 0.916194188518781
05/05/2022 18:51:58 - INFO - Distillation -   Epoch 13 finished
05/05/2022 18:51:58 - INFO - Distillation -   Epoch 14
05/05/2022 18:51:58 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 18:51:59 - INFO - Distillation -   Global step: 5712, epoch step:5
05/05/2022 18:52:05 - INFO - Distillation -   Global step: 5733, epoch step:26
05/05/2022 18:52:10 - INFO - Distillation -   Global step: 5754, epoch step:47
05/05/2022 18:52:16 - INFO - Distillation -   Global step: 5775, epoch step:68
05/05/2022 18:52:21 - INFO - Distillation -   Global step: 5796, epoch step:89
05/05/2022 18:52:27 - INFO - Distillation -   Global step: 5817, epoch step:110
05/05/2022 18:52:32 - INFO - Distillation -   Global step: 5838, epoch step:131
05/05/2022 18:52:38 - INFO - Distillation -   Global step: 5859, epoch step:152
05/05/2022 18:52:43 - INFO - Distillation -   Global step: 5880, epoch step:173
05/05/2022 18:52:49 - INFO - Distillation -   Global step: 5901, epoch step:194
05/05/2022 18:52:54 - INFO - Distillation -   Global step: 5922, epoch step:215
05/05/2022 18:53:00 - INFO - Distillation -   Global step: 5943, epoch step:236
05/05/2022 18:53:05 - INFO - Distillation -   Global step: 5964, epoch step:257
05/05/2022 18:53:11 - INFO - Distillation -   Global step: 5985, epoch step:278
05/05/2022 18:53:16 - INFO - Distillation -   Global step: 6006, epoch step:299
05/05/2022 18:53:22 - INFO - Distillation -   Global step: 6027, epoch step:320
05/05/2022 18:53:27 - INFO - Distillation -   Global step: 6048, epoch step:341
05/05/2022 18:53:33 - INFO - Distillation -   Global step: 6069, epoch step:362
05/05/2022 18:53:38 - INFO - Distillation -   Global step: 6090, epoch step:383
05/05/2022 18:53:44 - INFO - Distillation -   Global step: 6111, epoch step:404
05/05/2022 18:53:49 - INFO - Distillation -   Global step: 6132, epoch step:425
05/05/2022 18:53:53 - INFO - Distillation -   Saving at global step 6146, epoch step 439 epoch 14
05/05/2022 18:53:53 - INFO - Distillation -   Running callback function...
05/05/2022 18:53:53 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 18:53:53 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 18:53:53 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 18:53:53 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 18:53:53 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 18:53:53 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 18:53:54 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 18:53:54 - INFO - __main__ -     Num examples = 3453
05/05/2022 18:53:54 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   1%|▏         | 6/432 [00:00<00:07, 59.92it/s]Evaluating:   3%|▎         | 12/432 [00:00<00:07, 59.82it/s]Evaluating:   4%|▍         | 18/432 [00:00<00:06, 59.70it/s]Evaluating:   6%|▌         | 24/432 [00:00<00:06, 59.50it/s]Evaluating:   7%|▋         | 30/432 [00:00<00:06, 59.27it/s]Evaluating:   8%|▊         | 36/432 [00:00<00:06, 59.11it/s]Evaluating:  10%|▉         | 42/432 [00:00<00:06, 58.72it/s]Evaluating:  11%|█         | 48/432 [00:00<00:06, 58.38it/s]Evaluating:  12%|█▎        | 54/432 [00:00<00:06, 58.25it/s]Evaluating:  14%|█▍        | 60/432 [00:01<00:06, 58.07it/s]Evaluating:  15%|█▌        | 66/432 [00:01<00:06, 58.09it/s]Evaluating:  17%|█▋        | 72/432 [00:01<00:06, 58.11it/s]Evaluating:  18%|█▊        | 78/432 [00:01<00:06, 57.98it/s]Evaluating:  19%|█▉        | 84/432 [00:01<00:06, 57.83it/s]Evaluating:  21%|██        | 90/432 [00:01<00:05, 57.64it/s]Evaluating:  22%|██▏       | 96/432 [00:01<00:05, 57.55it/s]Evaluating:  24%|██▎       | 102/432 [00:01<00:05, 57.44it/s]Evaluating:  25%|██▌       | 108/432 [00:01<00:05, 57.22it/s]Evaluating:  26%|██▋       | 114/432 [00:01<00:05, 57.11it/s]Evaluating:  28%|██▊       | 120/432 [00:02<00:05, 56.91it/s]Evaluating:  29%|██▉       | 126/432 [00:02<00:05, 56.70it/s]Evaluating:  31%|███       | 132/432 [00:02<00:05, 56.54it/s]Evaluating:  32%|███▏      | 138/432 [00:02<00:05, 56.46it/s]Evaluating:  33%|███▎      | 144/432 [00:02<00:05, 56.39it/s]Evaluating:  35%|███▍      | 150/432 [00:02<00:05, 56.19it/s]Evaluating:  36%|███▌      | 156/432 [00:02<00:04, 56.13it/s]Evaluating:  38%|███▊      | 162/432 [00:02<00:04, 55.93it/s]Evaluating:  39%|███▉      | 168/432 [00:02<00:04, 55.67it/s]Evaluating:  40%|████      | 174/432 [00:03<00:04, 55.60it/s]Evaluating:  42%|████▏     | 180/432 [00:03<00:04, 55.51it/s]Evaluating:  43%|████▎     | 186/432 [00:03<00:04, 55.29it/s]Evaluating:  44%|████▍     | 192/432 [00:03<00:04, 55.04it/s]Evaluating:  46%|████▌     | 198/432 [00:03<00:04, 54.82it/s]Evaluating:  47%|████▋     | 204/432 [00:03<00:04, 54.51it/s]Evaluating:  49%|████▊     | 210/432 [00:03<00:04, 54.35it/s]Evaluating:  50%|█████     | 216/432 [00:03<00:03, 54.06it/s]Evaluating:  51%|█████▏    | 222/432 [00:03<00:03, 53.97it/s]Evaluating:  53%|█████▎    | 228/432 [00:04<00:03, 53.76it/s]Evaluating:  54%|█████▍    | 234/432 [00:04<00:03, 53.58it/s]Evaluating:  56%|█████▌    | 240/432 [00:04<00:03, 53.46it/s]Evaluating:  57%|█████▋    | 246/432 [00:04<00:03, 53.43it/s]Evaluating:  58%|█████▊    | 252/432 [00:04<00:03, 53.16it/s]Evaluating:  60%|█████▉    | 258/432 [00:04<00:03, 53.01it/s]Evaluating:  61%|██████    | 264/432 [00:04<00:03, 52.85it/s]Evaluating:  62%|██████▎   | 270/432 [00:04<00:03, 52.48it/s]Evaluating:  64%|██████▍   | 276/432 [00:04<00:02, 52.36it/s]Evaluating:  65%|██████▌   | 282/432 [00:05<00:02, 52.25it/s]Evaluating:  67%|██████▋   | 288/432 [00:05<00:02, 52.17it/s]Evaluating:  68%|██████▊   | 294/432 [00:05<00:02, 52.07it/s]Evaluating:  69%|██████▉   | 300/432 [00:05<00:02, 51.80it/s]Evaluating:  71%|███████   | 306/432 [00:05<00:02, 51.49it/s]Evaluating:  72%|███████▏  | 312/432 [00:05<00:02, 51.32it/s]Evaluating:  74%|███████▎  | 318/432 [00:05<00:02, 51.18it/s]Evaluating:  75%|███████▌  | 324/432 [00:05<00:02, 50.98it/s]Evaluating:  76%|███████▋  | 330/432 [00:05<00:02, 50.99it/s]Evaluating:  78%|███████▊  | 336/432 [00:06<00:01, 50.75it/s]Evaluating:  79%|███████▉  | 342/432 [00:06<00:01, 50.45it/s]Evaluating:  81%|████████  | 348/432 [00:06<00:01, 50.25it/s]Evaluating:  82%|████████▏ | 354/432 [00:06<00:01, 50.04it/s]Evaluating:  83%|████████▎ | 360/432 [00:06<00:01, 49.90it/s]Evaluating:  84%|████████▍ | 365/432 [00:06<00:01, 49.76it/s]Evaluating:  86%|████████▌ | 370/432 [00:06<00:01, 49.64it/s]Evaluating:  87%|████████▋ | 375/432 [00:06<00:01, 49.52it/s]Evaluating:  88%|████████▊ | 380/432 [00:07<00:01, 49.43it/s]Evaluating:  89%|████████▉ | 385/432 [00:07<00:00, 49.30it/s]Evaluating:  90%|█████████ | 390/432 [00:07<00:00, 49.15it/s]Evaluating:  91%|█████████▏| 395/432 [00:07<00:00, 49.00it/s]Evaluating:  93%|█████████▎| 400/432 [00:07<00:00, 48.89it/s]Evaluating:  94%|█████████▍| 405/432 [00:07<00:00, 48.69it/s]Evaluating:  95%|█████████▍| 410/432 [00:07<00:00, 48.62it/s]Evaluating:  96%|█████████▌| 415/432 [00:07<00:00, 48.69it/s]Evaluating:  97%|█████████▋| 420/432 [00:07<00:00, 48.58it/s]Evaluating:  98%|█████████▊| 425/432 [00:07<00:00, 48.67it/s]Evaluating: 100%|█████████▉| 430/432 [00:08<00:00, 48.62it/s]Evaluating: 100%|██████████| 432/432 [00:08<00:00, 53.51it/s]
05/05/2022 18:54:02 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 18:54:02 - INFO - __main__ -     f1 = 0.910980012327199
05/05/2022 18:54:02 - INFO - __main__ -     loss = 0.20019723208185025
05/05/2022 18:54:02 - INFO - __main__ -     precision = 0.9054787327148608
05/05/2022 18:54:02 - INFO - __main__ -     recall = 0.9165485471296952
05/05/2022 18:54:02 - INFO - Distillation -   Epoch 14 finished
05/05/2022 18:54:02 - INFO - Distillation -   Epoch 15
05/05/2022 18:54:02 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 18:54:04 - INFO - Distillation -   Global step: 6153, epoch step:7
05/05/2022 18:54:10 - INFO - Distillation -   Global step: 6174, epoch step:28
05/05/2022 18:54:15 - INFO - Distillation -   Global step: 6195, epoch step:49
05/05/2022 18:54:21 - INFO - Distillation -   Global step: 6216, epoch step:70
05/05/2022 18:54:26 - INFO - Distillation -   Global step: 6237, epoch step:91
05/05/2022 18:54:32 - INFO - Distillation -   Global step: 6258, epoch step:112
05/05/2022 18:54:37 - INFO - Distillation -   Global step: 6279, epoch step:133
05/05/2022 18:54:43 - INFO - Distillation -   Global step: 6300, epoch step:154
05/05/2022 18:54:48 - INFO - Distillation -   Global step: 6321, epoch step:175
05/05/2022 18:54:54 - INFO - Distillation -   Global step: 6342, epoch step:196
05/05/2022 18:54:59 - INFO - Distillation -   Global step: 6363, epoch step:217
05/05/2022 18:55:05 - INFO - Distillation -   Global step: 6384, epoch step:238
05/05/2022 18:55:10 - INFO - Distillation -   Global step: 6405, epoch step:259
05/05/2022 18:55:16 - INFO - Distillation -   Global step: 6426, epoch step:280
05/05/2022 18:55:21 - INFO - Distillation -   Global step: 6447, epoch step:301
05/05/2022 18:55:27 - INFO - Distillation -   Global step: 6468, epoch step:322
05/05/2022 18:55:32 - INFO - Distillation -   Global step: 6489, epoch step:343
05/05/2022 18:55:38 - INFO - Distillation -   Global step: 6510, epoch step:364
05/05/2022 18:55:43 - INFO - Distillation -   Global step: 6531, epoch step:385
05/05/2022 18:55:48 - INFO - Distillation -   Global step: 6552, epoch step:406
05/05/2022 18:55:54 - INFO - Distillation -   Global step: 6573, epoch step:427
05/05/2022 18:55:57 - INFO - Distillation -   Saving at global step 6585, epoch step 439 epoch 15
05/05/2022 18:55:58 - INFO - Distillation -   Running callback function...
05/05/2022 18:55:58 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 18:55:58 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 18:55:58 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 18:55:58 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 18:55:58 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 18:55:58 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 18:55:58 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 18:55:58 - INFO - __main__ -     Num examples = 3453
05/05/2022 18:55:58 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   2%|▏         | 7/432 [00:00<00:07, 60.21it/s]Evaluating:   3%|▎         | 14/432 [00:00<00:06, 60.03it/s]Evaluating:   5%|▍         | 21/432 [00:00<00:06, 59.79it/s]Evaluating:   6%|▋         | 27/432 [00:00<00:06, 59.62it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:06, 59.40it/s]Evaluating:   9%|▉         | 39/432 [00:00<00:06, 59.15it/s]Evaluating:  10%|█         | 45/432 [00:00<00:06, 58.89it/s]Evaluating:  12%|█▏        | 51/432 [00:00<00:06, 58.81it/s]Evaluating:  13%|█▎        | 57/432 [00:00<00:06, 58.74it/s]Evaluating:  15%|█▍        | 63/432 [00:01<00:06, 58.54it/s]Evaluating:  16%|█▌        | 69/432 [00:01<00:06, 58.41it/s]Evaluating:  17%|█▋        | 75/432 [00:01<00:06, 58.24it/s]Evaluating:  19%|█▉        | 81/432 [00:01<00:06, 58.17it/s]Evaluating:  20%|██        | 87/432 [00:01<00:05, 57.94it/s]Evaluating:  22%|██▏       | 93/432 [00:01<00:05, 57.78it/s]Evaluating:  23%|██▎       | 99/432 [00:01<00:05, 57.72it/s]Evaluating:  24%|██▍       | 105/432 [00:01<00:05, 57.50it/s]Evaluating:  26%|██▌       | 111/432 [00:01<00:05, 57.43it/s]Evaluating:  27%|██▋       | 117/432 [00:02<00:05, 57.32it/s]Evaluating:  28%|██▊       | 123/432 [00:02<00:05, 57.32it/s]Evaluating:  30%|██▉       | 129/432 [00:02<00:05, 57.09it/s]Evaluating:  31%|███▏      | 135/432 [00:02<00:05, 56.83it/s]Evaluating:  33%|███▎      | 141/432 [00:02<00:05, 56.63it/s]Evaluating:  34%|███▍      | 147/432 [00:02<00:05, 56.26it/s]Evaluating:  35%|███▌      | 153/432 [00:02<00:04, 56.10it/s]Evaluating:  37%|███▋      | 159/432 [00:02<00:04, 55.88it/s]Evaluating:  38%|███▊      | 165/432 [00:02<00:04, 55.59it/s]Evaluating:  40%|███▉      | 171/432 [00:02<00:04, 55.26it/s]Evaluating:  41%|████      | 177/432 [00:03<00:04, 55.06it/s]Evaluating:  42%|████▏     | 183/432 [00:03<00:04, 54.90it/s]Evaluating:  44%|████▍     | 189/432 [00:03<00:04, 54.71it/s]Evaluating:  45%|████▌     | 195/432 [00:03<00:04, 54.60it/s]Evaluating:  47%|████▋     | 201/432 [00:03<00:04, 54.43it/s]Evaluating:  48%|████▊     | 207/432 [00:03<00:04, 54.22it/s]Evaluating:  49%|████▉     | 213/432 [00:03<00:04, 54.05it/s]Evaluating:  51%|█████     | 219/432 [00:03<00:03, 53.89it/s]Evaluating:  52%|█████▏    | 225/432 [00:03<00:03, 53.69it/s]Evaluating:  53%|█████▎    | 231/432 [00:04<00:03, 53.58it/s]Evaluating:  55%|█████▍    | 237/432 [00:04<00:03, 53.45it/s]Evaluating:  56%|█████▋    | 243/432 [00:04<00:03, 53.36it/s]Evaluating:  58%|█████▊    | 249/432 [00:04<00:03, 53.17it/s]Evaluating:  59%|█████▉    | 255/432 [00:04<00:03, 52.83it/s]Evaluating:  60%|██████    | 261/432 [00:04<00:03, 52.67it/s]Evaluating:  62%|██████▏   | 267/432 [00:04<00:03, 52.51it/s]Evaluating:  63%|██████▎   | 273/432 [00:04<00:03, 52.39it/s]Evaluating:  65%|██████▍   | 279/432 [00:05<00:02, 52.22it/s]Evaluating:  66%|██████▌   | 285/432 [00:05<00:02, 52.15it/s]Evaluating:  67%|██████▋   | 291/432 [00:05<00:02, 52.13it/s]Evaluating:  69%|██████▉   | 297/432 [00:05<00:02, 52.13it/s]Evaluating:  70%|███████   | 303/432 [00:05<00:02, 51.87it/s]Evaluating:  72%|███████▏  | 309/432 [00:05<00:02, 51.57it/s]Evaluating:  73%|███████▎  | 315/432 [00:05<00:02, 51.40it/s]Evaluating:  74%|███████▍  | 321/432 [00:05<00:02, 51.19it/s]Evaluating:  76%|███████▌  | 327/432 [00:05<00:02, 51.12it/s]Evaluating:  77%|███████▋  | 333/432 [00:06<00:01, 50.98it/s]Evaluating:  78%|███████▊  | 339/432 [00:06<00:01, 50.91it/s]Evaluating:  80%|███████▉  | 345/432 [00:06<00:01, 50.81it/s]Evaluating:  81%|████████▏ | 351/432 [00:06<00:01, 50.62it/s]Evaluating:  83%|████████▎ | 357/432 [00:06<00:01, 50.32it/s]Evaluating:  84%|████████▍ | 363/432 [00:06<00:01, 50.07it/s]Evaluating:  85%|████████▌ | 369/432 [00:06<00:01, 50.06it/s]Evaluating:  87%|████████▋ | 375/432 [00:06<00:01, 49.94it/s]Evaluating:  88%|████████▊ | 380/432 [00:06<00:01, 49.88it/s]Evaluating:  89%|████████▉ | 385/432 [00:07<00:00, 49.74it/s]Evaluating:  90%|█████████ | 390/432 [00:07<00:00, 49.54it/s]Evaluating:  91%|█████████▏| 395/432 [00:07<00:00, 49.44it/s]Evaluating:  93%|█████████▎| 400/432 [00:07<00:00, 49.33it/s]Evaluating:  94%|█████████▍| 405/432 [00:07<00:00, 49.19it/s]Evaluating:  95%|█████████▍| 410/432 [00:07<00:00, 49.06it/s]Evaluating:  96%|█████████▌| 415/432 [00:07<00:00, 48.99it/s]Evaluating:  97%|█████████▋| 420/432 [00:07<00:00, 48.80it/s]Evaluating:  98%|█████████▊| 425/432 [00:07<00:00, 48.59it/s]Evaluating: 100%|█████████▉| 430/432 [00:08<00:00, 48.39it/s]Evaluating: 100%|██████████| 432/432 [00:08<00:00, 53.62it/s]
05/05/2022 18:56:07 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 18:56:07 - INFO - __main__ -     f1 = 0.9076192152032376
05/05/2022 18:56:07 - INFO - __main__ -     loss = 0.204899797691773
05/05/2022 18:56:07 - INFO - __main__ -     precision = 0.9014330653617616
05/05/2022 18:56:07 - INFO - __main__ -     recall = 0.9138908575478384
05/05/2022 18:56:07 - INFO - Distillation -   Epoch 15 finished
05/05/2022 18:56:07 - INFO - Distillation -   Epoch 16
05/05/2022 18:56:07 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 18:56:09 - INFO - Distillation -   Global step: 6594, epoch step:9
05/05/2022 18:56:15 - INFO - Distillation -   Global step: 6615, epoch step:30
05/05/2022 18:56:20 - INFO - Distillation -   Global step: 6636, epoch step:51
05/05/2022 18:56:26 - INFO - Distillation -   Global step: 6657, epoch step:72
05/05/2022 18:56:31 - INFO - Distillation -   Global step: 6678, epoch step:93
05/05/2022 18:56:37 - INFO - Distillation -   Global step: 6699, epoch step:114
05/05/2022 18:56:42 - INFO - Distillation -   Global step: 6720, epoch step:135
05/05/2022 18:56:48 - INFO - Distillation -   Global step: 6741, epoch step:156
05/05/2022 18:56:53 - INFO - Distillation -   Global step: 6762, epoch step:177
05/05/2022 18:56:59 - INFO - Distillation -   Global step: 6783, epoch step:198
05/05/2022 18:57:04 - INFO - Distillation -   Global step: 6804, epoch step:219
05/05/2022 18:57:09 - INFO - Distillation -   Global step: 6825, epoch step:240
05/05/2022 18:57:15 - INFO - Distillation -   Global step: 6846, epoch step:261
05/05/2022 18:57:20 - INFO - Distillation -   Global step: 6867, epoch step:282
05/05/2022 18:57:26 - INFO - Distillation -   Global step: 6888, epoch step:303
05/05/2022 18:57:31 - INFO - Distillation -   Global step: 6909, epoch step:324
05/05/2022 18:57:37 - INFO - Distillation -   Global step: 6930, epoch step:345
05/05/2022 18:57:42 - INFO - Distillation -   Global step: 6951, epoch step:366
05/05/2022 18:57:48 - INFO - Distillation -   Global step: 6972, epoch step:387
05/05/2022 18:57:53 - INFO - Distillation -   Global step: 6993, epoch step:408
05/05/2022 18:57:59 - INFO - Distillation -   Global step: 7014, epoch step:429
05/05/2022 18:58:01 - INFO - Distillation -   Saving at global step 7024, epoch step 439 epoch 16
05/05/2022 18:58:02 - INFO - Distillation -   Running callback function...
05/05/2022 18:58:02 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 18:58:02 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 18:58:02 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 18:58:02 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 18:58:02 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 18:58:02 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 18:58:02 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 18:58:02 - INFO - __main__ -     Num examples = 3453
05/05/2022 18:58:02 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   1%|▏         | 6/432 [00:00<00:07, 59.89it/s]Evaluating:   3%|▎         | 12/432 [00:00<00:07, 59.80it/s]Evaluating:   4%|▍         | 18/432 [00:00<00:06, 59.55it/s]Evaluating:   6%|▌         | 24/432 [00:00<00:06, 59.47it/s]Evaluating:   7%|▋         | 30/432 [00:00<00:06, 59.24it/s]Evaluating:   8%|▊         | 36/432 [00:00<00:06, 59.13it/s]Evaluating:  10%|▉         | 42/432 [00:00<00:06, 59.06it/s]Evaluating:  11%|█         | 48/432 [00:00<00:06, 58.97it/s]Evaluating:  12%|█▎        | 54/432 [00:00<00:06, 58.91it/s]Evaluating:  14%|█▍        | 60/432 [00:01<00:06, 58.89it/s]Evaluating:  15%|█▌        | 66/432 [00:01<00:06, 58.78it/s]Evaluating:  17%|█▋        | 72/432 [00:01<00:06, 58.67it/s]Evaluating:  18%|█▊        | 78/432 [00:01<00:06, 58.44it/s]Evaluating:  19%|█▉        | 84/432 [00:01<00:06, 57.94it/s]Evaluating:  21%|██        | 90/432 [00:01<00:05, 57.49it/s]Evaluating:  22%|██▏       | 96/432 [00:01<00:05, 57.19it/s]Evaluating:  24%|██▎       | 102/432 [00:01<00:05, 56.97it/s]Evaluating:  25%|██▌       | 108/432 [00:01<00:05, 56.74it/s]Evaluating:  26%|██▋       | 114/432 [00:01<00:05, 56.60it/s]Evaluating:  28%|██▊       | 120/432 [00:02<00:05, 56.59it/s]Evaluating:  29%|██▉       | 126/432 [00:02<00:05, 56.44it/s]Evaluating:  31%|███       | 132/432 [00:02<00:05, 56.41it/s]Evaluating:  32%|███▏      | 138/432 [00:02<00:05, 56.42it/s]Evaluating:  33%|███▎      | 144/432 [00:02<00:05, 56.37it/s]Evaluating:  35%|███▍      | 150/432 [00:02<00:04, 56.43it/s]Evaluating:  36%|███▌      | 156/432 [00:02<00:04, 56.35it/s]Evaluating:  38%|███▊      | 162/432 [00:02<00:04, 56.15it/s]Evaluating:  39%|███▉      | 168/432 [00:02<00:04, 55.48it/s]Evaluating:  40%|████      | 174/432 [00:03<00:04, 55.25it/s]Evaluating:  42%|████▏     | 180/432 [00:03<00:04, 55.07it/s]Evaluating:  43%|████▎     | 186/432 [00:03<00:04, 55.07it/s]Evaluating:  44%|████▍     | 192/432 [00:03<00:04, 54.85it/s]Evaluating:  46%|████▌     | 198/432 [00:03<00:04, 54.57it/s]Evaluating:  47%|████▋     | 204/432 [00:03<00:04, 54.34it/s]Evaluating:  49%|████▊     | 210/432 [00:03<00:04, 54.32it/s]Evaluating:  50%|█████     | 216/432 [00:03<00:04, 53.96it/s]Evaluating:  51%|█████▏    | 222/432 [00:03<00:03, 53.53it/s]Evaluating:  53%|█████▎    | 228/432 [00:04<00:03, 53.46it/s]Evaluating:  54%|█████▍    | 234/432 [00:04<00:03, 53.36it/s]Evaluating:  56%|█████▌    | 240/432 [00:04<00:03, 53.38it/s]Evaluating:  57%|█████▋    | 246/432 [00:04<00:03, 53.36it/s]Evaluating:  58%|█████▊    | 252/432 [00:04<00:03, 52.85it/s]Evaluating:  60%|█████▉    | 258/432 [00:04<00:03, 52.76it/s]Evaluating:  61%|██████    | 264/432 [00:04<00:03, 52.66it/s]Evaluating:  62%|██████▎   | 270/432 [00:04<00:03, 52.38it/s]Evaluating:  64%|██████▍   | 276/432 [00:04<00:02, 52.20it/s]Evaluating:  65%|██████▌   | 282/432 [00:05<00:02, 52.10it/s]Evaluating:  67%|██████▋   | 288/432 [00:05<00:02, 52.00it/s]Evaluating:  68%|██████▊   | 294/432 [00:05<00:02, 51.93it/s]Evaluating:  69%|██████▉   | 300/432 [00:05<00:02, 51.81it/s]Evaluating:  71%|███████   | 306/432 [00:05<00:02, 51.53it/s]Evaluating:  72%|███████▏  | 312/432 [00:05<00:02, 51.46it/s]Evaluating:  74%|███████▎  | 318/432 [00:05<00:02, 51.44it/s]Evaluating:  75%|███████▌  | 324/432 [00:05<00:02, 51.43it/s]Evaluating:  76%|███████▋  | 330/432 [00:06<00:01, 51.29it/s]Evaluating:  78%|███████▊  | 336/432 [00:06<00:01, 51.06it/s]Evaluating:  79%|███████▉  | 342/432 [00:06<00:01, 50.83it/s]Evaluating:  81%|████████  | 348/432 [00:06<00:01, 50.68it/s]Evaluating:  82%|████████▏ | 354/432 [00:06<00:01, 50.28it/s]Evaluating:  83%|████████▎ | 360/432 [00:06<00:01, 49.90it/s]Evaluating:  84%|████████▍ | 365/432 [00:06<00:01, 49.79it/s]Evaluating:  86%|████████▌ | 370/432 [00:06<00:01, 49.79it/s]Evaluating:  87%|████████▋ | 375/432 [00:06<00:01, 49.63it/s]Evaluating:  88%|████████▊ | 380/432 [00:07<00:01, 49.39it/s]Evaluating:  89%|████████▉ | 385/432 [00:07<00:00, 49.13it/s]Evaluating:  90%|█████████ | 390/432 [00:07<00:00, 48.99it/s]Evaluating:  91%|█████████▏| 395/432 [00:07<00:00, 48.89it/s]Evaluating:  93%|█████████▎| 400/432 [00:07<00:00, 48.75it/s]Evaluating:  94%|█████████▍| 405/432 [00:07<00:00, 48.63it/s]Evaluating:  95%|█████████▍| 410/432 [00:07<00:00, 48.53it/s]Evaluating:  96%|█████████▌| 415/432 [00:07<00:00, 48.53it/s]Evaluating:  97%|█████████▋| 420/432 [00:07<00:00, 48.45it/s]Evaluating:  98%|█████████▊| 425/432 [00:07<00:00, 48.18it/s]Evaluating: 100%|█████████▉| 430/432 [00:08<00:00, 48.04it/s]Evaluating: 100%|██████████| 432/432 [00:08<00:00, 53.46it/s]
05/05/2022 18:58:11 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 18:58:11 - INFO - __main__ -     f1 = 0.9097797356828194
05/05/2022 18:58:11 - INFO - __main__ -     loss = 0.20144708781806267
05/05/2022 18:58:11 - INFO - __main__ -     precision = 0.9048370136698213
05/05/2022 18:58:11 - INFO - __main__ -     recall = 0.914776754075124
05/05/2022 18:58:11 - INFO - Distillation -   Epoch 16 finished
05/05/2022 18:58:11 - INFO - Distillation -   Epoch 17
05/05/2022 18:58:11 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 18:58:14 - INFO - Distillation -   Global step: 7035, epoch step:11
05/05/2022 18:58:20 - INFO - Distillation -   Global step: 7056, epoch step:32
05/05/2022 18:58:25 - INFO - Distillation -   Global step: 7077, epoch step:53
05/05/2022 18:58:31 - INFO - Distillation -   Global step: 7098, epoch step:74
05/05/2022 18:58:36 - INFO - Distillation -   Global step: 7119, epoch step:95
05/05/2022 18:58:41 - INFO - Distillation -   Global step: 7140, epoch step:116
05/05/2022 18:58:47 - INFO - Distillation -   Global step: 7161, epoch step:137
05/05/2022 18:58:52 - INFO - Distillation -   Global step: 7182, epoch step:158
05/05/2022 18:58:58 - INFO - Distillation -   Global step: 7203, epoch step:179
05/05/2022 18:59:03 - INFO - Distillation -   Global step: 7224, epoch step:200
05/05/2022 18:59:09 - INFO - Distillation -   Global step: 7245, epoch step:221
05/05/2022 18:59:14 - INFO - Distillation -   Global step: 7266, epoch step:242
05/05/2022 18:59:20 - INFO - Distillation -   Global step: 7287, epoch step:263
05/05/2022 18:59:25 - INFO - Distillation -   Global step: 7308, epoch step:284
05/05/2022 18:59:31 - INFO - Distillation -   Global step: 7329, epoch step:305
05/05/2022 18:59:36 - INFO - Distillation -   Global step: 7350, epoch step:326
05/05/2022 18:59:42 - INFO - Distillation -   Global step: 7371, epoch step:347
05/05/2022 18:59:47 - INFO - Distillation -   Global step: 7392, epoch step:368
05/05/2022 18:59:53 - INFO - Distillation -   Global step: 7413, epoch step:389
05/05/2022 18:59:58 - INFO - Distillation -   Global step: 7434, epoch step:410
05/05/2022 19:00:04 - INFO - Distillation -   Global step: 7455, epoch step:431
05/05/2022 19:00:06 - INFO - Distillation -   Saving at global step 7463, epoch step 439 epoch 17
05/05/2022 19:00:06 - INFO - Distillation -   Running callback function...
05/05/2022 19:00:06 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 19:00:06 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 19:00:06 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 19:00:06 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 19:00:06 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 19:00:06 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 19:00:07 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 19:00:07 - INFO - __main__ -     Num examples = 3453
05/05/2022 19:00:07 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   2%|▏         | 7/432 [00:00<00:07, 60.24it/s]Evaluating:   3%|▎         | 14/432 [00:00<00:06, 59.75it/s]Evaluating:   5%|▍         | 20/432 [00:00<00:06, 59.52it/s]Evaluating:   6%|▌         | 26/432 [00:00<00:06, 59.26it/s]Evaluating:   7%|▋         | 32/432 [00:00<00:06, 59.16it/s]Evaluating:   9%|▉         | 38/432 [00:00<00:06, 59.11it/s]Evaluating:  10%|█         | 44/432 [00:00<00:06, 58.87it/s]Evaluating:  12%|█▏        | 50/432 [00:00<00:06, 58.60it/s]Evaluating:  13%|█▎        | 56/432 [00:00<00:06, 58.50it/s]Evaluating:  14%|█▍        | 62/432 [00:01<00:06, 58.39it/s]Evaluating:  16%|█▌        | 68/432 [00:01<00:06, 58.18it/s]Evaluating:  17%|█▋        | 74/432 [00:01<00:06, 57.92it/s]Evaluating:  19%|█▊        | 80/432 [00:01<00:06, 57.76it/s]Evaluating:  20%|█▉        | 86/432 [00:01<00:06, 57.59it/s]Evaluating:  21%|██▏       | 92/432 [00:01<00:05, 57.45it/s]Evaluating:  23%|██▎       | 98/432 [00:01<00:05, 57.33it/s]Evaluating:  24%|██▍       | 104/432 [00:01<00:05, 57.20it/s]Evaluating:  25%|██▌       | 110/432 [00:01<00:05, 57.05it/s]Evaluating:  27%|██▋       | 116/432 [00:01<00:05, 56.92it/s]Evaluating:  28%|██▊       | 122/432 [00:02<00:05, 56.75it/s]Evaluating:  30%|██▉       | 128/432 [00:02<00:05, 56.61it/s]Evaluating:  31%|███       | 134/432 [00:02<00:05, 56.35it/s]Evaluating:  32%|███▏      | 140/432 [00:02<00:05, 56.00it/s]Evaluating:  34%|███▍      | 146/432 [00:02<00:05, 55.81it/s]Evaluating:  35%|███▌      | 152/432 [00:02<00:05, 55.62it/s]Evaluating:  37%|███▋      | 158/432 [00:02<00:04, 55.48it/s]Evaluating:  38%|███▊      | 164/432 [00:02<00:04, 55.26it/s]Evaluating:  39%|███▉      | 170/432 [00:02<00:04, 55.04it/s]Evaluating:  41%|████      | 176/432 [00:03<00:04, 55.18it/s]Evaluating:  42%|████▏     | 182/432 [00:03<00:04, 55.10it/s]Evaluating:  44%|████▎     | 188/432 [00:03<00:04, 55.01it/s]Evaluating:  45%|████▍     | 194/432 [00:03<00:04, 54.70it/s]Evaluating:  46%|████▋     | 200/432 [00:03<00:04, 54.53it/s]Evaluating:  48%|████▊     | 206/432 [00:03<00:04, 54.28it/s]Evaluating:  49%|████▉     | 212/432 [00:03<00:04, 54.14it/s]Evaluating:  50%|█████     | 218/432 [00:03<00:03, 54.05it/s]Evaluating:  52%|█████▏    | 224/432 [00:03<00:03, 53.84it/s]Evaluating:  53%|█████▎    | 230/432 [00:04<00:03, 53.80it/s]Evaluating:  55%|█████▍    | 236/432 [00:04<00:03, 53.58it/s]Evaluating:  56%|█████▌    | 242/432 [00:04<00:03, 53.38it/s]Evaluating:  57%|█████▋    | 248/432 [00:04<00:03, 53.06it/s]Evaluating:  59%|█████▉    | 254/432 [00:04<00:03, 52.86it/s]Evaluating:  60%|██████    | 260/432 [00:04<00:03, 52.69it/s]Evaluating:  62%|██████▏   | 266/432 [00:04<00:03, 52.53it/s]Evaluating:  63%|██████▎   | 272/432 [00:04<00:03, 52.25it/s]Evaluating:  64%|██████▍   | 278/432 [00:04<00:02, 52.05it/s]Evaluating:  66%|██████▌   | 284/432 [00:05<00:02, 52.05it/s]Evaluating:  67%|██████▋   | 290/432 [00:05<00:02, 52.03it/s]Evaluating:  69%|██████▊   | 296/432 [00:05<00:02, 51.95it/s]Evaluating:  70%|██████▉   | 302/432 [00:05<00:02, 51.81it/s]Evaluating:  71%|███████▏  | 308/432 [00:05<00:02, 51.70it/s]Evaluating:  73%|███████▎  | 314/432 [00:05<00:02, 51.67it/s]Evaluating:  74%|███████▍  | 320/432 [00:05<00:02, 51.61it/s]Evaluating:  75%|███████▌  | 326/432 [00:05<00:02, 51.41it/s]Evaluating:  77%|███████▋  | 332/432 [00:06<00:01, 51.37it/s]Evaluating:  78%|███████▊  | 338/432 [00:06<00:01, 51.25it/s]Evaluating:  80%|███████▉  | 344/432 [00:06<00:01, 50.97it/s]Evaluating:  81%|████████  | 350/432 [00:06<00:01, 50.90it/s]Evaluating:  82%|████████▏ | 356/432 [00:06<00:01, 50.62it/s]Evaluating:  84%|████████▍ | 362/432 [00:06<00:01, 50.39it/s]Evaluating:  85%|████████▌ | 368/432 [00:06<00:01, 50.17it/s]Evaluating:  87%|████████▋ | 374/432 [00:06<00:01, 49.88it/s]Evaluating:  88%|████████▊ | 379/432 [00:06<00:01, 49.69it/s]Evaluating:  89%|████████▉ | 384/432 [00:07<00:00, 49.47it/s]Evaluating:  90%|█████████ | 389/432 [00:07<00:00, 49.34it/s]Evaluating:  91%|█████████ | 394/432 [00:07<00:00, 49.19it/s]Evaluating:  92%|█████████▏| 399/432 [00:07<00:00, 49.13it/s]Evaluating:  94%|█████████▎| 404/432 [00:07<00:00, 49.04it/s]Evaluating:  95%|█████████▍| 409/432 [00:07<00:00, 48.96it/s]Evaluating:  96%|█████████▌| 414/432 [00:07<00:00, 48.95it/s]Evaluating:  97%|█████████▋| 419/432 [00:07<00:00, 48.85it/s]Evaluating:  98%|█████████▊| 424/432 [00:07<00:00, 48.81it/s]Evaluating:  99%|█████████▉| 429/432 [00:08<00:00, 48.55it/s]Evaluating: 100%|██████████| 432/432 [00:08<00:00, 53.54it/s]
05/05/2022 19:00:16 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 19:00:16 - INFO - __main__ -     f1 = 0.9106450475854777
05/05/2022 19:00:16 - INFO - __main__ -     loss = 0.19886443419389893
05/05/2022 19:00:16 - INFO - __main__ -     precision = 0.9058555399719496
05/05/2022 19:00:16 - INFO - __main__ -     recall = 0.9154854712969526
05/05/2022 19:00:16 - INFO - Distillation -   Epoch 17 finished
05/05/2022 19:00:16 - INFO - Distillation -   Epoch 18
05/05/2022 19:00:16 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 19:00:19 - INFO - Distillation -   Global step: 7476, epoch step:13
05/05/2022 19:00:24 - INFO - Distillation -   Global step: 7497, epoch step:34
05/05/2022 19:00:30 - INFO - Distillation -   Global step: 7518, epoch step:55
05/05/2022 19:00:35 - INFO - Distillation -   Global step: 7539, epoch step:76
05/05/2022 19:00:41 - INFO - Distillation -   Global step: 7560, epoch step:97
05/05/2022 19:00:46 - INFO - Distillation -   Global step: 7581, epoch step:118
05/05/2022 19:00:52 - INFO - Distillation -   Global step: 7602, epoch step:139
05/05/2022 19:00:57 - INFO - Distillation -   Global step: 7623, epoch step:160
05/05/2022 19:01:03 - INFO - Distillation -   Global step: 7644, epoch step:181
05/05/2022 19:01:08 - INFO - Distillation -   Global step: 7665, epoch step:202
05/05/2022 19:01:14 - INFO - Distillation -   Global step: 7686, epoch step:223
05/05/2022 19:01:19 - INFO - Distillation -   Global step: 7707, epoch step:244
05/05/2022 19:01:25 - INFO - Distillation -   Global step: 7728, epoch step:265
05/05/2022 19:01:30 - INFO - Distillation -   Global step: 7749, epoch step:286
05/05/2022 19:01:36 - INFO - Distillation -   Global step: 7770, epoch step:307
05/05/2022 19:01:41 - INFO - Distillation -   Global step: 7791, epoch step:328
05/05/2022 19:01:47 - INFO - Distillation -   Global step: 7812, epoch step:349
05/05/2022 19:01:52 - INFO - Distillation -   Global step: 7833, epoch step:370
05/05/2022 19:01:58 - INFO - Distillation -   Global step: 7854, epoch step:391
05/05/2022 19:02:03 - INFO - Distillation -   Global step: 7875, epoch step:412
05/05/2022 19:02:08 - INFO - Distillation -   Global step: 7896, epoch step:433
05/05/2022 19:02:10 - INFO - Distillation -   Saving at global step 7902, epoch step 439 epoch 18
05/05/2022 19:02:11 - INFO - Distillation -   Running callback function...
05/05/2022 19:02:11 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 19:02:11 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 19:02:11 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 19:02:11 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 19:02:11 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 19:02:11 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 19:02:11 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 19:02:11 - INFO - __main__ -     Num examples = 3453
05/05/2022 19:02:11 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   2%|▏         | 7/432 [00:00<00:07, 60.05it/s]Evaluating:   3%|▎         | 14/432 [00:00<00:06, 59.93it/s]Evaluating:   5%|▍         | 20/432 [00:00<00:06, 59.83it/s]Evaluating:   6%|▌         | 26/432 [00:00<00:06, 59.75it/s]Evaluating:   7%|▋         | 32/432 [00:00<00:06, 59.42it/s]Evaluating:   9%|▉         | 38/432 [00:00<00:06, 59.16it/s]Evaluating:  10%|█         | 44/432 [00:00<00:06, 58.94it/s]Evaluating:  12%|█▏        | 50/432 [00:00<00:06, 58.76it/s]Evaluating:  13%|█▎        | 56/432 [00:00<00:06, 58.51it/s]Evaluating:  14%|█▍        | 62/432 [00:01<00:06, 58.42it/s]Evaluating:  16%|█▌        | 68/432 [00:01<00:06, 58.29it/s]Evaluating:  17%|█▋        | 74/432 [00:01<00:06, 58.16it/s]Evaluating:  19%|█▊        | 80/432 [00:01<00:06, 57.95it/s]Evaluating:  20%|█▉        | 86/432 [00:01<00:05, 57.68it/s]Evaluating:  21%|██▏       | 92/432 [00:01<00:05, 57.43it/s]Evaluating:  23%|██▎       | 98/432 [00:01<00:05, 57.33it/s]Evaluating:  24%|██▍       | 104/432 [00:01<00:05, 57.24it/s]Evaluating:  25%|██▌       | 110/432 [00:01<00:05, 57.22it/s]Evaluating:  27%|██▋       | 116/432 [00:01<00:05, 57.08it/s]Evaluating:  28%|██▊       | 122/432 [00:02<00:05, 56.94it/s]Evaluating:  30%|██▉       | 128/432 [00:02<00:05, 56.96it/s]Evaluating:  31%|███       | 134/432 [00:02<00:05, 56.94it/s]Evaluating:  32%|███▏      | 140/432 [00:02<00:05, 56.79it/s]Evaluating:  34%|███▍      | 146/432 [00:02<00:05, 56.41it/s]Evaluating:  35%|███▌      | 152/432 [00:02<00:04, 56.16it/s]Evaluating:  37%|███▋      | 158/432 [00:02<00:04, 55.94it/s]Evaluating:  38%|███▊      | 164/432 [00:02<00:04, 55.65it/s]Evaluating:  39%|███▉      | 170/432 [00:02<00:04, 55.22it/s]Evaluating:  41%|████      | 176/432 [00:03<00:04, 54.94it/s]Evaluating:  42%|████▏     | 182/432 [00:03<00:04, 54.80it/s]Evaluating:  44%|████▎     | 188/432 [00:03<00:04, 54.53it/s]Evaluating:  45%|████▍     | 194/432 [00:03<00:04, 54.37it/s]Evaluating:  46%|████▋     | 200/432 [00:03<00:04, 54.20it/s]Evaluating:  48%|████▊     | 206/432 [00:03<00:04, 54.01it/s]Evaluating:  49%|████▉     | 212/432 [00:03<00:04, 53.83it/s]Evaluating:  50%|█████     | 218/432 [00:03<00:03, 53.68it/s]Evaluating:  52%|█████▏    | 224/432 [00:03<00:03, 53.58it/s]Evaluating:  53%|█████▎    | 230/432 [00:04<00:03, 53.57it/s]Evaluating:  55%|█████▍    | 236/432 [00:04<00:03, 53.57it/s]Evaluating:  56%|█████▌    | 242/432 [00:04<00:03, 53.49it/s]Evaluating:  57%|█████▋    | 248/432 [00:04<00:03, 53.24it/s]Evaluating:  59%|█████▉    | 254/432 [00:04<00:03, 53.18it/s]Evaluating:  60%|██████    | 260/432 [00:04<00:03, 53.04it/s]Evaluating:  62%|██████▏   | 266/432 [00:04<00:03, 52.88it/s]Evaluating:  63%|██████▎   | 272/432 [00:04<00:03, 52.81it/s]Evaluating:  64%|██████▍   | 278/432 [00:04<00:02, 52.62it/s]Evaluating:  66%|██████▌   | 284/432 [00:05<00:02, 52.33it/s]Evaluating:  67%|██████▋   | 290/432 [00:05<00:02, 52.14it/s]Evaluating:  69%|██████▊   | 296/432 [00:05<00:02, 52.04it/s]Evaluating:  70%|██████▉   | 302/432 [00:05<00:02, 51.75it/s]Evaluating:  71%|███████▏  | 308/432 [00:05<00:02, 51.62it/s]Evaluating:  73%|███████▎  | 314/432 [00:05<00:02, 51.56it/s]Evaluating:  74%|███████▍  | 320/432 [00:05<00:02, 51.24it/s]Evaluating:  75%|███████▌  | 326/432 [00:05<00:02, 51.05it/s]Evaluating:  77%|███████▋  | 332/432 [00:06<00:01, 51.05it/s]Evaluating:  78%|███████▊  | 338/432 [00:06<00:01, 51.00it/s]Evaluating:  80%|███████▉  | 344/432 [00:06<00:01, 50.78it/s]Evaluating:  81%|████████  | 350/432 [00:06<00:01, 50.75it/s]Evaluating:  82%|████████▏ | 356/432 [00:06<00:01, 50.67it/s]Evaluating:  84%|████████▍ | 362/432 [00:06<00:01, 50.34it/s]Evaluating:  85%|████████▌ | 368/432 [00:06<00:01, 50.18it/s]Evaluating:  87%|████████▋ | 374/432 [00:06<00:01, 50.07it/s]Evaluating:  88%|████████▊ | 380/432 [00:06<00:01, 49.88it/s]Evaluating:  89%|████████▉ | 386/432 [00:07<00:00, 49.93it/s]Evaluating:  91%|█████████ | 391/432 [00:07<00:00, 49.90it/s]Evaluating:  92%|█████████▏| 396/432 [00:07<00:00, 49.58it/s]Evaluating:  93%|█████████▎| 401/432 [00:07<00:00, 49.24it/s]Evaluating:  94%|█████████▍| 406/432 [00:07<00:00, 49.02it/s]Evaluating:  95%|█████████▌| 411/432 [00:07<00:00, 48.81it/s]Evaluating:  96%|█████████▋| 416/432 [00:07<00:00, 48.65it/s]Evaluating:  97%|█████████▋| 421/432 [00:07<00:00, 48.50it/s]Evaluating:  99%|█████████▊| 426/432 [00:07<00:00, 48.31it/s]Evaluating: 100%|█████████▉| 431/432 [00:08<00:00, 48.23it/s]Evaluating: 100%|██████████| 432/432 [00:08<00:00, 53.58it/s]
05/05/2022 19:02:20 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 19:02:20 - INFO - __main__ -     f1 = 0.9082738567274649
05/05/2022 19:02:20 - INFO - __main__ -     loss = 0.20158943401721924
05/05/2022 19:02:20 - INFO - __main__ -     precision = 0.9034180543382997
05/05/2022 19:02:20 - INFO - __main__ -     recall = 0.91318214032601
05/05/2022 19:02:20 - INFO - Distillation -   Epoch 18 finished
05/05/2022 19:02:20 - INFO - Distillation -   Epoch 19
05/05/2022 19:02:20 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 19:02:24 - INFO - Distillation -   Global step: 7917, epoch step:15
05/05/2022 19:02:29 - INFO - Distillation -   Global step: 7938, epoch step:36
05/05/2022 19:02:35 - INFO - Distillation -   Global step: 7959, epoch step:57
05/05/2022 19:02:40 - INFO - Distillation -   Global step: 7980, epoch step:78
05/05/2022 19:02:46 - INFO - Distillation -   Global step: 8001, epoch step:99
05/05/2022 19:02:51 - INFO - Distillation -   Global step: 8022, epoch step:120
05/05/2022 19:02:56 - INFO - Distillation -   Global step: 8043, epoch step:141
05/05/2022 19:03:02 - INFO - Distillation -   Global step: 8064, epoch step:162
05/05/2022 19:03:07 - INFO - Distillation -   Global step: 8085, epoch step:183
05/05/2022 19:03:13 - INFO - Distillation -   Global step: 8106, epoch step:204
05/05/2022 19:03:18 - INFO - Distillation -   Global step: 8127, epoch step:225
05/05/2022 19:03:24 - INFO - Distillation -   Global step: 8148, epoch step:246
05/05/2022 19:03:29 - INFO - Distillation -   Global step: 8169, epoch step:267
05/05/2022 19:03:35 - INFO - Distillation -   Global step: 8190, epoch step:288
05/05/2022 19:03:40 - INFO - Distillation -   Global step: 8211, epoch step:309
05/05/2022 19:03:46 - INFO - Distillation -   Global step: 8232, epoch step:330
05/05/2022 19:03:51 - INFO - Distillation -   Global step: 8253, epoch step:351
05/05/2022 19:03:57 - INFO - Distillation -   Global step: 8274, epoch step:372
05/05/2022 19:04:02 - INFO - Distillation -   Global step: 8295, epoch step:393
05/05/2022 19:04:08 - INFO - Distillation -   Global step: 8316, epoch step:414
05/05/2022 19:04:13 - INFO - Distillation -   Global step: 8337, epoch step:435
05/05/2022 19:04:14 - INFO - Distillation -   Saving at global step 8341, epoch step 439 epoch 19
05/05/2022 19:04:15 - INFO - Distillation -   Running callback function...
05/05/2022 19:04:15 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 19:04:15 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 19:04:15 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 19:04:15 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 19:04:15 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 19:04:15 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 19:04:15 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 19:04:15 - INFO - __main__ -     Num examples = 3453
05/05/2022 19:04:15 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   1%|▏         | 6/432 [00:00<00:07, 59.58it/s]Evaluating:   3%|▎         | 12/432 [00:00<00:07, 59.55it/s]Evaluating:   4%|▍         | 18/432 [00:00<00:06, 59.53it/s]Evaluating:   6%|▌         | 24/432 [00:00<00:06, 59.45it/s]Evaluating:   7%|▋         | 30/432 [00:00<00:06, 59.36it/s]Evaluating:   8%|▊         | 36/432 [00:00<00:06, 59.25it/s]Evaluating:  10%|▉         | 42/432 [00:00<00:06, 59.13it/s]Evaluating:  11%|█         | 48/432 [00:00<00:06, 59.06it/s]Evaluating:  12%|█▎        | 54/432 [00:00<00:06, 58.81it/s]Evaluating:  14%|█▍        | 60/432 [00:01<00:06, 58.76it/s]Evaluating:  15%|█▌        | 66/432 [00:01<00:06, 58.60it/s]Evaluating:  17%|█▋        | 72/432 [00:01<00:06, 58.35it/s]Evaluating:  18%|█▊        | 78/432 [00:01<00:06, 58.21it/s]Evaluating:  19%|█▉        | 84/432 [00:01<00:05, 58.08it/s]Evaluating:  21%|██        | 90/432 [00:01<00:05, 57.87it/s]Evaluating:  22%|██▏       | 96/432 [00:01<00:05, 57.78it/s]Evaluating:  24%|██▎       | 102/432 [00:01<00:05, 57.83it/s]Evaluating:  25%|██▌       | 108/432 [00:01<00:05, 57.66it/s]Evaluating:  26%|██▋       | 114/432 [00:01<00:05, 57.53it/s]Evaluating:  28%|██▊       | 120/432 [00:02<00:05, 57.44it/s]Evaluating:  29%|██▉       | 126/432 [00:02<00:05, 57.19it/s]Evaluating:  31%|███       | 132/432 [00:02<00:05, 56.91it/s]Evaluating:  32%|███▏      | 138/432 [00:02<00:05, 56.74it/s]Evaluating:  33%|███▎      | 144/432 [00:02<00:05, 56.59it/s]Evaluating:  35%|███▍      | 150/432 [00:02<00:05, 56.36it/s]Evaluating:  36%|███▌      | 156/432 [00:02<00:04, 56.16it/s]Evaluating:  38%|███▊      | 162/432 [00:02<00:04, 56.00it/s]Evaluating:  39%|███▉      | 168/432 [00:02<00:04, 55.71it/s]Evaluating:  40%|████      | 174/432 [00:03<00:04, 55.55it/s]Evaluating:  42%|████▏     | 180/432 [00:03<00:04, 55.40it/s]Evaluating:  43%|████▎     | 186/432 [00:03<00:04, 55.10it/s]Evaluating:  44%|████▍     | 192/432 [00:03<00:04, 54.99it/s]Evaluating:  46%|████▌     | 198/432 [00:03<00:04, 54.96it/s]Evaluating:  47%|████▋     | 204/432 [00:03<00:04, 54.74it/s]Evaluating:  49%|████▊     | 210/432 [00:03<00:04, 54.42it/s]Evaluating:  50%|█████     | 216/432 [00:03<00:03, 54.22it/s]Evaluating:  51%|█████▏    | 222/432 [00:03<00:03, 54.07it/s]Evaluating:  53%|█████▎    | 228/432 [00:04<00:03, 53.98it/s]Evaluating:  54%|█████▍    | 234/432 [00:04<00:03, 53.74it/s]Evaluating:  56%|█████▌    | 240/432 [00:04<00:03, 53.38it/s]Evaluating:  57%|█████▋    | 246/432 [00:04<00:03, 53.21it/s]Evaluating:  58%|█████▊    | 252/432 [00:04<00:03, 52.95it/s]Evaluating:  60%|█████▉    | 258/432 [00:04<00:03, 52.84it/s]Evaluating:  61%|██████    | 264/432 [00:04<00:03, 52.75it/s]Evaluating:  62%|██████▎   | 270/432 [00:04<00:03, 52.74it/s]Evaluating:  64%|██████▍   | 276/432 [00:04<00:02, 52.62it/s]Evaluating:  65%|██████▌   | 282/432 [00:05<00:02, 52.48it/s]Evaluating:  67%|██████▋   | 288/432 [00:05<00:02, 52.24it/s]Evaluating:  68%|██████▊   | 294/432 [00:05<00:02, 51.93it/s]Evaluating:  69%|██████▉   | 300/432 [00:05<00:02, 51.81it/s]Evaluating:  71%|███████   | 306/432 [00:05<00:02, 51.69it/s]Evaluating:  72%|███████▏  | 312/432 [00:05<00:02, 51.48it/s]Evaluating:  74%|███████▎  | 318/432 [00:05<00:02, 51.38it/s]Evaluating:  75%|███████▌  | 324/432 [00:05<00:02, 51.16it/s]Evaluating:  76%|███████▋  | 330/432 [00:05<00:01, 51.04it/s]Evaluating:  78%|███████▊  | 336/432 [00:06<00:01, 51.02it/s]Evaluating:  79%|███████▉  | 342/432 [00:06<00:01, 50.92it/s]Evaluating:  81%|████████  | 348/432 [00:06<00:01, 50.73it/s]Evaluating:  82%|████████▏ | 354/432 [00:06<00:01, 50.41it/s]Evaluating:  83%|████████▎ | 360/432 [00:06<00:01, 50.26it/s]Evaluating:  85%|████████▍ | 366/432 [00:06<00:01, 50.08it/s]Evaluating:  86%|████████▌ | 372/432 [00:06<00:01, 49.98it/s]Evaluating:  87%|████████▋ | 377/432 [00:06<00:01, 49.84it/s]Evaluating:  88%|████████▊ | 382/432 [00:07<00:01, 49.76it/s]Evaluating:  90%|████████▉ | 387/432 [00:07<00:00, 49.62it/s]Evaluating:  91%|█████████ | 392/432 [00:07<00:00, 49.51it/s]Evaluating:  92%|█████████▏| 397/432 [00:07<00:00, 49.30it/s]Evaluating:  93%|█████████▎| 402/432 [00:07<00:00, 49.25it/s]Evaluating:  94%|█████████▍| 407/432 [00:07<00:00, 49.29it/s]Evaluating:  95%|█████████▌| 412/432 [00:07<00:00, 49.31it/s]Evaluating:  97%|█████████▋| 417/432 [00:07<00:00, 49.23it/s]Evaluating:  98%|█████████▊| 422/432 [00:07<00:00, 49.11it/s]Evaluating:  99%|█████████▉| 427/432 [00:07<00:00, 48.94it/s]Evaluating: 100%|██████████| 432/432 [00:08<00:00, 53.74it/s]
05/05/2022 19:04:24 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 19:04:24 - INFO - __main__ -     f1 = 0.9096677535912576
05/05/2022 19:04:24 - INFO - __main__ -     loss = 0.20049302543625178
05/05/2022 19:04:24 - INFO - __main__ -     precision = 0.9049623005435735
05/05/2022 19:04:24 - INFO - __main__ -     recall = 0.9144223954642098
05/05/2022 19:04:24 - INFO - Distillation -   Epoch 19 finished
05/05/2022 19:04:24 - INFO - Distillation -   Epoch 20
05/05/2022 19:04:24 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 19:04:28 - INFO - Distillation -   Global step: 8358, epoch step:17
05/05/2022 19:04:34 - INFO - Distillation -   Global step: 8379, epoch step:38
05/05/2022 19:04:39 - INFO - Distillation -   Global step: 8400, epoch step:59
05/05/2022 19:04:45 - INFO - Distillation -   Global step: 8421, epoch step:80
05/05/2022 19:04:50 - INFO - Distillation -   Global step: 8442, epoch step:101
05/05/2022 19:04:56 - INFO - Distillation -   Global step: 8463, epoch step:122
05/05/2022 19:05:01 - INFO - Distillation -   Global step: 8484, epoch step:143
05/05/2022 19:05:07 - INFO - Distillation -   Global step: 8505, epoch step:164
05/05/2022 19:05:12 - INFO - Distillation -   Global step: 8526, epoch step:185
05/05/2022 19:05:18 - INFO - Distillation -   Global step: 8547, epoch step:206
05/05/2022 19:05:23 - INFO - Distillation -   Global step: 8568, epoch step:227
05/05/2022 19:05:29 - INFO - Distillation -   Global step: 8589, epoch step:248
05/05/2022 19:05:34 - INFO - Distillation -   Global step: 8610, epoch step:269
05/05/2022 19:05:40 - INFO - Distillation -   Global step: 8631, epoch step:290
05/05/2022 19:05:45 - INFO - Distillation -   Global step: 8652, epoch step:311
05/05/2022 19:05:51 - INFO - Distillation -   Global step: 8673, epoch step:332
05/05/2022 19:05:56 - INFO - Distillation -   Global step: 8694, epoch step:353
05/05/2022 19:06:01 - INFO - Distillation -   Global step: 8715, epoch step:374
05/05/2022 19:06:07 - INFO - Distillation -   Global step: 8736, epoch step:395
05/05/2022 19:06:12 - INFO - Distillation -   Global step: 8757, epoch step:416
05/05/2022 19:06:18 - INFO - Distillation -   Global step: 8778, epoch step:437
05/05/2022 19:06:18 - INFO - Distillation -   Saving at global step 8780, epoch step 439 epoch 20
05/05/2022 19:06:19 - INFO - Distillation -   Running callback function...
05/05/2022 19:06:19 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 19:06:19 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 19:06:19 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 19:06:19 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 19:06:19 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 19:06:19 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 19:06:19 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 19:06:19 - INFO - __main__ -     Num examples = 3453
05/05/2022 19:06:19 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   2%|▏         | 7/432 [00:00<00:07, 60.35it/s]Evaluating:   3%|▎         | 14/432 [00:00<00:06, 60.04it/s]Evaluating:   5%|▍         | 21/432 [00:00<00:06, 59.87it/s]Evaluating:   6%|▋         | 27/432 [00:00<00:06, 59.77it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:06, 59.57it/s]Evaluating:   9%|▉         | 39/432 [00:00<00:06, 59.33it/s]Evaluating:  10%|█         | 45/432 [00:00<00:06, 59.12it/s]Evaluating:  12%|█▏        | 51/432 [00:00<00:06, 59.03it/s]Evaluating:  13%|█▎        | 57/432 [00:00<00:06, 58.87it/s]Evaluating:  15%|█▍        | 63/432 [00:01<00:06, 58.66it/s]Evaluating:  16%|█▌        | 69/432 [00:01<00:06, 58.52it/s]Evaluating:  17%|█▋        | 75/432 [00:01<00:06, 58.28it/s]Evaluating:  19%|█▉        | 81/432 [00:01<00:06, 58.12it/s]Evaluating:  20%|██        | 87/432 [00:01<00:05, 57.81it/s]Evaluating:  22%|██▏       | 93/432 [00:01<00:05, 57.73it/s]Evaluating:  23%|██▎       | 99/432 [00:01<00:05, 57.65it/s]Evaluating:  24%|██▍       | 105/432 [00:01<00:05, 57.72it/s]Evaluating:  26%|██▌       | 111/432 [00:01<00:05, 57.61it/s]Evaluating:  27%|██▋       | 117/432 [00:02<00:05, 57.53it/s]Evaluating:  28%|██▊       | 123/432 [00:02<00:05, 57.16it/s]Evaluating:  30%|██▉       | 129/432 [00:02<00:05, 56.92it/s]Evaluating:  31%|███▏      | 135/432 [00:02<00:05, 56.68it/s]Evaluating:  33%|███▎      | 141/432 [00:02<00:05, 56.39it/s]Evaluating:  34%|███▍      | 147/432 [00:02<00:05, 56.20it/s]Evaluating:  35%|███▌      | 153/432 [00:02<00:04, 55.92it/s]Evaluating:  37%|███▋      | 159/432 [00:02<00:04, 55.75it/s]Evaluating:  38%|███▊      | 165/432 [00:02<00:04, 55.62it/s]Evaluating:  40%|███▉      | 171/432 [00:02<00:04, 55.34it/s]Evaluating:  41%|████      | 177/432 [00:03<00:04, 55.15it/s]Evaluating:  42%|████▏     | 183/432 [00:03<00:04, 55.04it/s]Evaluating:  44%|████▍     | 189/432 [00:03<00:04, 55.03it/s]Evaluating:  45%|████▌     | 195/432 [00:03<00:04, 54.86it/s]Evaluating:  47%|████▋     | 201/432 [00:03<00:04, 54.62it/s]Evaluating:  48%|████▊     | 207/432 [00:03<00:04, 54.41it/s]Evaluating:  49%|████▉     | 213/432 [00:03<00:04, 54.24it/s]Evaluating:  51%|█████     | 219/432 [00:03<00:03, 54.14it/s]Evaluating:  52%|█████▏    | 225/432 [00:03<00:03, 53.75it/s]Evaluating:  53%|█████▎    | 231/432 [00:04<00:03, 53.45it/s]Evaluating:  55%|█████▍    | 237/432 [00:04<00:03, 53.17it/s]Evaluating:  56%|█████▋    | 243/432 [00:04<00:03, 52.94it/s]Evaluating:  58%|█████▊    | 249/432 [00:04<00:03, 52.69it/s]Evaluating:  59%|█████▉    | 255/432 [00:04<00:03, 52.71it/s]Evaluating:  60%|██████    | 261/432 [00:04<00:03, 52.81it/s]Evaluating:  62%|██████▏   | 267/432 [00:04<00:03, 52.76it/s]Evaluating:  63%|██████▎   | 273/432 [00:04<00:03, 52.62it/s]Evaluating:  65%|██████▍   | 279/432 [00:04<00:02, 52.47it/s]Evaluating:  66%|██████▌   | 285/432 [00:05<00:02, 52.18it/s]Evaluating:  67%|██████▋   | 291/432 [00:05<00:02, 51.93it/s]Evaluating:  69%|██████▉   | 297/432 [00:05<00:02, 51.82it/s]Evaluating:  70%|███████   | 303/432 [00:05<00:02, 51.61it/s]Evaluating:  72%|███████▏  | 309/432 [00:05<00:02, 51.48it/s]Evaluating:  73%|███████▎  | 315/432 [00:05<00:02, 51.37it/s]Evaluating:  74%|███████▍  | 321/432 [00:05<00:02, 51.27it/s]Evaluating:  76%|███████▌  | 327/432 [00:05<00:02, 51.16it/s]Evaluating:  77%|███████▋  | 333/432 [00:06<00:01, 51.10it/s]Evaluating:  78%|███████▊  | 339/432 [00:06<00:01, 51.02it/s]Evaluating:  80%|███████▉  | 345/432 [00:06<00:01, 51.00it/s]Evaluating:  81%|████████▏ | 351/432 [00:06<00:01, 50.86it/s]Evaluating:  83%|████████▎ | 357/432 [00:06<00:01, 50.57it/s]Evaluating:  84%|████████▍ | 363/432 [00:06<00:01, 50.45it/s]Evaluating:  85%|████████▌ | 369/432 [00:06<00:01, 50.29it/s]Evaluating:  87%|████████▋ | 375/432 [00:06<00:01, 50.13it/s]Evaluating:  88%|████████▊ | 381/432 [00:07<00:01, 49.94it/s]Evaluating:  89%|████████▉ | 386/432 [00:07<00:00, 49.83it/s]Evaluating:  91%|█████████ | 391/432 [00:07<00:00, 49.73it/s]Evaluating:  92%|█████████▏| 396/432 [00:07<00:00, 49.67it/s]Evaluating:  93%|█████████▎| 401/432 [00:07<00:00, 49.66it/s]Evaluating:  94%|█████████▍| 406/432 [00:07<00:00, 49.30it/s]Evaluating:  95%|█████████▌| 411/432 [00:07<00:00, 49.09it/s]Evaluating:  96%|█████████▋| 416/432 [00:07<00:00, 48.88it/s]Evaluating:  97%|█████████▋| 421/432 [00:07<00:00, 48.75it/s]Evaluating:  99%|█████████▊| 426/432 [00:07<00:00, 48.60it/s]Evaluating: 100%|█████████▉| 431/432 [00:08<00:00, 48.60it/s]Evaluating: 100%|██████████| 432/432 [00:08<00:00, 53.69it/s]
05/05/2022 19:06:28 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 19:06:28 - INFO - __main__ -     f1 = 0.909924202362066
05/05/2022 19:06:28 - INFO - __main__ -     loss = 0.2002596426620155
05/05/2022 19:06:28 - INFO - __main__ -     precision = 0.90529638723255
05/05/2022 19:06:28 - INFO - __main__ -     recall = 0.9145995747696669
05/05/2022 19:06:28 - INFO - Distillation -   Epoch 20 finished
05/05/2022 19:06:28 - INFO - __main__ -   Saving model checkpoint to /home/hs3228/TextBrewer/output_model/conll_distill/student_layer9
05/05/2022 19:06:28 - INFO - transformers.configuration_utils -   Configuration saved in /home/hs3228/TextBrewer/output_model/conll_distill/student_layer9/config.json
05/05/2022 19:06:29 - INFO - transformers.modeling_utils -   Model weights saved in /home/hs3228/TextBrewer/output_model/conll_distill/student_layer9/pytorch_model.bin
05/05/2022 19:06:29 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/student_layer9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/student_layer9' is a path or url to a directory containing tokenizer files.
05/05/2022 19:06:29 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/student_layer9/vocab.txt
05/05/2022 19:06:29 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/student_layer9/added_tokens.json
05/05/2022 19:06:29 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/student_layer9/special_tokens_map.json
05/05/2022 19:06:29 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/student_layer9/tokenizer_config.json
05/05/2022 19:06:29 - INFO - __main__ -   Evaluate the following checkpoints: ['/home/hs3228/TextBrewer/output_model/conll_distill/student_layer9']
05/05/2022 19:06:29 - INFO - transformers.configuration_utils -   loading configuration file /home/hs3228/TextBrewer/output_model/conll_distill/student_layer9/config.json
05/05/2022 19:06:29 - INFO - transformers.configuration_utils -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 9,
  "num_labels": 9,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

05/05/2022 19:06:29 - INFO - transformers.modeling_utils -   loading weights file /home/hs3228/TextBrewer/output_model/conll_distill/student_layer9/pytorch_model.bin
05/05/2022 19:06:31 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_dev_teacher_128
05/05/2022 19:06:31 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 19:06:31 - INFO - __main__ -     Num examples = 3250
05/05/2022 19:06:31 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/407 [00:00<?, ?it/s]Evaluating:   1%|▏         | 6/407 [00:00<00:07, 53.89it/s]Evaluating:   3%|▎         | 12/407 [00:00<00:07, 55.32it/s]Evaluating:   4%|▍         | 18/407 [00:00<00:06, 56.70it/s]Evaluating:   6%|▌         | 24/407 [00:00<00:06, 57.56it/s]Evaluating:   7%|▋         | 30/407 [00:00<00:06, 58.18it/s]Evaluating:   9%|▉         | 36/407 [00:00<00:06, 58.60it/s]Evaluating:  10%|█         | 42/407 [00:00<00:06, 58.62it/s]Evaluating:  12%|█▏        | 48/407 [00:00<00:06, 58.74it/s]Evaluating:  13%|█▎        | 54/407 [00:00<00:06, 58.72it/s]Evaluating:  15%|█▍        | 60/407 [00:01<00:05, 58.75it/s]Evaluating:  16%|█▌        | 66/407 [00:01<00:05, 58.76it/s]Evaluating:  18%|█▊        | 72/407 [00:01<00:05, 58.58it/s]Evaluating:  19%|█▉        | 78/407 [00:01<00:05, 58.38it/s]Evaluating:  21%|██        | 84/407 [00:01<00:05, 58.25it/s]Evaluating:  22%|██▏       | 90/407 [00:01<00:05, 58.05it/s]Evaluating:  24%|██▎       | 96/407 [00:01<00:05, 57.94it/s]Evaluating:  25%|██▌       | 102/407 [00:01<00:05, 57.80it/s]Evaluating:  27%|██▋       | 108/407 [00:01<00:05, 57.75it/s]Evaluating:  28%|██▊       | 114/407 [00:01<00:05, 57.63it/s]Evaluating:  29%|██▉       | 120/407 [00:02<00:05, 57.24it/s]Evaluating:  31%|███       | 126/407 [00:02<00:04, 57.03it/s]Evaluating:  32%|███▏      | 132/407 [00:02<00:04, 56.83it/s]Evaluating:  34%|███▍      | 138/407 [00:02<00:04, 56.58it/s]Evaluating:  35%|███▌      | 144/407 [00:02<00:04, 56.44it/s]Evaluating:  37%|███▋      | 150/407 [00:02<00:04, 56.41it/s]Evaluating:  38%|███▊      | 156/407 [00:02<00:04, 56.25it/s]Evaluating:  40%|███▉      | 162/407 [00:02<00:04, 55.98it/s]Evaluating:  41%|████▏     | 168/407 [00:02<00:04, 55.95it/s]Evaluating:  43%|████▎     | 174/407 [00:03<00:04, 55.64it/s]Evaluating:  44%|████▍     | 180/407 [00:03<00:04, 55.70it/s]Evaluating:  46%|████▌     | 186/407 [00:03<00:03, 55.69it/s]Evaluating:  47%|████▋     | 192/407 [00:03<00:03, 55.58it/s]Evaluating:  49%|████▊     | 198/407 [00:03<00:03, 55.44it/s]Evaluating:  50%|█████     | 204/407 [00:03<00:03, 55.30it/s]Evaluating:  52%|█████▏    | 210/407 [00:03<00:03, 54.99it/s]Evaluating:  53%|█████▎    | 216/407 [00:03<00:03, 54.67it/s]Evaluating:  55%|█████▍    | 222/407 [00:03<00:03, 54.56it/s]Evaluating:  56%|█████▌    | 228/407 [00:04<00:03, 54.34it/s]Evaluating:  57%|█████▋    | 234/407 [00:04<00:03, 54.15it/s]Evaluating:  59%|█████▉    | 240/407 [00:04<00:03, 53.92it/s]Evaluating:  60%|██████    | 246/407 [00:04<00:02, 53.81it/s]Evaluating:  62%|██████▏   | 252/407 [00:04<00:02, 53.54it/s]Evaluating:  63%|██████▎   | 258/407 [00:04<00:02, 52.45it/s]Evaluating:  65%|██████▍   | 264/407 [00:04<00:02, 52.18it/s]Evaluating:  66%|██████▋   | 270/407 [00:04<00:02, 52.25it/s]Evaluating:  68%|██████▊   | 276/407 [00:04<00:02, 52.42it/s]Evaluating:  69%|██████▉   | 282/407 [00:05<00:02, 52.38it/s]Evaluating:  71%|███████   | 288/407 [00:05<00:02, 52.26it/s]Evaluating:  72%|███████▏  | 294/407 [00:05<00:02, 52.20it/s]Evaluating:  74%|███████▎  | 300/407 [00:05<00:02, 52.12it/s]Evaluating:  75%|███████▌  | 306/407 [00:05<00:01, 52.01it/s]Evaluating:  77%|███████▋  | 312/407 [00:05<00:01, 51.87it/s]Evaluating:  78%|███████▊  | 318/407 [00:05<00:01, 51.75it/s]Evaluating:  80%|███████▉  | 324/407 [00:05<00:01, 51.60it/s]Evaluating:  81%|████████  | 330/407 [00:05<00:01, 51.20it/s]Evaluating:  83%|████████▎ | 336/407 [00:06<00:01, 51.02it/s]Evaluating:  84%|████████▍ | 342/407 [00:06<00:01, 51.05it/s]Evaluating:  86%|████████▌ | 348/407 [00:06<00:01, 51.01it/s]Evaluating:  87%|████████▋ | 354/407 [00:06<00:01, 50.96it/s]Evaluating:  88%|████████▊ | 360/407 [00:06<00:00, 50.69it/s]Evaluating:  90%|████████▉ | 366/407 [00:06<00:00, 50.58it/s]Evaluating:  91%|█████████▏| 372/407 [00:06<00:00, 50.37it/s]Evaluating:  93%|█████████▎| 378/407 [00:06<00:00, 50.19it/s]Evaluating:  94%|█████████▍| 384/407 [00:07<00:00, 49.95it/s]Evaluating:  96%|█████████▌| 389/407 [00:07<00:00, 49.79it/s]Evaluating:  97%|█████████▋| 394/407 [00:07<00:00, 49.66it/s]Evaluating:  98%|█████████▊| 399/407 [00:07<00:00, 49.48it/s]Evaluating:  99%|█████████▉| 404/407 [00:07<00:00, 49.36it/s]Evaluating: 100%|██████████| 407/407 [00:07<00:00, 54.15it/s]
05/05/2022 19:06:40 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 19:06:40 - INFO - __main__ -     f1 = 0.9466072479609854
05/05/2022 19:06:40 - INFO - __main__ -     loss = 0.07427320696074197
05/05/2022 19:06:40 - INFO - __main__ -     precision = 0.9449387275474233
05/05/2022 19:06:40 - INFO - __main__ -     recall = 0.9482816711590296
05/05/2022 19:06:40 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/student_layer9' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/student_layer9' is a path or url to a directory containing tokenizer files.
05/05/2022 19:06:40 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/student_layer9/vocab.txt
05/05/2022 19:06:40 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/student_layer9/added_tokens.json
05/05/2022 19:06:40 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/student_layer9/special_tokens_map.json
05/05/2022 19:06:40 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/student_layer9/tokenizer_config.json
05/05/2022 19:06:40 - INFO - transformers.configuration_utils -   loading configuration file /home/hs3228/TextBrewer/output_model/conll_distill/student_layer9/config.json
05/05/2022 19:06:40 - INFO - transformers.configuration_utils -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 9,
  "num_labels": 9,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

05/05/2022 19:06:40 - INFO - transformers.modeling_utils -   loading weights file /home/hs3228/TextBrewer/output_model/conll_distill/student_layer9/pytorch_model.bin
05/05/2022 19:06:42 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 19:06:42 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 19:06:42 - INFO - __main__ -     Num examples = 3453
05/05/2022 19:06:42 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   2%|▏         | 7/432 [00:00<00:07, 60.34it/s]Evaluating:   3%|▎         | 14/432 [00:00<00:06, 60.07it/s]Evaluating:   5%|▍         | 21/432 [00:00<00:06, 59.81it/s]Evaluating:   6%|▋         | 27/432 [00:00<00:06, 59.72it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:06, 59.64it/s]Evaluating:   9%|▉         | 39/432 [00:00<00:06, 59.58it/s]Evaluating:  10%|█         | 45/432 [00:00<00:06, 59.39it/s]Evaluating:  12%|█▏        | 51/432 [00:00<00:06, 59.21it/s]Evaluating:  13%|█▎        | 57/432 [00:00<00:06, 59.09it/s]Evaluating:  15%|█▍        | 63/432 [00:01<00:06, 58.79it/s]Evaluating:  16%|█▌        | 69/432 [00:01<00:06, 58.59it/s]Evaluating:  17%|█▋        | 75/432 [00:01<00:06, 58.39it/s]Evaluating:  19%|█▉        | 81/432 [00:01<00:06, 58.33it/s]Evaluating:  20%|██        | 87/432 [00:01<00:05, 58.24it/s]Evaluating:  22%|██▏       | 93/432 [00:01<00:05, 58.15it/s]Evaluating:  23%|██▎       | 99/432 [00:01<00:05, 57.95it/s]Evaluating:  24%|██▍       | 105/432 [00:01<00:05, 57.76it/s]Evaluating:  26%|██▌       | 111/432 [00:01<00:05, 57.58it/s]Evaluating:  27%|██▋       | 117/432 [00:01<00:05, 57.50it/s]Evaluating:  28%|██▊       | 123/432 [00:02<00:05, 57.27it/s]Evaluating:  30%|██▉       | 129/432 [00:02<00:05, 57.00it/s]Evaluating:  31%|███▏      | 135/432 [00:02<00:05, 56.83it/s]Evaluating:  33%|███▎      | 141/432 [00:02<00:05, 56.70it/s]Evaluating:  34%|███▍      | 147/432 [00:02<00:05, 56.22it/s]Evaluating:  35%|███▌      | 153/432 [00:02<00:04, 56.20it/s]Evaluating:  37%|███▋      | 159/432 [00:02<00:04, 56.12it/s]Evaluating:  38%|███▊      | 165/432 [00:02<00:04, 56.21it/s]Evaluating:  40%|███▉      | 171/432 [00:02<00:04, 56.20it/s]Evaluating:  41%|████      | 177/432 [00:03<00:04, 56.01it/s]Evaluating:  42%|████▏     | 183/432 [00:03<00:04, 55.81it/s]Evaluating:  44%|████▍     | 189/432 [00:03<00:04, 55.46it/s]Evaluating:  45%|████▌     | 195/432 [00:03<00:04, 55.24it/s]Evaluating:  47%|████▋     | 201/432 [00:03<00:04, 54.78it/s]Evaluating:  48%|████▊     | 207/432 [00:03<00:04, 54.68it/s]Evaluating:  49%|████▉     | 213/432 [00:03<00:04, 54.54it/s]Evaluating:  51%|█████     | 219/432 [00:03<00:03, 54.63it/s]Evaluating:  52%|█████▏    | 225/432 [00:03<00:03, 54.23it/s]Evaluating:  53%|█████▎    | 231/432 [00:04<00:03, 54.09it/s]Evaluating:  55%|█████▍    | 237/432 [00:04<00:03, 54.04it/s]Evaluating:  56%|█████▋    | 243/432 [00:04<00:03, 53.79it/s]Evaluating:  58%|█████▊    | 249/432 [00:04<00:03, 53.78it/s]Evaluating:  59%|█████▉    | 255/432 [00:04<00:03, 53.61it/s]Evaluating:  60%|██████    | 261/432 [00:04<00:03, 53.43it/s]Evaluating:  62%|██████▏   | 267/432 [00:04<00:03, 53.29it/s]Evaluating:  63%|██████▎   | 273/432 [00:04<00:02, 53.17it/s]Evaluating:  65%|██████▍   | 279/432 [00:04<00:02, 52.99it/s]Evaluating:  66%|██████▌   | 285/432 [00:05<00:02, 52.88it/s]Evaluating:  67%|██████▋   | 291/432 [00:05<00:02, 52.67it/s]Evaluating:  69%|██████▉   | 297/432 [00:05<00:02, 52.47it/s]Evaluating:  70%|███████   | 303/432 [00:05<00:02, 52.20it/s]Evaluating:  72%|███████▏  | 309/432 [00:05<00:02, 52.08it/s]Evaluating:  73%|███████▎  | 315/432 [00:05<00:02, 52.06it/s]Evaluating:  74%|███████▍  | 321/432 [00:05<00:02, 51.99it/s]Evaluating:  76%|███████▌  | 327/432 [00:05<00:02, 51.87it/s]Evaluating:  77%|███████▋  | 333/432 [00:06<00:01, 51.71it/s]Evaluating:  78%|███████▊  | 339/432 [00:06<00:01, 51.51it/s]Evaluating:  80%|███████▉  | 345/432 [00:06<00:01, 51.33it/s]Evaluating:  81%|████████▏ | 351/432 [00:06<00:01, 51.12it/s]Evaluating:  83%|████████▎ | 357/432 [00:06<00:01, 50.96it/s]Evaluating:  84%|████████▍ | 363/432 [00:06<00:01, 50.70it/s]Evaluating:  85%|████████▌ | 369/432 [00:06<00:01, 50.54it/s]Evaluating:  87%|████████▋ | 375/432 [00:06<00:01, 50.47it/s]Evaluating:  88%|████████▊ | 381/432 [00:06<00:01, 50.38it/s]Evaluating:  90%|████████▉ | 387/432 [00:07<00:00, 50.35it/s]Evaluating:  91%|█████████ | 393/432 [00:07<00:00, 50.24it/s]Evaluating:  92%|█████████▏| 399/432 [00:07<00:00, 50.13it/s]Evaluating:  94%|█████████▍| 405/432 [00:07<00:00, 49.91it/s]Evaluating:  95%|█████████▍| 410/432 [00:07<00:00, 49.66it/s]Evaluating:  96%|█████████▌| 415/432 [00:07<00:00, 49.51it/s]Evaluating:  97%|█████████▋| 420/432 [00:07<00:00, 49.47it/s]Evaluating:  98%|█████████▊| 425/432 [00:07<00:00, 49.39it/s]Evaluating: 100%|█████████▉| 430/432 [00:07<00:00, 49.23it/s]Evaluating: 100%|██████████| 432/432 [00:07<00:00, 54.13it/s]
05/05/2022 19:06:51 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 19:06:51 - INFO - __main__ -     f1 = 0.909924202362066
05/05/2022 19:06:51 - INFO - __main__ -     loss = 0.2002596426620155
05/05/2022 19:06:51 - INFO - __main__ -     precision = 0.90529638723255
05/05/2022 19:06:51 - INFO - __main__ -     recall = 0.9145995747696669
05/05/2022 19:06:51 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for '.'.
05/05/2022 19:06:51 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for '2'.
05/05/2022 19:06:51 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for '-'.
05/05/2022 19:06:51 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for 'Norm'.
05/05/2022 19:06:51 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for 'Hewitt'.
05/05/2022 19:06:51 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for '('.
05/05/2022 19:06:51 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for 'New'.
05/05/2022 19:06:51 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for 'Zealand'.
05/05/2022 19:06:51 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for ')'.
05/05/2022 19:06:51 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for ','.
05/05/2022 19:06:51 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for '1'.
05/05/2022 19:06:51 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for '-'.
05/05/2022 19:06:51 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for 'Nick'.
05/05/2022 19:06:51 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for 'Popplewell'.
05/05/2022 19:06:51 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for '('.
05/05/2022 19:06:51 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for 'Ireland'.
05/05/2022 19:06:51 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for ')'.
05/05/2022 19:06:51 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for '.'.
