nohup: ignoring input
2022/05/06 14:43:05 - INFO - pytorch_pretrained_bert.modeling -  Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
2022/05/06 14:43:05 - INFO - pytorch_pretrained_bert.my_modeling -  Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
2022/05/06 14:43:05 - INFO - Main -  vocab_file:/home/hs3228/TextBrewer/model/chinese_roberta_wwm_ext_pytorch/vocab.txt
2022/05/06 14:43:05 - INFO - Main -  output_dir:/home/hs3228/TextBrewer/output_model/cmrc/student/cmrc2018_t20_TbaseST3_AllSmmdH1_lr15e10_opt1
2022/05/06 14:43:05 - INFO - Main -  train_file:/home/hs3228/TextBrewer/data/cmrc2018/squad-style-data/cmrc2018_train.json
2022/05/06 14:43:05 - INFO - Main -  predict_file:/home/hs3228/TextBrewer/data/cmrc2018/squad-style-data/cmrc2018_dev.json
2022/05/06 14:43:05 - INFO - Main -  do_lower_case:True
2022/05/06 14:43:05 - INFO - Main -  max_seq_length:512
2022/05/06 14:43:05 - INFO - Main -  doc_stride:128
2022/05/06 14:43:05 - INFO - Main -  max_query_length:64
2022/05/06 14:43:05 - INFO - Main -  do_train:True
2022/05/06 14:43:05 - INFO - Main -  do_predict:True
2022/05/06 14:43:05 - INFO - Main -  train_batch_size:12
2022/05/06 14:43:05 - INFO - Main -  predict_batch_size:8
2022/05/06 14:43:05 - INFO - Main -  learning_rate:0.00015
2022/05/06 14:43:05 - INFO - Main -  num_train_epochs:10.0
2022/05/06 14:43:05 - INFO - Main -  warmup_proportion:0.1
2022/05/06 14:43:05 - INFO - Main -  n_best_size:20
2022/05/06 14:43:05 - INFO - Main -  max_answer_length:30
2022/05/06 14:43:05 - INFO - Main -  verbose_logging:False
2022/05/06 14:43:05 - INFO - Main -  no_cuda:False
2022/05/06 14:43:05 - INFO - Main -  gradient_accumulation_steps:1
2022/05/06 14:43:05 - INFO - Main -  local_rank:-1
2022/05/06 14:43:05 - INFO - Main -  fp16:False
2022/05/06 14:43:05 - INFO - Main -  random_seed:9580
2022/05/06 14:43:05 - INFO - Main -  fake_file_1:/home/hs3228/TextBrewer/data/DRCD/DRCD_training.json
2022/05/06 14:43:05 - INFO - Main -  fake_file_2:None
2022/05/06 14:43:05 - INFO - Main -  load_model_type:bert
2022/05/06 14:43:05 - INFO - Main -  weight_decay_rate:0.01
2022/05/06 14:43:05 - INFO - Main -  do_eval:True
2022/05/06 14:43:05 - INFO - Main -  PRINT_EVERY:200
2022/05/06 14:43:05 - INFO - Main -  weight:1.0
2022/05/06 14:43:05 - INFO - Main -  ckpt_frequency:1
2022/05/06 14:43:05 - INFO - Main -  tuned_checkpoint_T:/home/hs3228/TextBrewer/output_model/cmrc/teacher/cmrc2018_base_lr3e10_teacher/gs885.pkl
2022/05/06 14:43:05 - INFO - Main -  tuned_checkpoint_S:None
2022/05/06 14:43:05 - INFO - Main -  init_checkpoint_S:/home/hs3228/TextBrewer/model/chinese_roberta_wwm_ext_pytorch/pytorch_model.bin
2022/05/06 14:43:05 - INFO - Main -  bert_config_file_T:/home/hs3228/TextBrewer/model/chinese_roberta_wwm_ext_pytorch/bert_config.json
2022/05/06 14:43:05 - INFO - Main -  bert_config_file_S:../student_config/roberta_wwm_config/bert_config_L3.json
2022/05/06 14:43:05 - INFO - Main -  temperature:20.0
2022/05/06 14:43:05 - INFO - Main -  teacher_cached:False
2022/05/06 14:43:05 - INFO - Main -  s_opt1:1.0
2022/05/06 14:43:05 - INFO - Main -  s_opt2:0.0
2022/05/06 14:43:05 - INFO - Main -  s_opt3:1.0
2022/05/06 14:43:05 - INFO - Main -  schedule:slanted_triangular
2022/05/06 14:43:05 - INFO - Main -  null_score_diff_threshold:99.0
2022/05/06 14:43:05 - INFO - Main -  tag:RB
2022/05/06 14:43:05 - INFO - Main -  no_inputs_mask:False
2022/05/06 14:43:05 - INFO - Main -  no_logits:False
2022/05/06 14:43:05 - INFO - Main -  output_att_score:true
2022/05/06 14:43:05 - INFO - Main -  output_att_sum:false
2022/05/06 14:43:05 - INFO - Main -  output_encoded_layers:true
2022/05/06 14:43:05 - INFO - Main -  output_attention_layers:true
2022/05/06 14:43:05 - INFO - Main -  matches:['L3_hidden_mse', 'L3_hidden_smmd']
2022/05/06 14:43:06 - INFO - Main -  device cuda n_gpu 1 distributed training False
2022/05/06 14:43:06 - INFO - utils -  Loading dataset cmrc2018_train.json128_l512_cHA.tRB.pkl 
2022/05/06 14:43:10 - INFO - utils -  Loading dataset DRCD_training.json128_l512_cHA.tRB.pkl 
2022/05/06 14:43:22 - INFO - utils -  Loading dataset cmrc2018_dev.json128_l512_cHA.tRB.pkl 
2022/05/06 14:43:32 - INFO - Main -  Length of all_trainable_params: 2
2022/05/06 14:43:32 - INFO - Main -  ***** Running training *****
2022/05/06 14:43:32 - INFO - Main -    Num orig examples = 37078
2022/05/06 14:43:32 - INFO - Main -    Num split examples = 43945
2022/05/06 14:43:32 - INFO - Main -    Forward batch size = 12
2022/05/06 14:43:32 - INFO - Main -    Num backward steps = 36620
2022/05/06 14:43:32 - INFO - Main -  [{'layer_T': 0, 'layer_S': 0, 'feature': 'hidden', 'loss': 'hidden_mse', 'weight': 1}, {'layer_T': 4, 'layer_S': 1, 'feature': 'hidden', 'loss': 'hidden_mse', 'weight': 1}, {'layer_T': 8, 'layer_S': 2, 'feature': 'hidden', 'loss': 'hidden_mse', 'weight': 1}, {'layer_T': 12, 'layer_S': 3, 'feature': 'hidden', 'loss': 'hidden_mse', 'weight': 1}, {'layer_T': [0, 0], 'layer_S': [0, 0], 'feature': 'hidden', 'loss': 'mmd', 'weight': 1}, {'layer_T': [4, 4], 'layer_S': [1, 1], 'feature': 'hidden', 'loss': 'mmd', 'weight': 1}, {'layer_T': [8, 8], 'layer_S': [2, 2], 'feature': 'hidden', 'loss': 'mmd', 'weight': 1}, {'layer_T': [12, 12], 'layer_S': [3, 3], 'feature': 'hidden', 'loss': 'mmd', 'weight': 1}]
2022/05/06 14:43:34 - INFO - Distillation -  Training steps per epoch: 3662
2022/05/06 14:43:34 - INFO - Distillation -  Checkpoints(step): [0]
2022/05/06 14:43:34 - INFO - Distillation -  Epoch 1
2022/05/06 14:43:34 - INFO - Distillation -  Length of current epoch in forward batch: 3662
/home/hs3228/TextBrewer/examples/cmrc2018_example/optimization.py:181: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  next_m.mul_(beta1).add_(1 - beta1, grad)
2022/05/06 14:44:24 - INFO - Distillation -  Global step: 183, epoch step:183
2022/05/06 14:45:13 - INFO - Distillation -  Global step: 366, epoch step:366
2022/05/06 14:46:02 - INFO - Distillation -  Global step: 549, epoch step:549
2022/05/06 14:46:52 - INFO - Distillation -  Global step: 732, epoch step:732
2022/05/06 14:47:41 - INFO - Distillation -  Global step: 915, epoch step:915
2022/05/06 14:48:30 - INFO - Distillation -  Global step: 1098, epoch step:1098
2022/05/06 14:49:20 - INFO - Distillation -  Global step: 1281, epoch step:1281
2022/05/06 14:50:09 - INFO - Distillation -  Global step: 1464, epoch step:1464
2022/05/06 14:50:59 - INFO - Distillation -  Global step: 1647, epoch step:1647
2022/05/06 14:51:48 - INFO - Distillation -  Global step: 1830, epoch step:1830
2022/05/06 14:52:37 - INFO - Distillation -  Global step: 2013, epoch step:2013
2022/05/06 14:53:27 - INFO - Distillation -  Global step: 2196, epoch step:2196
2022/05/06 14:54:16 - INFO - Distillation -  Global step: 2379, epoch step:2379
2022/05/06 14:55:06 - INFO - Distillation -  Global step: 2562, epoch step:2562
2022/05/06 14:55:55 - INFO - Distillation -  Global step: 2745, epoch step:2745
2022/05/06 14:56:44 - INFO - Distillation -  Global step: 2928, epoch step:2928
2022/05/06 14:57:34 - INFO - Distillation -  Global step: 3111, epoch step:3111
2022/05/06 14:58:23 - INFO - Distillation -  Global step: 3294, epoch step:3294
2022/05/06 14:59:13 - INFO - Distillation -  Global step: 3477, epoch step:3477
2022/05/06 15:00:02 - INFO - Distillation -  Global step: 3660, epoch step:3660
2022/05/06 15:00:03 - INFO - Distillation -  Saving at global step 3662, epoch step 3662 epoch 1
2022/05/06 15:00:03 - INFO - Distillation -  Running callback function...
2022/05/06 15:00:03 - INFO - train_eval -  Predicting...
2022/05/06 15:00:03 - INFO - train_eval -  ***** Running predictions *****
2022/05/06 15:00:03 - INFO - train_eval -    Num orig examples = 3219
2022/05/06 15:00:03 - INFO - train_eval -    Num split examples = 5526
2022/05/06 15:00:03 - INFO - train_eval -    Batch size = 8
2022/05/06 15:00:03 - INFO - train_eval -  Start evaluating
2022/05/06 15:00:03 - INFO - train_eval -  Processing example: 0
2022/05/06 15:00:08 - INFO - train_eval -  Processing example: 1000
2022/05/06 15:00:11 - INFO - train_eval -  Processing example: 2000
2022/05/06 15:00:14 - INFO - train_eval -  Processing example: 3000
2022/05/06 15:00:17 - INFO - train_eval -  Processing example: 4000
2022/05/06 15:00:20 - INFO - train_eval -  Processing example: 5000
2022/05/06 15:00:22 - INFO - train_eval -  Write predictions...
2022/05/06 15:00:22 - INFO - processing -  Writing predictions to: /home/hs3228/TextBrewer/output_model/cmrc/student/cmrc2018_t20_TbaseST3_AllSmmdH1_lr15e10_opt1/predictions_3662.json
2022/05/06 15:00:37 - INFO - train_eval -  ***** Eval results 3662 *****
2022/05/06 15:00:37 - INFO - train_eval -  {"AVERAGE": "60.576", "F1": "73.156", "EM": "47.996", "TOTAL": 3219, "SKIP": 0}

2022/05/06 15:00:37 - INFO - Distillation -  Epoch 1 finished
2022/05/06 15:00:37 - INFO - Distillation -  Epoch 2
2022/05/06 15:00:37 - INFO - Distillation -  Length of current epoch in forward batch: 3662
2022/05/06 15:01:26 - INFO - Distillation -  Global step: 3843, epoch step:181
2022/05/06 15:02:16 - INFO - Distillation -  Global step: 4026, epoch step:364
2022/05/06 15:03:05 - INFO - Distillation -  Global step: 4209, epoch step:547
2022/05/06 15:03:55 - INFO - Distillation -  Global step: 4392, epoch step:730
2022/05/06 15:04:44 - INFO - Distillation -  Global step: 4575, epoch step:913
2022/05/06 15:05:33 - INFO - Distillation -  Global step: 4758, epoch step:1096
2022/05/06 15:06:23 - INFO - Distillation -  Global step: 4941, epoch step:1279
2022/05/06 15:07:12 - INFO - Distillation -  Global step: 5124, epoch step:1462
2022/05/06 15:08:01 - INFO - Distillation -  Global step: 5307, epoch step:1645
2022/05/06 15:08:51 - INFO - Distillation -  Global step: 5490, epoch step:1828
2022/05/06 15:09:40 - INFO - Distillation -  Global step: 5673, epoch step:2011
2022/05/06 15:10:30 - INFO - Distillation -  Global step: 5856, epoch step:2194
2022/05/06 15:11:19 - INFO - Distillation -  Global step: 6039, epoch step:2377
2022/05/06 15:12:08 - INFO - Distillation -  Global step: 6222, epoch step:2560
2022/05/06 15:12:58 - INFO - Distillation -  Global step: 6405, epoch step:2743
2022/05/06 15:13:47 - INFO - Distillation -  Global step: 6588, epoch step:2926
2022/05/06 15:14:37 - INFO - Distillation -  Global step: 6771, epoch step:3109
2022/05/06 15:15:26 - INFO - Distillation -  Global step: 6954, epoch step:3292
2022/05/06 15:16:16 - INFO - Distillation -  Global step: 7137, epoch step:3475
2022/05/06 15:17:05 - INFO - Distillation -  Global step: 7320, epoch step:3658
2022/05/06 15:17:06 - INFO - Distillation -  Saving at global step 7324, epoch step 3662 epoch 2
2022/05/06 15:17:06 - INFO - Distillation -  Running callback function...
2022/05/06 15:17:06 - INFO - train_eval -  Predicting...
2022/05/06 15:17:06 - INFO - train_eval -  ***** Running predictions *****
2022/05/06 15:17:06 - INFO - train_eval -    Num orig examples = 3219
2022/05/06 15:17:06 - INFO - train_eval -    Num split examples = 5526
2022/05/06 15:17:06 - INFO - train_eval -    Batch size = 8
2022/05/06 15:17:07 - INFO - train_eval -  Start evaluating
2022/05/06 15:17:07 - INFO - train_eval -  Processing example: 0
2022/05/06 15:17:10 - INFO - train_eval -  Processing example: 1000
2022/05/06 15:17:13 - INFO - train_eval -  Processing example: 2000
2022/05/06 15:17:16 - INFO - train_eval -  Processing example: 3000
2022/05/06 15:17:19 - INFO - train_eval -  Processing example: 4000
2022/05/06 15:17:22 - INFO - train_eval -  Processing example: 5000
2022/05/06 15:17:24 - INFO - train_eval -  Write predictions...
2022/05/06 15:17:24 - INFO - processing -  Writing predictions to: /home/hs3228/TextBrewer/output_model/cmrc/student/cmrc2018_t20_TbaseST3_AllSmmdH1_lr15e10_opt1/predictions_7324.json
2022/05/06 15:17:41 - INFO - train_eval -  ***** Eval results 7324 *****
2022/05/06 15:17:41 - INFO - train_eval -  {"AVERAGE": "65.601", "F1": "76.620", "EM": "54.582", "TOTAL": 3219, "SKIP": 0}

2022/05/06 15:17:41 - INFO - Distillation -  Epoch 2 finished
2022/05/06 15:17:41 - INFO - Distillation -  Epoch 3
2022/05/06 15:17:41 - INFO - Distillation -  Length of current epoch in forward batch: 3662
2022/05/06 15:18:29 - INFO - Distillation -  Global step: 7503, epoch step:179
2022/05/06 15:19:18 - INFO - Distillation -  Global step: 7686, epoch step:362
2022/05/06 15:20:08 - INFO - Distillation -  Global step: 7869, epoch step:545
2022/05/06 15:20:57 - INFO - Distillation -  Global step: 8052, epoch step:728
2022/05/06 15:21:47 - INFO - Distillation -  Global step: 8235, epoch step:911
2022/05/06 15:22:36 - INFO - Distillation -  Global step: 8418, epoch step:1094
2022/05/06 15:23:26 - INFO - Distillation -  Global step: 8601, epoch step:1277
2022/05/06 15:24:15 - INFO - Distillation -  Global step: 8784, epoch step:1460
2022/05/06 15:25:04 - INFO - Distillation -  Global step: 8967, epoch step:1643
2022/05/06 15:25:54 - INFO - Distillation -  Global step: 9150, epoch step:1826
2022/05/06 15:26:43 - INFO - Distillation -  Global step: 9333, epoch step:2009
2022/05/06 15:27:33 - INFO - Distillation -  Global step: 9516, epoch step:2192
2022/05/06 15:28:22 - INFO - Distillation -  Global step: 9699, epoch step:2375
2022/05/06 15:29:12 - INFO - Distillation -  Global step: 9882, epoch step:2558
2022/05/06 15:30:01 - INFO - Distillation -  Global step: 10065, epoch step:2741
2022/05/06 15:30:50 - INFO - Distillation -  Global step: 10248, epoch step:2924
2022/05/06 15:31:40 - INFO - Distillation -  Global step: 10431, epoch step:3107
2022/05/06 15:32:29 - INFO - Distillation -  Global step: 10614, epoch step:3290
2022/05/06 15:33:19 - INFO - Distillation -  Global step: 10797, epoch step:3473
2022/05/06 15:34:08 - INFO - Distillation -  Global step: 10980, epoch step:3656
2022/05/06 15:34:10 - INFO - Distillation -  Saving at global step 10986, epoch step 3662 epoch 3
2022/05/06 15:34:10 - INFO - Distillation -  Running callback function...
2022/05/06 15:34:10 - INFO - train_eval -  Predicting...
2022/05/06 15:34:10 - INFO - train_eval -  ***** Running predictions *****
2022/05/06 15:34:10 - INFO - train_eval -    Num orig examples = 3219
2022/05/06 15:34:10 - INFO - train_eval -    Num split examples = 5526
2022/05/06 15:34:10 - INFO - train_eval -    Batch size = 8
2022/05/06 15:34:10 - INFO - train_eval -  Start evaluating
2022/05/06 15:34:10 - INFO - train_eval -  Processing example: 0
2022/05/06 15:34:13 - INFO - train_eval -  Processing example: 1000
2022/05/06 15:34:17 - INFO - train_eval -  Processing example: 2000
2022/05/06 15:34:20 - INFO - train_eval -  Processing example: 3000
2022/05/06 15:34:23 - INFO - train_eval -  Processing example: 4000
2022/05/06 15:34:26 - INFO - train_eval -  Processing example: 5000
2022/05/06 15:34:27 - INFO - train_eval -  Write predictions...
2022/05/06 15:34:27 - INFO - processing -  Writing predictions to: /home/hs3228/TextBrewer/output_model/cmrc/student/cmrc2018_t20_TbaseST3_AllSmmdH1_lr15e10_opt1/predictions_10986.json
2022/05/06 15:34:42 - INFO - train_eval -  ***** Eval results 10986 *****
2022/05/06 15:34:42 - INFO - train_eval -  {"AVERAGE": "67.448", "F1": "78.170", "EM": "56.726", "TOTAL": 3219, "SKIP": 0}

2022/05/06 15:34:42 - INFO - Distillation -  Epoch 3 finished
2022/05/06 15:34:42 - INFO - Distillation -  Epoch 4
2022/05/06 15:34:42 - INFO - Distillation -  Length of current epoch in forward batch: 3662
2022/05/06 15:35:30 - INFO - Distillation -  Global step: 11163, epoch step:177
2022/05/06 15:36:19 - INFO - Distillation -  Global step: 11346, epoch step:360
2022/05/06 15:37:09 - INFO - Distillation -  Global step: 11529, epoch step:543
2022/05/06 15:37:58 - INFO - Distillation -  Global step: 11712, epoch step:726
2022/05/06 15:38:48 - INFO - Distillation -  Global step: 11895, epoch step:909
2022/05/06 15:39:37 - INFO - Distillation -  Global step: 12078, epoch step:1092
2022/05/06 15:40:26 - INFO - Distillation -  Global step: 12261, epoch step:1275
2022/05/06 15:41:16 - INFO - Distillation -  Global step: 12444, epoch step:1458
2022/05/06 15:42:05 - INFO - Distillation -  Global step: 12627, epoch step:1641
2022/05/06 15:42:55 - INFO - Distillation -  Global step: 12810, epoch step:1824
2022/05/06 15:43:44 - INFO - Distillation -  Global step: 12993, epoch step:2007
2022/05/06 15:44:33 - INFO - Distillation -  Global step: 13176, epoch step:2190
2022/05/06 15:45:23 - INFO - Distillation -  Global step: 13359, epoch step:2373
2022/05/06 15:46:12 - INFO - Distillation -  Global step: 13542, epoch step:2556
2022/05/06 15:47:01 - INFO - Distillation -  Global step: 13725, epoch step:2739
2022/05/06 15:47:51 - INFO - Distillation -  Global step: 13908, epoch step:2922
2022/05/06 15:48:40 - INFO - Distillation -  Global step: 14091, epoch step:3105
2022/05/06 15:49:30 - INFO - Distillation -  Global step: 14274, epoch step:3288
2022/05/06 15:50:19 - INFO - Distillation -  Global step: 14457, epoch step:3471
2022/05/06 15:51:08 - INFO - Distillation -  Global step: 14640, epoch step:3654
2022/05/06 15:51:11 - INFO - Distillation -  Saving at global step 14648, epoch step 3662 epoch 4
2022/05/06 15:51:11 - INFO - Distillation -  Running callback function...
2022/05/06 15:51:11 - INFO - train_eval -  Predicting...
2022/05/06 15:51:11 - INFO - train_eval -  ***** Running predictions *****
2022/05/06 15:51:11 - INFO - train_eval -    Num orig examples = 3219
2022/05/06 15:51:11 - INFO - train_eval -    Num split examples = 5526
2022/05/06 15:51:11 - INFO - train_eval -    Batch size = 8
2022/05/06 15:51:11 - INFO - train_eval -  Start evaluating
2022/05/06 15:51:11 - INFO - train_eval -  Processing example: 0
2022/05/06 15:51:14 - INFO - train_eval -  Processing example: 1000
2022/05/06 15:51:17 - INFO - train_eval -  Processing example: 2000
2022/05/06 15:51:20 - INFO - train_eval -  Processing example: 3000
2022/05/06 15:51:23 - INFO - train_eval -  Processing example: 4000
2022/05/06 15:51:27 - INFO - train_eval -  Processing example: 5000
2022/05/06 15:51:28 - INFO - train_eval -  Write predictions...
2022/05/06 15:51:28 - INFO - processing -  Writing predictions to: /home/hs3228/TextBrewer/output_model/cmrc/student/cmrc2018_t20_TbaseST3_AllSmmdH1_lr15e10_opt1/predictions_14648.json
2022/05/06 15:51:44 - INFO - train_eval -  ***** Eval results 14648 *****
2022/05/06 15:51:44 - INFO - train_eval -  {"AVERAGE": "68.337", "F1": "79.109", "EM": "57.564", "TOTAL": 3219, "SKIP": 0}

2022/05/06 15:51:45 - INFO - Distillation -  Epoch 4 finished
2022/05/06 15:51:45 - INFO - Distillation -  Epoch 5
2022/05/06 15:51:45 - INFO - Distillation -  Length of current epoch in forward batch: 3662
2022/05/06 15:52:32 - INFO - Distillation -  Global step: 14823, epoch step:175
2022/05/06 15:53:21 - INFO - Distillation -  Global step: 15006, epoch step:358
2022/05/06 15:54:10 - INFO - Distillation -  Global step: 15189, epoch step:541
2022/05/06 15:55:00 - INFO - Distillation -  Global step: 15372, epoch step:724
2022/05/06 15:55:49 - INFO - Distillation -  Global step: 15555, epoch step:907
2022/05/06 15:56:38 - INFO - Distillation -  Global step: 15738, epoch step:1090
2022/05/06 15:57:28 - INFO - Distillation -  Global step: 15921, epoch step:1273
2022/05/06 15:58:17 - INFO - Distillation -  Global step: 16104, epoch step:1456
2022/05/06 15:59:06 - INFO - Distillation -  Global step: 16287, epoch step:1639
2022/05/06 15:59:56 - INFO - Distillation -  Global step: 16470, epoch step:1822
2022/05/06 16:00:45 - INFO - Distillation -  Global step: 16653, epoch step:2005
2022/05/06 16:01:34 - INFO - Distillation -  Global step: 16836, epoch step:2188
2022/05/06 16:02:24 - INFO - Distillation -  Global step: 17019, epoch step:2371
2022/05/06 16:03:13 - INFO - Distillation -  Global step: 17202, epoch step:2554
2022/05/06 16:04:02 - INFO - Distillation -  Global step: 17385, epoch step:2737
2022/05/06 16:04:52 - INFO - Distillation -  Global step: 17568, epoch step:2920
2022/05/06 16:05:41 - INFO - Distillation -  Global step: 17751, epoch step:3103
2022/05/06 16:06:31 - INFO - Distillation -  Global step: 17934, epoch step:3286
2022/05/06 16:07:20 - INFO - Distillation -  Global step: 18117, epoch step:3469
2022/05/06 16:08:10 - INFO - Distillation -  Global step: 18300, epoch step:3652
2022/05/06 16:08:12 - INFO - Distillation -  Saving at global step 18310, epoch step 3662 epoch 5
2022/05/06 16:08:13 - INFO - Distillation -  Running callback function...
2022/05/06 16:08:13 - INFO - train_eval -  Predicting...
2022/05/06 16:08:13 - INFO - train_eval -  ***** Running predictions *****
2022/05/06 16:08:13 - INFO - train_eval -    Num orig examples = 3219
2022/05/06 16:08:13 - INFO - train_eval -    Num split examples = 5526
2022/05/06 16:08:13 - INFO - train_eval -    Batch size = 8
2022/05/06 16:08:13 - INFO - train_eval -  Start evaluating
2022/05/06 16:08:13 - INFO - train_eval -  Processing example: 0
2022/05/06 16:08:16 - INFO - train_eval -  Processing example: 1000
2022/05/06 16:08:19 - INFO - train_eval -  Processing example: 2000
2022/05/06 16:08:22 - INFO - train_eval -  Processing example: 3000
2022/05/06 16:08:25 - INFO - train_eval -  Processing example: 4000
2022/05/06 16:08:28 - INFO - train_eval -  Processing example: 5000
2022/05/06 16:08:30 - INFO - train_eval -  Write predictions...
2022/05/06 16:08:30 - INFO - processing -  Writing predictions to: /home/hs3228/TextBrewer/output_model/cmrc/student/cmrc2018_t20_TbaseST3_AllSmmdH1_lr15e10_opt1/predictions_18310.json
2022/05/06 16:08:46 - INFO - train_eval -  ***** Eval results 18310 *****
2022/05/06 16:08:46 - INFO - train_eval -  {"AVERAGE": "67.785", "F1": "78.813", "EM": "56.757", "TOTAL": 3219, "SKIP": 0}

2022/05/06 16:08:46 - INFO - Distillation -  Epoch 5 finished
2022/05/06 16:08:46 - INFO - Distillation -  Epoch 6
2022/05/06 16:08:46 - INFO - Distillation -  Length of current epoch in forward batch: 3662
2022/05/06 16:09:32 - INFO - Distillation -  Global step: 18483, epoch step:173
2022/05/06 16:10:22 - INFO - Distillation -  Global step: 18666, epoch step:356
2022/05/06 16:11:11 - INFO - Distillation -  Global step: 18849, epoch step:539
2022/05/06 16:12:00 - INFO - Distillation -  Global step: 19032, epoch step:722
2022/05/06 16:12:50 - INFO - Distillation -  Global step: 19215, epoch step:905
2022/05/06 16:13:39 - INFO - Distillation -  Global step: 19398, epoch step:1088
2022/05/06 16:14:29 - INFO - Distillation -  Global step: 19581, epoch step:1271
2022/05/06 16:15:18 - INFO - Distillation -  Global step: 19764, epoch step:1454
2022/05/06 16:16:07 - INFO - Distillation -  Global step: 19947, epoch step:1637
2022/05/06 16:16:57 - INFO - Distillation -  Global step: 20130, epoch step:1820
2022/05/06 16:17:46 - INFO - Distillation -  Global step: 20313, epoch step:2003
2022/05/06 16:18:36 - INFO - Distillation -  Global step: 20496, epoch step:2186
2022/05/06 16:19:25 - INFO - Distillation -  Global step: 20679, epoch step:2369
2022/05/06 16:20:14 - INFO - Distillation -  Global step: 20862, epoch step:2552
2022/05/06 16:21:04 - INFO - Distillation -  Global step: 21045, epoch step:2735
2022/05/06 16:21:53 - INFO - Distillation -  Global step: 21228, epoch step:2918
2022/05/06 16:22:42 - INFO - Distillation -  Global step: 21411, epoch step:3101
2022/05/06 16:23:32 - INFO - Distillation -  Global step: 21594, epoch step:3284
2022/05/06 16:24:21 - INFO - Distillation -  Global step: 21777, epoch step:3467
2022/05/06 16:25:10 - INFO - Distillation -  Global step: 21960, epoch step:3650
2022/05/06 16:25:14 - INFO - Distillation -  Saving at global step 21972, epoch step 3662 epoch 6
2022/05/06 16:25:14 - INFO - Distillation -  Running callback function...
2022/05/06 16:25:14 - INFO - train_eval -  Predicting...
2022/05/06 16:25:14 - INFO - train_eval -  ***** Running predictions *****
2022/05/06 16:25:14 - INFO - train_eval -    Num orig examples = 3219
2022/05/06 16:25:14 - INFO - train_eval -    Num split examples = 5526
2022/05/06 16:25:14 - INFO - train_eval -    Batch size = 8
2022/05/06 16:25:14 - INFO - train_eval -  Start evaluating
2022/05/06 16:25:14 - INFO - train_eval -  Processing example: 0
2022/05/06 16:25:17 - INFO - train_eval -  Processing example: 1000
2022/05/06 16:25:20 - INFO - train_eval -  Processing example: 2000
2022/05/06 16:25:23 - INFO - train_eval -  Processing example: 3000
2022/05/06 16:25:26 - INFO - train_eval -  Processing example: 4000
2022/05/06 16:25:30 - INFO - train_eval -  Processing example: 5000
2022/05/06 16:25:31 - INFO - train_eval -  Write predictions...
2022/05/06 16:25:31 - INFO - processing -  Writing predictions to: /home/hs3228/TextBrewer/output_model/cmrc/student/cmrc2018_t20_TbaseST3_AllSmmdH1_lr15e10_opt1/predictions_21972.json
2022/05/06 16:25:48 - INFO - train_eval -  ***** Eval results 21972 *****
2022/05/06 16:25:48 - INFO - train_eval -  {"AVERAGE": "68.551", "F1": "79.568", "EM": "57.533", "TOTAL": 3219, "SKIP": 0}

2022/05/06 16:25:48 - INFO - Distillation -  Epoch 6 finished
2022/05/06 16:25:48 - INFO - Distillation -  Epoch 7
2022/05/06 16:25:48 - INFO - Distillation -  Length of current epoch in forward batch: 3662
2022/05/06 16:26:34 - INFO - Distillation -  Global step: 22143, epoch step:171
2022/05/06 16:27:23 - INFO - Distillation -  Global step: 22326, epoch step:354
2022/05/06 16:28:12 - INFO - Distillation -  Global step: 22509, epoch step:537
2022/05/06 16:29:02 - INFO - Distillation -  Global step: 22692, epoch step:720
2022/05/06 16:29:51 - INFO - Distillation -  Global step: 22875, epoch step:903
2022/05/06 16:30:41 - INFO - Distillation -  Global step: 23058, epoch step:1086
2022/05/06 16:31:30 - INFO - Distillation -  Global step: 23241, epoch step:1269
2022/05/06 16:32:19 - INFO - Distillation -  Global step: 23424, epoch step:1452
2022/05/06 16:33:09 - INFO - Distillation -  Global step: 23607, epoch step:1635
2022/05/06 16:33:58 - INFO - Distillation -  Global step: 23790, epoch step:1818
2022/05/06 16:34:47 - INFO - Distillation -  Global step: 23973, epoch step:2001
2022/05/06 16:35:37 - INFO - Distillation -  Global step: 24156, epoch step:2184
2022/05/06 16:36:26 - INFO - Distillation -  Global step: 24339, epoch step:2367
2022/05/06 16:37:15 - INFO - Distillation -  Global step: 24522, epoch step:2550
2022/05/06 16:38:05 - INFO - Distillation -  Global step: 24705, epoch step:2733
2022/05/06 16:38:54 - INFO - Distillation -  Global step: 24888, epoch step:2916
2022/05/06 16:39:43 - INFO - Distillation -  Global step: 25071, epoch step:3099
2022/05/06 16:40:33 - INFO - Distillation -  Global step: 25254, epoch step:3282
2022/05/06 16:41:22 - INFO - Distillation -  Global step: 25437, epoch step:3465
2022/05/06 16:42:11 - INFO - Distillation -  Global step: 25620, epoch step:3648
2022/05/06 16:42:15 - INFO - Distillation -  Saving at global step 25634, epoch step 3662 epoch 7
2022/05/06 16:42:15 - INFO - Distillation -  Running callback function...
2022/05/06 16:42:15 - INFO - train_eval -  Predicting...
2022/05/06 16:42:15 - INFO - train_eval -  ***** Running predictions *****
2022/05/06 16:42:15 - INFO - train_eval -    Num orig examples = 3219
2022/05/06 16:42:15 - INFO - train_eval -    Num split examples = 5526
2022/05/06 16:42:15 - INFO - train_eval -    Batch size = 8
2022/05/06 16:42:16 - INFO - train_eval -  Start evaluating
2022/05/06 16:42:16 - INFO - train_eval -  Processing example: 0
2022/05/06 16:42:19 - INFO - train_eval -  Processing example: 1000
2022/05/06 16:42:22 - INFO - train_eval -  Processing example: 2000
2022/05/06 16:42:25 - INFO - train_eval -  Processing example: 3000
2022/05/06 16:42:28 - INFO - train_eval -  Processing example: 4000
2022/05/06 16:42:31 - INFO - train_eval -  Processing example: 5000
2022/05/06 16:42:33 - INFO - train_eval -  Write predictions...
2022/05/06 16:42:33 - INFO - processing -  Writing predictions to: /home/hs3228/TextBrewer/output_model/cmrc/student/cmrc2018_t20_TbaseST3_AllSmmdH1_lr15e10_opt1/predictions_25634.json
2022/05/06 16:42:49 - INFO - train_eval -  ***** Eval results 25634 *****
2022/05/06 16:42:49 - INFO - train_eval -  {"AVERAGE": "68.341", "F1": "79.272", "EM": "57.409", "TOTAL": 3219, "SKIP": 0}

2022/05/06 16:42:49 - INFO - Distillation -  Epoch 7 finished
2022/05/06 16:42:49 - INFO - Distillation -  Epoch 8
2022/05/06 16:42:49 - INFO - Distillation -  Length of current epoch in forward batch: 3662
2022/05/06 16:43:34 - INFO - Distillation -  Global step: 25803, epoch step:169
2022/05/06 16:44:24 - INFO - Distillation -  Global step: 25986, epoch step:352
2022/05/06 16:45:13 - INFO - Distillation -  Global step: 26169, epoch step:535
2022/05/06 16:46:02 - INFO - Distillation -  Global step: 26352, epoch step:718
2022/05/06 16:46:52 - INFO - Distillation -  Global step: 26535, epoch step:901
2022/05/06 16:47:41 - INFO - Distillation -  Global step: 26718, epoch step:1084
2022/05/06 16:48:30 - INFO - Distillation -  Global step: 26901, epoch step:1267
2022/05/06 16:49:20 - INFO - Distillation -  Global step: 27084, epoch step:1450
2022/05/06 16:50:09 - INFO - Distillation -  Global step: 27267, epoch step:1633
2022/05/06 16:50:58 - INFO - Distillation -  Global step: 27450, epoch step:1816
2022/05/06 16:51:48 - INFO - Distillation -  Global step: 27633, epoch step:1999
2022/05/06 16:52:37 - INFO - Distillation -  Global step: 27816, epoch step:2182
2022/05/06 16:53:26 - INFO - Distillation -  Global step: 27999, epoch step:2365
2022/05/06 16:54:16 - INFO - Distillation -  Global step: 28182, epoch step:2548
2022/05/06 16:55:05 - INFO - Distillation -  Global step: 28365, epoch step:2731
2022/05/06 16:55:54 - INFO - Distillation -  Global step: 28548, epoch step:2914
2022/05/06 16:56:44 - INFO - Distillation -  Global step: 28731, epoch step:3097
2022/05/06 16:57:33 - INFO - Distillation -  Global step: 28914, epoch step:3280
2022/05/06 16:58:22 - INFO - Distillation -  Global step: 29097, epoch step:3463
2022/05/06 16:59:12 - INFO - Distillation -  Global step: 29280, epoch step:3646
2022/05/06 16:59:16 - INFO - Distillation -  Saving at global step 29296, epoch step 3662 epoch 8
2022/05/06 16:59:17 - INFO - Distillation -  Running callback function...
2022/05/06 16:59:17 - INFO - train_eval -  Predicting...
2022/05/06 16:59:17 - INFO - train_eval -  ***** Running predictions *****
2022/05/06 16:59:17 - INFO - train_eval -    Num orig examples = 3219
2022/05/06 16:59:17 - INFO - train_eval -    Num split examples = 5526
2022/05/06 16:59:17 - INFO - train_eval -    Batch size = 8
2022/05/06 16:59:17 - INFO - train_eval -  Start evaluating
2022/05/06 16:59:17 - INFO - train_eval -  Processing example: 0
2022/05/06 16:59:20 - INFO - train_eval -  Processing example: 1000
2022/05/06 16:59:23 - INFO - train_eval -  Processing example: 2000
2022/05/06 16:59:26 - INFO - train_eval -  Processing example: 3000
2022/05/06 16:59:29 - INFO - train_eval -  Processing example: 4000
2022/05/06 16:59:32 - INFO - train_eval -  Processing example: 5000
2022/05/06 16:59:34 - INFO - train_eval -  Write predictions...
2022/05/06 16:59:34 - INFO - processing -  Writing predictions to: /home/hs3228/TextBrewer/output_model/cmrc/student/cmrc2018_t20_TbaseST3_AllSmmdH1_lr15e10_opt1/predictions_29296.json
2022/05/06 16:59:49 - INFO - train_eval -  ***** Eval results 29296 *****
2022/05/06 16:59:49 - INFO - train_eval -  {"AVERAGE": "68.639", "F1": "79.217", "EM": "58.062", "TOTAL": 3219, "SKIP": 0}

2022/05/06 16:59:49 - INFO - Distillation -  Epoch 8 finished
2022/05/06 16:59:49 - INFO - Distillation -  Epoch 9
2022/05/06 16:59:49 - INFO - Distillation -  Length of current epoch in forward batch: 3662
2022/05/06 17:00:34 - INFO - Distillation -  Global step: 29463, epoch step:167
2022/05/06 17:01:23 - INFO - Distillation -  Global step: 29646, epoch step:350
2022/05/06 17:02:13 - INFO - Distillation -  Global step: 29829, epoch step:533
2022/05/06 17:03:02 - INFO - Distillation -  Global step: 30012, epoch step:716
2022/05/06 17:03:51 - INFO - Distillation -  Global step: 30195, epoch step:899
2022/05/06 17:04:41 - INFO - Distillation -  Global step: 30378, epoch step:1082
2022/05/06 17:05:30 - INFO - Distillation -  Global step: 30561, epoch step:1265
2022/05/06 17:06:20 - INFO - Distillation -  Global step: 30744, epoch step:1448
2022/05/06 17:07:09 - INFO - Distillation -  Global step: 30927, epoch step:1631
2022/05/06 17:07:58 - INFO - Distillation -  Global step: 31110, epoch step:1814
2022/05/06 17:08:48 - INFO - Distillation -  Global step: 31293, epoch step:1997
2022/05/06 17:09:37 - INFO - Distillation -  Global step: 31476, epoch step:2180
2022/05/06 17:10:26 - INFO - Distillation -  Global step: 31659, epoch step:2363
2022/05/06 17:11:15 - INFO - Distillation -  Global step: 31842, epoch step:2546
2022/05/06 17:12:05 - INFO - Distillation -  Global step: 32025, epoch step:2729
2022/05/06 17:12:54 - INFO - Distillation -  Global step: 32208, epoch step:2912
2022/05/06 17:13:43 - INFO - Distillation -  Global step: 32391, epoch step:3095
2022/05/06 17:14:33 - INFO - Distillation -  Global step: 32574, epoch step:3278
2022/05/06 17:15:22 - INFO - Distillation -  Global step: 32757, epoch step:3461
2022/05/06 17:16:11 - INFO - Distillation -  Global step: 32940, epoch step:3644
2022/05/06 17:16:16 - INFO - Distillation -  Saving at global step 32958, epoch step 3662 epoch 9
2022/05/06 17:16:16 - INFO - Distillation -  Running callback function...
2022/05/06 17:16:16 - INFO - train_eval -  Predicting...
2022/05/06 17:16:16 - INFO - train_eval -  ***** Running predictions *****
2022/05/06 17:16:16 - INFO - train_eval -    Num orig examples = 3219
2022/05/06 17:16:16 - INFO - train_eval -    Num split examples = 5526
2022/05/06 17:16:16 - INFO - train_eval -    Batch size = 8
2022/05/06 17:16:17 - INFO - train_eval -  Start evaluating
2022/05/06 17:16:17 - INFO - train_eval -  Processing example: 0
2022/05/06 17:16:20 - INFO - train_eval -  Processing example: 1000
2022/05/06 17:16:23 - INFO - train_eval -  Processing example: 2000
2022/05/06 17:16:26 - INFO - train_eval -  Processing example: 3000
2022/05/06 17:16:29 - INFO - train_eval -  Processing example: 4000
2022/05/06 17:16:32 - INFO - train_eval -  Processing example: 5000
2022/05/06 17:16:34 - INFO - train_eval -  Write predictions...
2022/05/06 17:16:34 - INFO - processing -  Writing predictions to: /home/hs3228/TextBrewer/output_model/cmrc/student/cmrc2018_t20_TbaseST3_AllSmmdH1_lr15e10_opt1/predictions_32958.json
2022/05/06 17:16:50 - INFO - train_eval -  ***** Eval results 32958 *****
2022/05/06 17:16:50 - INFO - train_eval -  {"AVERAGE": "69.107", "F1": "79.967", "EM": "58.248", "TOTAL": 3219, "SKIP": 0}

2022/05/06 17:16:50 - INFO - Distillation -  Epoch 9 finished
2022/05/06 17:16:50 - INFO - Distillation -  Epoch 10
2022/05/06 17:16:50 - INFO - Distillation -  Length of current epoch in forward batch: 3662
2022/05/06 17:17:35 - INFO - Distillation -  Global step: 33123, epoch step:165
2022/05/06 17:18:24 - INFO - Distillation -  Global step: 33306, epoch step:348
2022/05/06 17:19:13 - INFO - Distillation -  Global step: 33489, epoch step:531
2022/05/06 17:20:02 - INFO - Distillation -  Global step: 33672, epoch step:714
2022/05/06 17:20:52 - INFO - Distillation -  Global step: 33855, epoch step:897
2022/05/06 17:21:41 - INFO - Distillation -  Global step: 34038, epoch step:1080
2022/05/06 17:22:30 - INFO - Distillation -  Global step: 34221, epoch step:1263
2022/05/06 17:23:20 - INFO - Distillation -  Global step: 34404, epoch step:1446
2022/05/06 17:24:09 - INFO - Distillation -  Global step: 34587, epoch step:1629
2022/05/06 17:24:58 - INFO - Distillation -  Global step: 34770, epoch step:1812
2022/05/06 17:25:48 - INFO - Distillation -  Global step: 34953, epoch step:1995
2022/05/06 17:26:37 - INFO - Distillation -  Global step: 35136, epoch step:2178
2022/05/06 17:27:27 - INFO - Distillation -  Global step: 35319, epoch step:2361
2022/05/06 17:28:16 - INFO - Distillation -  Global step: 35502, epoch step:2544
2022/05/06 17:29:05 - INFO - Distillation -  Global step: 35685, epoch step:2727
2022/05/06 17:29:55 - INFO - Distillation -  Global step: 35868, epoch step:2910
2022/05/06 17:30:44 - INFO - Distillation -  Global step: 36051, epoch step:3093
2022/05/06 17:31:33 - INFO - Distillation -  Global step: 36234, epoch step:3276
2022/05/06 17:32:23 - INFO - Distillation -  Global step: 36417, epoch step:3459
2022/05/06 17:33:12 - INFO - Distillation -  Global step: 36600, epoch step:3642
2022/05/06 17:33:17 - INFO - Distillation -  Saving at global step 36620, epoch step 3662 epoch 10
2022/05/06 17:33:18 - INFO - Distillation -  Running callback function...
2022/05/06 17:33:18 - INFO - train_eval -  Predicting...
2022/05/06 17:33:18 - INFO - train_eval -  ***** Running predictions *****
2022/05/06 17:33:18 - INFO - train_eval -    Num orig examples = 3219
2022/05/06 17:33:18 - INFO - train_eval -    Num split examples = 5526
2022/05/06 17:33:18 - INFO - train_eval -    Batch size = 8
2022/05/06 17:33:18 - INFO - train_eval -  Start evaluating
2022/05/06 17:33:18 - INFO - train_eval -  Processing example: 0
2022/05/06 17:33:21 - INFO - train_eval -  Processing example: 1000
2022/05/06 17:33:24 - INFO - train_eval -  Processing example: 2000
2022/05/06 17:33:27 - INFO - train_eval -  Processing example: 3000
2022/05/06 17:33:30 - INFO - train_eval -  Processing example: 4000
2022/05/06 17:33:33 - INFO - train_eval -  Processing example: 5000
2022/05/06 17:33:35 - INFO - train_eval -  Write predictions...
2022/05/06 17:33:35 - INFO - processing -  Writing predictions to: /home/hs3228/TextBrewer/output_model/cmrc/student/cmrc2018_t20_TbaseST3_AllSmmdH1_lr15e10_opt1/predictions_36620.json
2022/05/06 17:33:50 - INFO - train_eval -  ***** Eval results 36620 *****
2022/05/06 17:33:50 - INFO - train_eval -  {"AVERAGE": "70.587", "F1": "80.906", "EM": "60.267", "TOTAL": 3219, "SKIP": 0}

2022/05/06 17:33:50 - INFO - Distillation -  Epoch 10 finished
