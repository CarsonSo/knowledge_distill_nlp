nohup: ignoring input
2022/05/06 03:20:51 - INFO - pytorch_pretrained_bert.modeling -  Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
2022/05/06 03:20:51 - INFO - pytorch_pretrained_bert.my_modeling -  Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
2022/05/06 03:20:52 - INFO - Main -  vocab_file:/home/hs3228/TextBrewer/model/chinese_roberta_wwm_ext_pytorch/vocab.txt
2022/05/06 03:20:52 - INFO - Main -  output_dir:/home/hs3228/TextBrewer/output_model/cmrc/student/cmrc2018_t8_TbaseST3_AllSmmdH1_lr15e10_opt1
2022/05/06 03:20:52 - INFO - Main -  train_file:/home/hs3228/TextBrewer/data/cmrc2018/squad-style-data/cmrc2018_train.json
2022/05/06 03:20:52 - INFO - Main -  predict_file:/home/hs3228/TextBrewer/data/cmrc2018/squad-style-data/cmrc2018_dev.json
2022/05/06 03:20:52 - INFO - Main -  do_lower_case:True
2022/05/06 03:20:52 - INFO - Main -  max_seq_length:512
2022/05/06 03:20:52 - INFO - Main -  doc_stride:128
2022/05/06 03:20:52 - INFO - Main -  max_query_length:64
2022/05/06 03:20:52 - INFO - Main -  do_train:True
2022/05/06 03:20:52 - INFO - Main -  do_predict:True
2022/05/06 03:20:52 - INFO - Main -  train_batch_size:12
2022/05/06 03:20:52 - INFO - Main -  predict_batch_size:8
2022/05/06 03:20:52 - INFO - Main -  learning_rate:0.00015
2022/05/06 03:20:52 - INFO - Main -  num_train_epochs:10.0
2022/05/06 03:20:52 - INFO - Main -  warmup_proportion:0.1
2022/05/06 03:20:52 - INFO - Main -  n_best_size:20
2022/05/06 03:20:52 - INFO - Main -  max_answer_length:30
2022/05/06 03:20:52 - INFO - Main -  verbose_logging:False
2022/05/06 03:20:52 - INFO - Main -  no_cuda:False
2022/05/06 03:20:52 - INFO - Main -  gradient_accumulation_steps:1
2022/05/06 03:20:52 - INFO - Main -  local_rank:-1
2022/05/06 03:20:52 - INFO - Main -  fp16:False
2022/05/06 03:20:52 - INFO - Main -  random_seed:9580
2022/05/06 03:20:52 - INFO - Main -  fake_file_1:/home/hs3228/TextBrewer/data/DRCD/DRCD_training.json
2022/05/06 03:20:52 - INFO - Main -  fake_file_2:None
2022/05/06 03:20:52 - INFO - Main -  load_model_type:bert
2022/05/06 03:20:52 - INFO - Main -  weight_decay_rate:0.01
2022/05/06 03:20:52 - INFO - Main -  do_eval:True
2022/05/06 03:20:52 - INFO - Main -  PRINT_EVERY:200
2022/05/06 03:20:52 - INFO - Main -  weight:1.0
2022/05/06 03:20:52 - INFO - Main -  ckpt_frequency:1
2022/05/06 03:20:52 - INFO - Main -  tuned_checkpoint_T:/home/hs3228/TextBrewer/output_model/cmrc/teacher/cmrc2018_base_lr3e10_teacher/gs885.pkl
2022/05/06 03:20:52 - INFO - Main -  tuned_checkpoint_S:None
2022/05/06 03:20:52 - INFO - Main -  init_checkpoint_S:/home/hs3228/TextBrewer/model/chinese_roberta_wwm_ext_pytorch/pytorch_model.bin
2022/05/06 03:20:52 - INFO - Main -  bert_config_file_T:/home/hs3228/TextBrewer/model/chinese_roberta_wwm_ext_pytorch/bert_config.json
2022/05/06 03:20:52 - INFO - Main -  bert_config_file_S:../student_config/roberta_wwm_config/bert_config_L3.json
2022/05/06 03:20:52 - INFO - Main -  temperature:8.0
2022/05/06 03:20:52 - INFO - Main -  teacher_cached:False
2022/05/06 03:20:52 - INFO - Main -  s_opt1:1.0
2022/05/06 03:20:52 - INFO - Main -  s_opt2:0.0
2022/05/06 03:20:52 - INFO - Main -  s_opt3:1.0
2022/05/06 03:20:52 - INFO - Main -  schedule:slanted_triangular
2022/05/06 03:20:52 - INFO - Main -  null_score_diff_threshold:99.0
2022/05/06 03:20:52 - INFO - Main -  tag:RB
2022/05/06 03:20:52 - INFO - Main -  no_inputs_mask:False
2022/05/06 03:20:52 - INFO - Main -  no_logits:False
2022/05/06 03:20:52 - INFO - Main -  output_att_score:true
2022/05/06 03:20:52 - INFO - Main -  output_att_sum:false
2022/05/06 03:20:52 - INFO - Main -  output_encoded_layers:true
2022/05/06 03:20:52 - INFO - Main -  output_attention_layers:true
2022/05/06 03:20:52 - INFO - Main -  matches:['L3_hidden_mse', 'L3_hidden_smmd']
2022/05/06 03:20:52 - WARNING - Main -  Output directory () already exists and is not empty.
2022/05/06 03:20:52 - INFO - Main -  device cuda n_gpu 1 distributed training False
2022/05/06 03:20:52 - INFO - utils -  Loading dataset cmrc2018_train.json128_l512_cHA.tRB.pkl 
2022/05/06 03:20:56 - INFO - utils -  Loading dataset DRCD_training.json128_l512_cHA.tRB.pkl 
2022/05/06 03:21:08 - INFO - utils -  Loading dataset cmrc2018_dev.json128_l512_cHA.tRB.pkl 
2022/05/06 03:21:16 - INFO - Main -  Length of all_trainable_params: 2
2022/05/06 03:21:16 - INFO - Main -  ***** Running training *****
2022/05/06 03:21:16 - INFO - Main -    Num orig examples = 37078
2022/05/06 03:21:16 - INFO - Main -    Num split examples = 43945
2022/05/06 03:21:16 - INFO - Main -    Forward batch size = 12
2022/05/06 03:21:16 - INFO - Main -    Num backward steps = 36620
2022/05/06 03:21:16 - INFO - Main -  [{'layer_T': 0, 'layer_S': 0, 'feature': 'hidden', 'loss': 'hidden_mse', 'weight': 1}, {'layer_T': 4, 'layer_S': 1, 'feature': 'hidden', 'loss': 'hidden_mse', 'weight': 1}, {'layer_T': 8, 'layer_S': 2, 'feature': 'hidden', 'loss': 'hidden_mse', 'weight': 1}, {'layer_T': 12, 'layer_S': 3, 'feature': 'hidden', 'loss': 'hidden_mse', 'weight': 1}, {'layer_T': [0, 0], 'layer_S': [0, 0], 'feature': 'hidden', 'loss': 'mmd', 'weight': 1}, {'layer_T': [4, 4], 'layer_S': [1, 1], 'feature': 'hidden', 'loss': 'mmd', 'weight': 1}, {'layer_T': [8, 8], 'layer_S': [2, 2], 'feature': 'hidden', 'loss': 'mmd', 'weight': 1}, {'layer_T': [12, 12], 'layer_S': [3, 3], 'feature': 'hidden', 'loss': 'mmd', 'weight': 1}]
2022/05/06 03:21:19 - INFO - Distillation -  Training steps per epoch: 3662
2022/05/06 03:21:19 - INFO - Distillation -  Checkpoints(step): [0]
2022/05/06 03:21:19 - INFO - Distillation -  Epoch 1
2022/05/06 03:21:19 - INFO - Distillation -  Length of current epoch in forward batch: 3662
/home/hs3228/TextBrewer/examples/cmrc2018_example/optimization.py:181: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  next_m.mul_(beta1).add_(1 - beta1, grad)
2022/05/06 03:22:08 - INFO - Distillation -  Global step: 183, epoch step:183
2022/05/06 03:22:57 - INFO - Distillation -  Global step: 366, epoch step:366
2022/05/06 03:23:46 - INFO - Distillation -  Global step: 549, epoch step:549
2022/05/06 03:24:35 - INFO - Distillation -  Global step: 732, epoch step:732
2022/05/06 03:25:25 - INFO - Distillation -  Global step: 915, epoch step:915
2022/05/06 03:26:14 - INFO - Distillation -  Global step: 1098, epoch step:1098
2022/05/06 03:27:03 - INFO - Distillation -  Global step: 1281, epoch step:1281
2022/05/06 03:27:52 - INFO - Distillation -  Global step: 1464, epoch step:1464
2022/05/06 03:28:42 - INFO - Distillation -  Global step: 1647, epoch step:1647
2022/05/06 03:29:31 - INFO - Distillation -  Global step: 1830, epoch step:1830
2022/05/06 03:30:20 - INFO - Distillation -  Global step: 2013, epoch step:2013
2022/05/06 03:31:09 - INFO - Distillation -  Global step: 2196, epoch step:2196
2022/05/06 03:31:59 - INFO - Distillation -  Global step: 2379, epoch step:2379
2022/05/06 03:32:48 - INFO - Distillation -  Global step: 2562, epoch step:2562
2022/05/06 03:33:37 - INFO - Distillation -  Global step: 2745, epoch step:2745
2022/05/06 03:34:26 - INFO - Distillation -  Global step: 2928, epoch step:2928
2022/05/06 03:35:16 - INFO - Distillation -  Global step: 3111, epoch step:3111
2022/05/06 03:36:05 - INFO - Distillation -  Global step: 3294, epoch step:3294
2022/05/06 03:36:54 - INFO - Distillation -  Global step: 3477, epoch step:3477
2022/05/06 03:37:44 - INFO - Distillation -  Global step: 3660, epoch step:3660
2022/05/06 03:37:44 - INFO - Distillation -  Saving at global step 3662, epoch step 3662 epoch 1
2022/05/06 03:37:45 - INFO - Distillation -  Running callback function...
2022/05/06 03:37:45 - INFO - train_eval -  Predicting...
2022/05/06 03:37:45 - INFO - train_eval -  ***** Running predictions *****
2022/05/06 03:37:45 - INFO - train_eval -    Num orig examples = 3219
2022/05/06 03:37:45 - INFO - train_eval -    Num split examples = 5526
2022/05/06 03:37:45 - INFO - train_eval -    Batch size = 8
2022/05/06 03:37:45 - INFO - train_eval -  Start evaluating
2022/05/06 03:37:45 - INFO - train_eval -  Processing example: 0
2022/05/06 03:37:50 - INFO - train_eval -  Processing example: 1000
2022/05/06 03:37:53 - INFO - train_eval -  Processing example: 2000
2022/05/06 03:37:56 - INFO - train_eval -  Processing example: 3000
2022/05/06 03:37:59 - INFO - train_eval -  Processing example: 4000
2022/05/06 03:38:02 - INFO - train_eval -  Processing example: 5000
2022/05/06 03:38:03 - INFO - train_eval -  Write predictions...
2022/05/06 03:38:03 - INFO - processing -  Writing predictions to: /home/hs3228/TextBrewer/output_model/cmrc/student/cmrc2018_t8_TbaseST3_AllSmmdH1_lr15e10_opt1/predictions_3662.json
2022/05/06 03:38:19 - INFO - train_eval -  ***** Eval results 3662 *****
2022/05/06 03:38:19 - INFO - train_eval -  {"AVERAGE": "63.961", "F1": "75.391", "EM": "52.532", "TOTAL": 3219, "SKIP": 0}

2022/05/06 03:38:19 - INFO - Distillation -  Epoch 1 finished
2022/05/06 03:38:19 - INFO - Distillation -  Epoch 2
2022/05/06 03:38:19 - INFO - Distillation -  Length of current epoch in forward batch: 3662
2022/05/06 03:39:07 - INFO - Distillation -  Global step: 3843, epoch step:181
2022/05/06 03:39:57 - INFO - Distillation -  Global step: 4026, epoch step:364
2022/05/06 03:40:46 - INFO - Distillation -  Global step: 4209, epoch step:547
2022/05/06 03:41:36 - INFO - Distillation -  Global step: 4392, epoch step:730
2022/05/06 03:42:25 - INFO - Distillation -  Global step: 4575, epoch step:913
2022/05/06 03:43:14 - INFO - Distillation -  Global step: 4758, epoch step:1096
2022/05/06 03:44:04 - INFO - Distillation -  Global step: 4941, epoch step:1279
2022/05/06 03:44:53 - INFO - Distillation -  Global step: 5124, epoch step:1462
2022/05/06 03:45:42 - INFO - Distillation -  Global step: 5307, epoch step:1645
2022/05/06 03:46:31 - INFO - Distillation -  Global step: 5490, epoch step:1828
2022/05/06 03:47:21 - INFO - Distillation -  Global step: 5673, epoch step:2011
2022/05/06 03:48:10 - INFO - Distillation -  Global step: 5856, epoch step:2194
2022/05/06 03:48:59 - INFO - Distillation -  Global step: 6039, epoch step:2377
2022/05/06 03:49:49 - INFO - Distillation -  Global step: 6222, epoch step:2560
2022/05/06 03:50:38 - INFO - Distillation -  Global step: 6405, epoch step:2743
2022/05/06 03:51:27 - INFO - Distillation -  Global step: 6588, epoch step:2926
2022/05/06 03:52:16 - INFO - Distillation -  Global step: 6771, epoch step:3109
2022/05/06 03:53:06 - INFO - Distillation -  Global step: 6954, epoch step:3292
2022/05/06 03:53:55 - INFO - Distillation -  Global step: 7137, epoch step:3475
2022/05/06 03:54:44 - INFO - Distillation -  Global step: 7320, epoch step:3658
2022/05/06 03:54:45 - INFO - Distillation -  Saving at global step 7324, epoch step 3662 epoch 2
2022/05/06 03:54:46 - INFO - Distillation -  Running callback function...
2022/05/06 03:54:46 - INFO - train_eval -  Predicting...
2022/05/06 03:54:46 - INFO - train_eval -  ***** Running predictions *****
2022/05/06 03:54:46 - INFO - train_eval -    Num orig examples = 3219
2022/05/06 03:54:46 - INFO - train_eval -    Num split examples = 5526
2022/05/06 03:54:46 - INFO - train_eval -    Batch size = 8
2022/05/06 03:54:46 - INFO - train_eval -  Start evaluating
2022/05/06 03:54:46 - INFO - train_eval -  Processing example: 0
2022/05/06 03:54:49 - INFO - train_eval -  Processing example: 1000
2022/05/06 03:54:52 - INFO - train_eval -  Processing example: 2000
2022/05/06 03:54:55 - INFO - train_eval -  Processing example: 3000
2022/05/06 03:54:58 - INFO - train_eval -  Processing example: 4000
2022/05/06 03:55:01 - INFO - train_eval -  Processing example: 5000
2022/05/06 03:55:03 - INFO - train_eval -  Write predictions...
2022/05/06 03:55:03 - INFO - processing -  Writing predictions to: /home/hs3228/TextBrewer/output_model/cmrc/student/cmrc2018_t8_TbaseST3_AllSmmdH1_lr15e10_opt1/predictions_7324.json
2022/05/06 03:55:20 - INFO - train_eval -  ***** Eval results 7324 *****
2022/05/06 03:55:20 - INFO - train_eval -  {"AVERAGE": "65.752", "F1": "76.954", "EM": "54.551", "TOTAL": 3219, "SKIP": 0}

2022/05/06 03:55:20 - INFO - Distillation -  Epoch 2 finished
2022/05/06 03:55:20 - INFO - Distillation -  Epoch 3
2022/05/06 03:55:20 - INFO - Distillation -  Length of current epoch in forward batch: 3662
2022/05/06 03:56:08 - INFO - Distillation -  Global step: 7503, epoch step:179
2022/05/06 03:56:57 - INFO - Distillation -  Global step: 7686, epoch step:362
2022/05/06 03:57:47 - INFO - Distillation -  Global step: 7869, epoch step:545
2022/05/06 03:58:36 - INFO - Distillation -  Global step: 8052, epoch step:728
2022/05/06 03:59:26 - INFO - Distillation -  Global step: 8235, epoch step:911
2022/05/06 04:00:15 - INFO - Distillation -  Global step: 8418, epoch step:1094
2022/05/06 04:01:04 - INFO - Distillation -  Global step: 8601, epoch step:1277
2022/05/06 04:01:54 - INFO - Distillation -  Global step: 8784, epoch step:1460
2022/05/06 04:02:43 - INFO - Distillation -  Global step: 8967, epoch step:1643
2022/05/06 04:03:32 - INFO - Distillation -  Global step: 9150, epoch step:1826
2022/05/06 04:04:22 - INFO - Distillation -  Global step: 9333, epoch step:2009
2022/05/06 04:05:11 - INFO - Distillation -  Global step: 9516, epoch step:2192
2022/05/06 04:06:00 - INFO - Distillation -  Global step: 9699, epoch step:2375
2022/05/06 04:06:50 - INFO - Distillation -  Global step: 9882, epoch step:2558
2022/05/06 04:07:39 - INFO - Distillation -  Global step: 10065, epoch step:2741
2022/05/06 04:08:28 - INFO - Distillation -  Global step: 10248, epoch step:2924
2022/05/06 04:09:18 - INFO - Distillation -  Global step: 10431, epoch step:3107
2022/05/06 04:10:07 - INFO - Distillation -  Global step: 10614, epoch step:3290
2022/05/06 04:10:56 - INFO - Distillation -  Global step: 10797, epoch step:3473
2022/05/06 04:11:46 - INFO - Distillation -  Global step: 10980, epoch step:3656
2022/05/06 04:11:47 - INFO - Distillation -  Saving at global step 10986, epoch step 3662 epoch 3
2022/05/06 04:11:48 - INFO - Distillation -  Running callback function...
2022/05/06 04:11:48 - INFO - train_eval -  Predicting...
2022/05/06 04:11:48 - INFO - train_eval -  ***** Running predictions *****
2022/05/06 04:11:48 - INFO - train_eval -    Num orig examples = 3219
2022/05/06 04:11:48 - INFO - train_eval -    Num split examples = 5526
2022/05/06 04:11:48 - INFO - train_eval -    Batch size = 8
2022/05/06 04:11:48 - INFO - train_eval -  Start evaluating
2022/05/06 04:11:48 - INFO - train_eval -  Processing example: 0
2022/05/06 04:11:51 - INFO - train_eval -  Processing example: 1000
2022/05/06 04:11:54 - INFO - train_eval -  Processing example: 2000
2022/05/06 04:11:57 - INFO - train_eval -  Processing example: 3000
2022/05/06 04:12:00 - INFO - train_eval -  Processing example: 4000
2022/05/06 04:12:03 - INFO - train_eval -  Processing example: 5000
2022/05/06 04:12:05 - INFO - train_eval -  Write predictions...
2022/05/06 04:12:05 - INFO - processing -  Writing predictions to: /home/hs3228/TextBrewer/output_model/cmrc/student/cmrc2018_t8_TbaseST3_AllSmmdH1_lr15e10_opt1/predictions_10986.json
2022/05/06 04:12:20 - INFO - train_eval -  ***** Eval results 10986 *****
2022/05/06 04:12:20 - INFO - train_eval -  {"AVERAGE": "66.953", "F1": "78.111", "EM": "55.794", "TOTAL": 3219, "SKIP": 0}

2022/05/06 04:12:20 - INFO - Distillation -  Epoch 3 finished
2022/05/06 04:12:20 - INFO - Distillation -  Epoch 4
2022/05/06 04:12:20 - INFO - Distillation -  Length of current epoch in forward batch: 3662
2022/05/06 04:13:08 - INFO - Distillation -  Global step: 11163, epoch step:177
2022/05/06 04:13:57 - INFO - Distillation -  Global step: 11346, epoch step:360
2022/05/06 04:14:46 - INFO - Distillation -  Global step: 11529, epoch step:543
2022/05/06 04:15:36 - INFO - Distillation -  Global step: 11712, epoch step:726
2022/05/06 04:16:25 - INFO - Distillation -  Global step: 11895, epoch step:909
2022/05/06 04:17:14 - INFO - Distillation -  Global step: 12078, epoch step:1092
2022/05/06 04:18:04 - INFO - Distillation -  Global step: 12261, epoch step:1275
2022/05/06 04:18:53 - INFO - Distillation -  Global step: 12444, epoch step:1458
2022/05/06 04:19:42 - INFO - Distillation -  Global step: 12627, epoch step:1641
2022/05/06 04:20:32 - INFO - Distillation -  Global step: 12810, epoch step:1824
2022/05/06 04:21:21 - INFO - Distillation -  Global step: 12993, epoch step:2007
2022/05/06 04:22:10 - INFO - Distillation -  Global step: 13176, epoch step:2190
2022/05/06 04:22:59 - INFO - Distillation -  Global step: 13359, epoch step:2373
2022/05/06 04:23:49 - INFO - Distillation -  Global step: 13542, epoch step:2556
2022/05/06 04:24:38 - INFO - Distillation -  Global step: 13725, epoch step:2739
2022/05/06 04:25:27 - INFO - Distillation -  Global step: 13908, epoch step:2922
2022/05/06 04:26:17 - INFO - Distillation -  Global step: 14091, epoch step:3105
2022/05/06 04:27:06 - INFO - Distillation -  Global step: 14274, epoch step:3288
2022/05/06 04:27:55 - INFO - Distillation -  Global step: 14457, epoch step:3471
2022/05/06 04:28:45 - INFO - Distillation -  Global step: 14640, epoch step:3654
2022/05/06 04:28:47 - INFO - Distillation -  Saving at global step 14648, epoch step 3662 epoch 4
2022/05/06 04:28:47 - INFO - Distillation -  Running callback function...
2022/05/06 04:28:47 - INFO - train_eval -  Predicting...
2022/05/06 04:28:47 - INFO - train_eval -  ***** Running predictions *****
2022/05/06 04:28:47 - INFO - train_eval -    Num orig examples = 3219
2022/05/06 04:28:47 - INFO - train_eval -    Num split examples = 5526
2022/05/06 04:28:47 - INFO - train_eval -    Batch size = 8
2022/05/06 04:28:48 - INFO - train_eval -  Start evaluating
2022/05/06 04:28:48 - INFO - train_eval -  Processing example: 0
2022/05/06 04:28:51 - INFO - train_eval -  Processing example: 1000
2022/05/06 04:28:54 - INFO - train_eval -  Processing example: 2000
2022/05/06 04:28:57 - INFO - train_eval -  Processing example: 3000
2022/05/06 04:29:00 - INFO - train_eval -  Processing example: 4000
2022/05/06 04:29:03 - INFO - train_eval -  Processing example: 5000
2022/05/06 04:29:05 - INFO - train_eval -  Write predictions...
2022/05/06 04:29:05 - INFO - processing -  Writing predictions to: /home/hs3228/TextBrewer/output_model/cmrc/student/cmrc2018_t8_TbaseST3_AllSmmdH1_lr15e10_opt1/predictions_14648.json
2022/05/06 04:29:22 - INFO - train_eval -  ***** Eval results 14648 *****
2022/05/06 04:29:22 - INFO - train_eval -  {"AVERAGE": "69.453", "F1": "80.255", "EM": "58.652", "TOTAL": 3219, "SKIP": 0}

2022/05/06 04:29:22 - INFO - Distillation -  Epoch 4 finished
2022/05/06 04:29:22 - INFO - Distillation -  Epoch 5
2022/05/06 04:29:22 - INFO - Distillation -  Length of current epoch in forward batch: 3662
2022/05/06 04:30:09 - INFO - Distillation -  Global step: 14823, epoch step:175
2022/05/06 04:30:58 - INFO - Distillation -  Global step: 15006, epoch step:358
2022/05/06 04:31:47 - INFO - Distillation -  Global step: 15189, epoch step:541
2022/05/06 04:32:37 - INFO - Distillation -  Global step: 15372, epoch step:724
2022/05/06 04:33:26 - INFO - Distillation -  Global step: 15555, epoch step:907
2022/05/06 04:34:15 - INFO - Distillation -  Global step: 15738, epoch step:1090
2022/05/06 04:35:05 - INFO - Distillation -  Global step: 15921, epoch step:1273
2022/05/06 04:35:54 - INFO - Distillation -  Global step: 16104, epoch step:1456
2022/05/06 04:36:43 - INFO - Distillation -  Global step: 16287, epoch step:1639
2022/05/06 04:37:32 - INFO - Distillation -  Global step: 16470, epoch step:1822
2022/05/06 04:38:22 - INFO - Distillation -  Global step: 16653, epoch step:2005
2022/05/06 04:39:11 - INFO - Distillation -  Global step: 16836, epoch step:2188
2022/05/06 04:40:00 - INFO - Distillation -  Global step: 17019, epoch step:2371
2022/05/06 04:40:50 - INFO - Distillation -  Global step: 17202, epoch step:2554
2022/05/06 04:41:39 - INFO - Distillation -  Global step: 17385, epoch step:2737
2022/05/06 04:42:28 - INFO - Distillation -  Global step: 17568, epoch step:2920
2022/05/06 04:43:17 - INFO - Distillation -  Global step: 17751, epoch step:3103
2022/05/06 04:44:07 - INFO - Distillation -  Global step: 17934, epoch step:3286
2022/05/06 04:44:56 - INFO - Distillation -  Global step: 18117, epoch step:3469
2022/05/06 04:45:45 - INFO - Distillation -  Global step: 18300, epoch step:3652
2022/05/06 04:45:48 - INFO - Distillation -  Saving at global step 18310, epoch step 3662 epoch 5
2022/05/06 04:45:48 - INFO - Distillation -  Running callback function...
2022/05/06 04:45:48 - INFO - train_eval -  Predicting...
2022/05/06 04:45:48 - INFO - train_eval -  ***** Running predictions *****
2022/05/06 04:45:48 - INFO - train_eval -    Num orig examples = 3219
2022/05/06 04:45:48 - INFO - train_eval -    Num split examples = 5526
2022/05/06 04:45:48 - INFO - train_eval -    Batch size = 8
2022/05/06 04:45:48 - INFO - train_eval -  Start evaluating
2022/05/06 04:45:48 - INFO - train_eval -  Processing example: 0
2022/05/06 04:45:52 - INFO - train_eval -  Processing example: 1000
2022/05/06 04:45:55 - INFO - train_eval -  Processing example: 2000
2022/05/06 04:45:58 - INFO - train_eval -  Processing example: 3000
2022/05/06 04:46:01 - INFO - train_eval -  Processing example: 4000
2022/05/06 04:46:04 - INFO - train_eval -  Processing example: 5000
2022/05/06 04:46:05 - INFO - train_eval -  Write predictions...
2022/05/06 04:46:05 - INFO - processing -  Writing predictions to: /home/hs3228/TextBrewer/output_model/cmrc/student/cmrc2018_t8_TbaseST3_AllSmmdH1_lr15e10_opt1/predictions_18310.json
2022/05/06 04:46:21 - INFO - train_eval -  ***** Eval results 18310 *****
2022/05/06 04:46:21 - INFO - train_eval -  {"AVERAGE": "67.842", "F1": "79.052", "EM": "56.632", "TOTAL": 3219, "SKIP": 0}

2022/05/06 04:46:21 - INFO - Distillation -  Epoch 5 finished
2022/05/06 04:46:21 - INFO - Distillation -  Epoch 6
2022/05/06 04:46:21 - INFO - Distillation -  Length of current epoch in forward batch: 3662
2022/05/06 04:47:07 - INFO - Distillation -  Global step: 18483, epoch step:173
2022/05/06 04:47:56 - INFO - Distillation -  Global step: 18666, epoch step:356
2022/05/06 04:48:46 - INFO - Distillation -  Global step: 18849, epoch step:539
2022/05/06 04:49:35 - INFO - Distillation -  Global step: 19032, epoch step:722
2022/05/06 04:50:24 - INFO - Distillation -  Global step: 19215, epoch step:905
2022/05/06 04:51:14 - INFO - Distillation -  Global step: 19398, epoch step:1088
2022/05/06 04:52:03 - INFO - Distillation -  Global step: 19581, epoch step:1271
2022/05/06 04:52:52 - INFO - Distillation -  Global step: 19764, epoch step:1454
2022/05/06 04:53:41 - INFO - Distillation -  Global step: 19947, epoch step:1637
2022/05/06 04:54:31 - INFO - Distillation -  Global step: 20130, epoch step:1820
2022/05/06 04:55:20 - INFO - Distillation -  Global step: 20313, epoch step:2003
2022/05/06 04:56:09 - INFO - Distillation -  Global step: 20496, epoch step:2186
2022/05/06 04:56:59 - INFO - Distillation -  Global step: 20679, epoch step:2369
2022/05/06 04:57:48 - INFO - Distillation -  Global step: 20862, epoch step:2552
2022/05/06 04:58:37 - INFO - Distillation -  Global step: 21045, epoch step:2735
2022/05/06 04:59:26 - INFO - Distillation -  Global step: 21228, epoch step:2918
2022/05/06 05:00:15 - INFO - Distillation -  Global step: 21411, epoch step:3101
2022/05/06 05:01:05 - INFO - Distillation -  Global step: 21594, epoch step:3284
2022/05/06 05:01:54 - INFO - Distillation -  Global step: 21777, epoch step:3467
2022/05/06 05:02:43 - INFO - Distillation -  Global step: 21960, epoch step:3650
2022/05/06 05:02:46 - INFO - Distillation -  Saving at global step 21972, epoch step 3662 epoch 6
2022/05/06 05:02:47 - INFO - Distillation -  Running callback function...
2022/05/06 05:02:47 - INFO - train_eval -  Predicting...
2022/05/06 05:02:47 - INFO - train_eval -  ***** Running predictions *****
2022/05/06 05:02:47 - INFO - train_eval -    Num orig examples = 3219
2022/05/06 05:02:47 - INFO - train_eval -    Num split examples = 5526
2022/05/06 05:02:47 - INFO - train_eval -    Batch size = 8
2022/05/06 05:02:47 - INFO - train_eval -  Start evaluating
2022/05/06 05:02:47 - INFO - train_eval -  Processing example: 0
2022/05/06 05:02:50 - INFO - train_eval -  Processing example: 1000
2022/05/06 05:02:53 - INFO - train_eval -  Processing example: 2000
2022/05/06 05:02:56 - INFO - train_eval -  Processing example: 3000
2022/05/06 05:02:59 - INFO - train_eval -  Processing example: 4000
2022/05/06 05:03:02 - INFO - train_eval -  Processing example: 5000
2022/05/06 05:03:04 - INFO - train_eval -  Write predictions...
2022/05/06 05:03:04 - INFO - processing -  Writing predictions to: /home/hs3228/TextBrewer/output_model/cmrc/student/cmrc2018_t8_TbaseST3_AllSmmdH1_lr15e10_opt1/predictions_21972.json
2022/05/06 05:03:21 - INFO - train_eval -  ***** Eval results 21972 *****
2022/05/06 05:03:21 - INFO - train_eval -  {"AVERAGE": "70.179", "F1": "80.556", "EM": "59.801", "TOTAL": 3219, "SKIP": 0}

2022/05/06 05:03:21 - INFO - Distillation -  Epoch 6 finished
2022/05/06 05:03:21 - INFO - Distillation -  Epoch 7
2022/05/06 05:03:21 - INFO - Distillation -  Length of current epoch in forward batch: 3662
2022/05/06 05:04:06 - INFO - Distillation -  Global step: 22143, epoch step:171
2022/05/06 05:04:56 - INFO - Distillation -  Global step: 22326, epoch step:354
2022/05/06 05:05:45 - INFO - Distillation -  Global step: 22509, epoch step:537
2022/05/06 05:06:34 - INFO - Distillation -  Global step: 22692, epoch step:720
2022/05/06 05:07:23 - INFO - Distillation -  Global step: 22875, epoch step:903
2022/05/06 05:08:13 - INFO - Distillation -  Global step: 23058, epoch step:1086
2022/05/06 05:09:02 - INFO - Distillation -  Global step: 23241, epoch step:1269
2022/05/06 05:09:51 - INFO - Distillation -  Global step: 23424, epoch step:1452
2022/05/06 05:10:40 - INFO - Distillation -  Global step: 23607, epoch step:1635
2022/05/06 05:11:30 - INFO - Distillation -  Global step: 23790, epoch step:1818
2022/05/06 05:12:19 - INFO - Distillation -  Global step: 23973, epoch step:2001
2022/05/06 05:13:08 - INFO - Distillation -  Global step: 24156, epoch step:2184
2022/05/06 05:13:57 - INFO - Distillation -  Global step: 24339, epoch step:2367
2022/05/06 05:14:47 - INFO - Distillation -  Global step: 24522, epoch step:2550
2022/05/06 05:15:36 - INFO - Distillation -  Global step: 24705, epoch step:2733
2022/05/06 05:16:25 - INFO - Distillation -  Global step: 24888, epoch step:2916
2022/05/06 05:17:14 - INFO - Distillation -  Global step: 25071, epoch step:3099
2022/05/06 05:18:04 - INFO - Distillation -  Global step: 25254, epoch step:3282
2022/05/06 05:18:53 - INFO - Distillation -  Global step: 25437, epoch step:3465
2022/05/06 05:19:42 - INFO - Distillation -  Global step: 25620, epoch step:3648
2022/05/06 05:19:46 - INFO - Distillation -  Saving at global step 25634, epoch step 3662 epoch 7
2022/05/06 05:19:46 - INFO - Distillation -  Running callback function...
2022/05/06 05:19:46 - INFO - train_eval -  Predicting...
2022/05/06 05:19:46 - INFO - train_eval -  ***** Running predictions *****
2022/05/06 05:19:46 - INFO - train_eval -    Num orig examples = 3219
2022/05/06 05:19:46 - INFO - train_eval -    Num split examples = 5526
2022/05/06 05:19:46 - INFO - train_eval -    Batch size = 8
2022/05/06 05:19:47 - INFO - train_eval -  Start evaluating
2022/05/06 05:19:47 - INFO - train_eval -  Processing example: 0
2022/05/06 05:19:50 - INFO - train_eval -  Processing example: 1000
2022/05/06 05:19:53 - INFO - train_eval -  Processing example: 2000
2022/05/06 05:19:56 - INFO - train_eval -  Processing example: 3000
2022/05/06 05:19:59 - INFO - train_eval -  Processing example: 4000
2022/05/06 05:20:02 - INFO - train_eval -  Processing example: 5000
2022/05/06 05:20:03 - INFO - train_eval -  Write predictions...
2022/05/06 05:20:03 - INFO - processing -  Writing predictions to: /home/hs3228/TextBrewer/output_model/cmrc/student/cmrc2018_t8_TbaseST3_AllSmmdH1_lr15e10_opt1/predictions_25634.json
2022/05/06 05:20:20 - INFO - train_eval -  ***** Eval results 25634 *****
2022/05/06 05:20:20 - INFO - train_eval -  {"AVERAGE": "68.847", "F1": "79.788", "EM": "57.906", "TOTAL": 3219, "SKIP": 0}

2022/05/06 05:20:20 - INFO - Distillation -  Epoch 7 finished
2022/05/06 05:20:20 - INFO - Distillation -  Epoch 8
2022/05/06 05:20:20 - INFO - Distillation -  Length of current epoch in forward batch: 3662
2022/05/06 05:21:06 - INFO - Distillation -  Global step: 25803, epoch step:169
2022/05/06 05:21:55 - INFO - Distillation -  Global step: 25986, epoch step:352
2022/05/06 05:22:44 - INFO - Distillation -  Global step: 26169, epoch step:535
2022/05/06 05:23:33 - INFO - Distillation -  Global step: 26352, epoch step:718
2022/05/06 05:24:23 - INFO - Distillation -  Global step: 26535, epoch step:901
2022/05/06 05:25:12 - INFO - Distillation -  Global step: 26718, epoch step:1084
2022/05/06 05:26:01 - INFO - Distillation -  Global step: 26901, epoch step:1267
2022/05/06 05:26:50 - INFO - Distillation -  Global step: 27084, epoch step:1450
2022/05/06 05:27:40 - INFO - Distillation -  Global step: 27267, epoch step:1633
2022/05/06 05:28:29 - INFO - Distillation -  Global step: 27450, epoch step:1816
2022/05/06 05:29:18 - INFO - Distillation -  Global step: 27633, epoch step:1999
2022/05/06 05:30:08 - INFO - Distillation -  Global step: 27816, epoch step:2182
2022/05/06 05:30:57 - INFO - Distillation -  Global step: 27999, epoch step:2365
2022/05/06 05:31:46 - INFO - Distillation -  Global step: 28182, epoch step:2548
2022/05/06 05:32:35 - INFO - Distillation -  Global step: 28365, epoch step:2731
2022/05/06 05:33:25 - INFO - Distillation -  Global step: 28548, epoch step:2914
2022/05/06 05:34:14 - INFO - Distillation -  Global step: 28731, epoch step:3097
2022/05/06 05:35:03 - INFO - Distillation -  Global step: 28914, epoch step:3280
2022/05/06 05:35:52 - INFO - Distillation -  Global step: 29097, epoch step:3463
2022/05/06 05:36:42 - INFO - Distillation -  Global step: 29280, epoch step:3646
2022/05/06 05:36:46 - INFO - Distillation -  Saving at global step 29296, epoch step 3662 epoch 8
2022/05/06 05:36:46 - INFO - Distillation -  Running callback function...
2022/05/06 05:36:46 - INFO - train_eval -  Predicting...
2022/05/06 05:36:46 - INFO - train_eval -  ***** Running predictions *****
2022/05/06 05:36:46 - INFO - train_eval -    Num orig examples = 3219
2022/05/06 05:36:46 - INFO - train_eval -    Num split examples = 5526
2022/05/06 05:36:46 - INFO - train_eval -    Batch size = 8
2022/05/06 05:36:46 - INFO - train_eval -  Start evaluating
2022/05/06 05:36:46 - INFO - train_eval -  Processing example: 0
2022/05/06 05:36:49 - INFO - train_eval -  Processing example: 1000
2022/05/06 05:36:53 - INFO - train_eval -  Processing example: 2000
2022/05/06 05:36:56 - INFO - train_eval -  Processing example: 3000
2022/05/06 05:36:59 - INFO - train_eval -  Processing example: 4000
2022/05/06 05:37:02 - INFO - train_eval -  Processing example: 5000
2022/05/06 05:37:03 - INFO - train_eval -  Write predictions...
2022/05/06 05:37:03 - INFO - processing -  Writing predictions to: /home/hs3228/TextBrewer/output_model/cmrc/student/cmrc2018_t8_TbaseST3_AllSmmdH1_lr15e10_opt1/predictions_29296.json
2022/05/06 05:37:18 - INFO - train_eval -  ***** Eval results 29296 *****
2022/05/06 05:37:18 - INFO - train_eval -  {"AVERAGE": "70.022", "F1": "80.368", "EM": "59.677", "TOTAL": 3219, "SKIP": 0}

2022/05/06 05:37:18 - INFO - Distillation -  Epoch 8 finished
2022/05/06 05:37:18 - INFO - Distillation -  Epoch 9
2022/05/06 05:37:18 - INFO - Distillation -  Length of current epoch in forward batch: 3662
2022/05/06 05:38:03 - INFO - Distillation -  Global step: 29463, epoch step:167
2022/05/06 05:38:52 - INFO - Distillation -  Global step: 29646, epoch step:350
2022/05/06 05:39:42 - INFO - Distillation -  Global step: 29829, epoch step:533
2022/05/06 05:40:31 - INFO - Distillation -  Global step: 30012, epoch step:716
2022/05/06 05:41:20 - INFO - Distillation -  Global step: 30195, epoch step:899
2022/05/06 05:42:09 - INFO - Distillation -  Global step: 30378, epoch step:1082
2022/05/06 05:42:58 - INFO - Distillation -  Global step: 30561, epoch step:1265
2022/05/06 05:43:48 - INFO - Distillation -  Global step: 30744, epoch step:1448
2022/05/06 05:44:37 - INFO - Distillation -  Global step: 30927, epoch step:1631
2022/05/06 05:45:26 - INFO - Distillation -  Global step: 31110, epoch step:1814
2022/05/06 05:46:15 - INFO - Distillation -  Global step: 31293, epoch step:1997
2022/05/06 05:47:04 - INFO - Distillation -  Global step: 31476, epoch step:2180
2022/05/06 05:47:54 - INFO - Distillation -  Global step: 31659, epoch step:2363
2022/05/06 05:48:43 - INFO - Distillation -  Global step: 31842, epoch step:2546
2022/05/06 05:49:32 - INFO - Distillation -  Global step: 32025, epoch step:2729
2022/05/06 05:50:21 - INFO - Distillation -  Global step: 32208, epoch step:2912
2022/05/06 05:51:10 - INFO - Distillation -  Global step: 32391, epoch step:3095
2022/05/06 05:52:00 - INFO - Distillation -  Global step: 32574, epoch step:3278
2022/05/06 05:52:49 - INFO - Distillation -  Global step: 32757, epoch step:3461
2022/05/06 05:53:38 - INFO - Distillation -  Global step: 32940, epoch step:3644
2022/05/06 05:53:43 - INFO - Distillation -  Saving at global step 32958, epoch step 3662 epoch 9
2022/05/06 05:53:43 - INFO - Distillation -  Running callback function...
2022/05/06 05:53:43 - INFO - train_eval -  Predicting...
2022/05/06 05:53:43 - INFO - train_eval -  ***** Running predictions *****
2022/05/06 05:53:43 - INFO - train_eval -    Num orig examples = 3219
2022/05/06 05:53:43 - INFO - train_eval -    Num split examples = 5526
2022/05/06 05:53:43 - INFO - train_eval -    Batch size = 8
2022/05/06 05:53:43 - INFO - train_eval -  Start evaluating
2022/05/06 05:53:43 - INFO - train_eval -  Processing example: 0
2022/05/06 05:53:46 - INFO - train_eval -  Processing example: 1000
2022/05/06 05:53:49 - INFO - train_eval -  Processing example: 2000
2022/05/06 05:53:52 - INFO - train_eval -  Processing example: 3000
2022/05/06 05:53:56 - INFO - train_eval -  Processing example: 4000
2022/05/06 05:53:59 - INFO - train_eval -  Processing example: 5000
2022/05/06 05:54:00 - INFO - train_eval -  Write predictions...
2022/05/06 05:54:00 - INFO - processing -  Writing predictions to: /home/hs3228/TextBrewer/output_model/cmrc/student/cmrc2018_t8_TbaseST3_AllSmmdH1_lr15e10_opt1/predictions_32958.json
2022/05/06 05:54:16 - INFO - train_eval -  ***** Eval results 32958 *****
2022/05/06 05:54:16 - INFO - train_eval -  {"AVERAGE": "69.620", "F1": "80.246", "EM": "58.993", "TOTAL": 3219, "SKIP": 0}

2022/05/06 05:54:17 - INFO - Distillation -  Epoch 9 finished
2022/05/06 05:54:17 - INFO - Distillation -  Epoch 10
2022/05/06 05:54:17 - INFO - Distillation -  Length of current epoch in forward batch: 3662
2022/05/06 05:55:01 - INFO - Distillation -  Global step: 33123, epoch step:165
2022/05/06 05:55:50 - INFO - Distillation -  Global step: 33306, epoch step:348
2022/05/06 05:56:39 - INFO - Distillation -  Global step: 33489, epoch step:531
2022/05/06 05:57:28 - INFO - Distillation -  Global step: 33672, epoch step:714
2022/05/06 05:58:18 - INFO - Distillation -  Global step: 33855, epoch step:897
2022/05/06 05:59:07 - INFO - Distillation -  Global step: 34038, epoch step:1080
2022/05/06 05:59:56 - INFO - Distillation -  Global step: 34221, epoch step:1263
2022/05/06 06:00:45 - INFO - Distillation -  Global step: 34404, epoch step:1446
2022/05/06 06:01:34 - INFO - Distillation -  Global step: 34587, epoch step:1629
2022/05/06 06:02:24 - INFO - Distillation -  Global step: 34770, epoch step:1812
2022/05/06 06:03:13 - INFO - Distillation -  Global step: 34953, epoch step:1995
2022/05/06 06:04:02 - INFO - Distillation -  Global step: 35136, epoch step:2178
2022/05/06 06:04:51 - INFO - Distillation -  Global step: 35319, epoch step:2361
2022/05/06 06:05:40 - INFO - Distillation -  Global step: 35502, epoch step:2544
2022/05/06 06:06:30 - INFO - Distillation -  Global step: 35685, epoch step:2727
2022/05/06 06:07:19 - INFO - Distillation -  Global step: 35868, epoch step:2910
2022/05/06 06:08:08 - INFO - Distillation -  Global step: 36051, epoch step:3093
2022/05/06 06:08:57 - INFO - Distillation -  Global step: 36234, epoch step:3276
2022/05/06 06:09:46 - INFO - Distillation -  Global step: 36417, epoch step:3459
2022/05/06 06:10:36 - INFO - Distillation -  Global step: 36600, epoch step:3642
2022/05/06 06:10:41 - INFO - Distillation -  Saving at global step 36620, epoch step 3662 epoch 10
2022/05/06 06:10:41 - INFO - Distillation -  Running callback function...
2022/05/06 06:10:41 - INFO - train_eval -  Predicting...
2022/05/06 06:10:41 - INFO - train_eval -  ***** Running predictions *****
2022/05/06 06:10:41 - INFO - train_eval -    Num orig examples = 3219
2022/05/06 06:10:41 - INFO - train_eval -    Num split examples = 5526
2022/05/06 06:10:41 - INFO - train_eval -    Batch size = 8
2022/05/06 06:10:42 - INFO - train_eval -  Start evaluating
2022/05/06 06:10:42 - INFO - train_eval -  Processing example: 0
2022/05/06 06:10:45 - INFO - train_eval -  Processing example: 1000
2022/05/06 06:10:48 - INFO - train_eval -  Processing example: 2000
2022/05/06 06:10:51 - INFO - train_eval -  Processing example: 3000
2022/05/06 06:10:54 - INFO - train_eval -  Processing example: 4000
2022/05/06 06:10:57 - INFO - train_eval -  Processing example: 5000
2022/05/06 06:10:59 - INFO - train_eval -  Write predictions...
2022/05/06 06:10:59 - INFO - processing -  Writing predictions to: /home/hs3228/TextBrewer/output_model/cmrc/student/cmrc2018_t8_TbaseST3_AllSmmdH1_lr15e10_opt1/predictions_36620.json
2022/05/06 06:11:13 - INFO - train_eval -  ***** Eval results 36620 *****
2022/05/06 06:11:13 - INFO - train_eval -  {"AVERAGE": "71.713", "F1": "81.606", "EM": "61.820", "TOTAL": 3219, "SKIP": 0}

2022/05/06 06:11:13 - INFO - Distillation -  Epoch 10 finished
