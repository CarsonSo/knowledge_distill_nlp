nohup: ignoring input
05/05/2022 20:56:37 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
05/05/2022 20:56:37 - INFO - transformers.configuration_utils -   loading configuration file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/config.json
05/05/2022 20:56:37 - INFO - transformers.configuration_utils -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 9,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

05/05/2022 20:56:37 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' is a path or url to a directory containing tokenizer files.
05/05/2022 20:56:37 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/vocab.txt
05/05/2022 20:56:37 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/added_tokens.json
05/05/2022 20:56:37 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/special_tokens_map.json
05/05/2022 20:56:37 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/tokenizer_config.json
05/05/2022 20:56:37 - INFO - transformers.modeling_utils -   loading weights file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/pytorch_model.bin
05/05/2022 20:56:39 - INFO - transformers.configuration_utils -   loading configuration file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/config.json
05/05/2022 20:56:39 - INFO - transformers.configuration_utils -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 9,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

05/05/2022 20:56:39 - INFO - transformers.modeling_utils -   loading weights file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/pytorch_model.bin
05/05/2022 20:56:41 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForTokenClassification: ['bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']
05/05/2022 20:56:44 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/home/hs3228/TextBrewer/data/conll', device=device(type='cuda'), do_distill=True, do_eval=True, do_lower_case=False, do_predict=True, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, labels='', learning_rate=0.0001, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw', model_name_or_path_student=None, model_type='bert', n_gpu=1, no_cuda=False, num_hidden_layers=5, num_train_epochs=20.0, output_dir='/home/hs3228/TextBrewer/output_model/conll_distill/student_raw_layer5', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=32, save_steps=750, seed=42, server_ip='', server_port='', tokenizer_name='', warmup_steps=0.1, weight_decay=0.0)
05/05/2022 20:56:44 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_train_teacher_raw_128
05/05/2022 20:56:46 - INFO - __main__ -   ***** Running training *****
05/05/2022 20:56:46 - INFO - __main__ -     Num examples = 14041
05/05/2022 20:56:46 - INFO - __main__ -     Num Epochs = 20
05/05/2022 20:56:46 - INFO - __main__ -     Instantaneous batch size per GPU = 32
05/05/2022 20:56:46 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32
05/05/2022 20:56:46 - INFO - __main__ -     Gradient Accumulation steps = 1
05/05/2022 20:56:46 - INFO - __main__ -     Total optimization steps = 8780
05/05/2022 20:56:46 - INFO - Distillation -   Training steps per epoch: 439
05/05/2022 20:56:46 - INFO - Distillation -   Checkpoints(step): [0]
05/05/2022 20:56:46 - INFO - Distillation -   Epoch 1
05/05/2022 20:56:46 - INFO - Distillation -   Length of current epoch in forward batch: 439
/home/hs3228/miniconda3/envs/prac/lib/python3.6/site-packages/transformers/optimization.py:146: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
05/05/2022 20:56:50 - INFO - Distillation -   Global step: 21, epoch step:21
05/05/2022 20:56:53 - INFO - Distillation -   Global step: 42, epoch step:42
05/05/2022 20:56:57 - INFO - Distillation -   Global step: 63, epoch step:63
05/05/2022 20:57:01 - INFO - Distillation -   Global step: 84, epoch step:84
05/05/2022 20:57:05 - INFO - Distillation -   Global step: 105, epoch step:105
05/05/2022 20:57:08 - INFO - Distillation -   Global step: 126, epoch step:126
05/05/2022 20:57:12 - INFO - Distillation -   Global step: 147, epoch step:147
05/05/2022 20:57:16 - INFO - Distillation -   Global step: 168, epoch step:168
05/05/2022 20:57:20 - INFO - Distillation -   Global step: 189, epoch step:189
05/05/2022 20:57:23 - INFO - Distillation -   Global step: 210, epoch step:210
05/05/2022 20:57:27 - INFO - Distillation -   Global step: 231, epoch step:231
05/05/2022 20:57:31 - INFO - Distillation -   Global step: 252, epoch step:252
05/05/2022 20:57:35 - INFO - Distillation -   Global step: 273, epoch step:273
05/05/2022 20:57:39 - INFO - Distillation -   Global step: 294, epoch step:294
05/05/2022 20:57:42 - INFO - Distillation -   Global step: 315, epoch step:315
05/05/2022 20:57:46 - INFO - Distillation -   Global step: 336, epoch step:336
05/05/2022 20:57:50 - INFO - Distillation -   Global step: 357, epoch step:357
05/05/2022 20:57:54 - INFO - Distillation -   Global step: 378, epoch step:378
05/05/2022 20:57:57 - INFO - Distillation -   Global step: 399, epoch step:399
05/05/2022 20:58:01 - INFO - Distillation -   Global step: 420, epoch step:420
05/05/2022 20:58:05 - INFO - Distillation -   Saving at global step 439, epoch step 439 epoch 1
05/05/2022 20:58:06 - INFO - Distillation -   Running callback function...
05/05/2022 20:58:06 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' is a path or url to a directory containing tokenizer files.
05/05/2022 20:58:06 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/vocab.txt
05/05/2022 20:58:06 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/added_tokens.json
05/05/2022 20:58:06 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/special_tokens_map.json
05/05/2022 20:58:06 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/tokenizer_config.json
05/05/2022 20:58:06 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_raw_128
05/05/2022 20:58:07 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 20:58:07 - INFO - __main__ -     Num examples = 3453
05/05/2022 20:58:07 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 103.28it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:03, 103.76it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 103.51it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 102.73it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 101.40it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 100.67it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 100.45it/s]Evaluating:  20%|██        | 88/432 [00:00<00:03, 100.28it/s]Evaluating:  23%|██▎       | 99/432 [00:00<00:03, 100.05it/s]Evaluating:  25%|██▌       | 110/432 [00:01<00:03, 99.30it/s]Evaluating:  28%|██▊       | 120/432 [00:01<00:03, 98.30it/s]Evaluating:  30%|███       | 130/432 [00:01<00:03, 97.56it/s]Evaluating:  32%|███▏      | 140/432 [00:01<00:03, 97.02it/s]Evaluating:  35%|███▍      | 150/432 [00:01<00:02, 96.68it/s]Evaluating:  37%|███▋      | 160/432 [00:01<00:02, 95.52it/s]Evaluating:  39%|███▉      | 170/432 [00:01<00:02, 94.35it/s]Evaluating:  42%|████▏     | 180/432 [00:01<00:02, 93.22it/s]Evaluating:  44%|████▍     | 190/432 [00:01<00:02, 91.49it/s]Evaluating:  46%|████▋     | 200/432 [00:02<00:02, 90.30it/s]Evaluating:  49%|████▊     | 210/432 [00:02<00:02, 89.19it/s]Evaluating:  51%|█████     | 219/432 [00:02<00:02, 88.31it/s]Evaluating:  53%|█████▎    | 228/432 [00:02<00:02, 87.59it/s]Evaluating:  55%|█████▍    | 237/432 [00:02<00:02, 86.88it/s]Evaluating:  57%|█████▋    | 246/432 [00:02<00:02, 86.03it/s]Evaluating:  59%|█████▉    | 255/432 [00:02<00:02, 85.02it/s]Evaluating:  61%|██████    | 264/432 [00:02<00:01, 84.62it/s]Evaluating:  63%|██████▎   | 273/432 [00:02<00:01, 84.24it/s]Evaluating:  65%|██████▌   | 282/432 [00:03<00:01, 83.94it/s]Evaluating:  67%|██████▋   | 291/432 [00:03<00:01, 83.45it/s]Evaluating:  69%|██████▉   | 300/432 [00:03<00:01, 82.97it/s]Evaluating:  72%|███████▏  | 309/432 [00:03<00:01, 82.24it/s]Evaluating:  74%|███████▎  | 318/432 [00:03<00:01, 81.69it/s]Evaluating:  76%|███████▌  | 327/432 [00:03<00:01, 81.10it/s]Evaluating:  78%|███████▊  | 336/432 [00:03<00:01, 80.31it/s]Evaluating:  80%|███████▉  | 345/432 [00:03<00:01, 79.18it/s]Evaluating:  82%|████████▏ | 353/432 [00:03<00:01, 78.84it/s]Evaluating:  84%|████████▎ | 361/432 [00:04<00:00, 78.62it/s]Evaluating:  85%|████████▌ | 369/432 [00:04<00:00, 78.10it/s]Evaluating:  87%|████████▋ | 377/432 [00:04<00:00, 77.74it/s]Evaluating:  89%|████████▉ | 385/432 [00:04<00:00, 77.53it/s]Evaluating:  91%|█████████ | 393/432 [00:04<00:00, 77.31it/s]Evaluating:  93%|█████████▎| 401/432 [00:04<00:00, 77.00it/s]Evaluating:  95%|█████████▍| 409/432 [00:04<00:00, 76.36it/s]Evaluating:  97%|█████████▋| 417/432 [00:04<00:00, 75.57it/s]Evaluating:  98%|█████████▊| 425/432 [00:04<00:00, 75.11it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 87.03it/s]
05/05/2022 20:58:12 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 20:58:12 - INFO - __main__ -     f1 = 0.8735410267661254
05/05/2022 20:58:12 - INFO - __main__ -     loss = 0.15095579962139205
05/05/2022 20:58:12 - INFO - __main__ -     precision = 0.865414710485133
05/05/2022 20:58:12 - INFO - __main__ -     recall = 0.8818214032600992
05/05/2022 20:58:12 - INFO - Distillation -   Epoch 1 finished
05/05/2022 20:58:12 - INFO - Distillation -   Epoch 2
05/05/2022 20:58:12 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 20:58:13 - INFO - Distillation -   Global step: 441, epoch step:2
05/05/2022 20:58:17 - INFO - Distillation -   Global step: 462, epoch step:23
05/05/2022 20:58:20 - INFO - Distillation -   Global step: 483, epoch step:44
05/05/2022 20:58:24 - INFO - Distillation -   Global step: 504, epoch step:65
05/05/2022 20:58:28 - INFO - Distillation -   Global step: 525, epoch step:86
05/05/2022 20:58:32 - INFO - Distillation -   Global step: 546, epoch step:107
05/05/2022 20:58:36 - INFO - Distillation -   Global step: 567, epoch step:128
05/05/2022 20:58:39 - INFO - Distillation -   Global step: 588, epoch step:149
05/05/2022 20:58:43 - INFO - Distillation -   Global step: 609, epoch step:170
05/05/2022 20:58:47 - INFO - Distillation -   Global step: 630, epoch step:191
05/05/2022 20:58:51 - INFO - Distillation -   Global step: 651, epoch step:212
05/05/2022 20:58:55 - INFO - Distillation -   Global step: 672, epoch step:233
05/05/2022 20:58:58 - INFO - Distillation -   Global step: 693, epoch step:254
05/05/2022 20:59:02 - INFO - Distillation -   Global step: 714, epoch step:275
05/05/2022 20:59:06 - INFO - Distillation -   Global step: 735, epoch step:296
05/05/2022 20:59:10 - INFO - Distillation -   Global step: 756, epoch step:317
05/05/2022 20:59:14 - INFO - Distillation -   Global step: 777, epoch step:338
05/05/2022 20:59:17 - INFO - Distillation -   Global step: 798, epoch step:359
05/05/2022 20:59:21 - INFO - Distillation -   Global step: 819, epoch step:380
05/05/2022 20:59:25 - INFO - Distillation -   Global step: 840, epoch step:401
05/05/2022 20:59:29 - INFO - Distillation -   Global step: 861, epoch step:422
05/05/2022 20:59:32 - INFO - Distillation -   Saving at global step 878, epoch step 439 epoch 2
05/05/2022 20:59:34 - INFO - Distillation -   Running callback function...
05/05/2022 20:59:34 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' is a path or url to a directory containing tokenizer files.
05/05/2022 20:59:34 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/vocab.txt
05/05/2022 20:59:34 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/added_tokens.json
05/05/2022 20:59:34 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/special_tokens_map.json
05/05/2022 20:59:34 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/tokenizer_config.json
05/05/2022 20:59:34 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_raw_128
05/05/2022 20:59:34 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 20:59:34 - INFO - __main__ -     Num examples = 3453
05/05/2022 20:59:34 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 103.10it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:03, 102.96it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 102.87it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 101.72it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 101.05it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 100.29it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 100.00it/s]Evaluating:  20%|██        | 88/432 [00:00<00:03, 100.01it/s]Evaluating:  23%|██▎       | 99/432 [00:00<00:03, 99.80it/s] Evaluating:  25%|██▌       | 109/432 [00:01<00:03, 99.34it/s]Evaluating:  28%|██▊       | 119/432 [00:01<00:03, 98.54it/s]Evaluating:  30%|██▉       | 129/432 [00:01<00:03, 97.43it/s]Evaluating:  32%|███▏      | 139/432 [00:01<00:03, 96.55it/s]Evaluating:  34%|███▍      | 149/432 [00:01<00:02, 95.95it/s]Evaluating:  37%|███▋      | 159/432 [00:01<00:02, 95.42it/s]Evaluating:  39%|███▉      | 169/432 [00:01<00:02, 94.20it/s]Evaluating:  41%|████▏     | 179/432 [00:01<00:02, 93.34it/s]Evaluating:  44%|████▍     | 189/432 [00:01<00:02, 92.22it/s]Evaluating:  46%|████▌     | 199/432 [00:02<00:02, 91.03it/s]Evaluating:  48%|████▊     | 209/432 [00:02<00:02, 89.69it/s]Evaluating:  50%|█████     | 218/432 [00:02<00:02, 88.79it/s]Evaluating:  53%|█████▎    | 227/432 [00:02<00:02, 88.19it/s]Evaluating:  55%|█████▍    | 236/432 [00:02<00:02, 87.63it/s]Evaluating:  57%|█████▋    | 245/432 [00:02<00:02, 87.03it/s]Evaluating:  59%|█████▉    | 254/432 [00:02<00:02, 86.20it/s]Evaluating:  61%|██████    | 263/432 [00:02<00:01, 85.40it/s]Evaluating:  63%|██████▎   | 272/432 [00:02<00:01, 84.69it/s]Evaluating:  65%|██████▌   | 281/432 [00:03<00:01, 84.07it/s]Evaluating:  67%|██████▋   | 290/432 [00:03<00:01, 83.37it/s]Evaluating:  69%|██████▉   | 299/432 [00:03<00:01, 82.74it/s]Evaluating:  71%|███████▏  | 308/432 [00:03<00:01, 82.21it/s]Evaluating:  73%|███████▎  | 317/432 [00:03<00:01, 81.76it/s]Evaluating:  75%|███████▌  | 326/432 [00:03<00:01, 81.35it/s]Evaluating:  78%|███████▊  | 335/432 [00:03<00:01, 80.92it/s]Evaluating:  80%|███████▉  | 344/432 [00:03<00:01, 80.76it/s]Evaluating:  82%|████████▏ | 353/432 [00:03<00:00, 80.46it/s]Evaluating:  84%|████████▍ | 362/432 [00:04<00:00, 79.82it/s]Evaluating:  86%|████████▌ | 370/432 [00:04<00:00, 79.27it/s]Evaluating:  88%|████████▊ | 378/432 [00:04<00:00, 78.75it/s]Evaluating:  89%|████████▉ | 386/432 [00:04<00:00, 77.98it/s]Evaluating:  91%|█████████ | 394/432 [00:04<00:00, 77.45it/s]Evaluating:  93%|█████████▎| 402/432 [00:04<00:00, 77.03it/s]Evaluating:  95%|█████████▍| 410/432 [00:04<00:00, 76.40it/s]Evaluating:  97%|█████████▋| 418/432 [00:04<00:00, 75.74it/s]Evaluating:  99%|█████████▊| 426/432 [00:04<00:00, 75.17it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 87.27it/s]
05/05/2022 20:59:40 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 20:59:40 - INFO - __main__ -     f1 = 0.8792034238798149
05/05/2022 20:59:40 - INFO - __main__ -     loss = 0.17340308786591171
05/05/2022 20:59:40 - INFO - __main__ -     precision = 0.8670111972437554
05/05/2022 20:59:40 - INFO - __main__ -     recall = 0.8917434443656981
05/05/2022 20:59:40 - INFO - Distillation -   Epoch 2 finished
05/05/2022 20:59:40 - INFO - Distillation -   Epoch 3
05/05/2022 20:59:40 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 20:59:40 - INFO - Distillation -   Global step: 882, epoch step:4
05/05/2022 20:59:44 - INFO - Distillation -   Global step: 903, epoch step:25
05/05/2022 20:59:48 - INFO - Distillation -   Global step: 924, epoch step:46
05/05/2022 20:59:52 - INFO - Distillation -   Global step: 945, epoch step:67
05/05/2022 20:59:56 - INFO - Distillation -   Global step: 966, epoch step:88
05/05/2022 20:59:59 - INFO - Distillation -   Global step: 987, epoch step:109
05/05/2022 21:00:03 - INFO - Distillation -   Global step: 1008, epoch step:130
05/05/2022 21:00:07 - INFO - Distillation -   Global step: 1029, epoch step:151
05/05/2022 21:00:11 - INFO - Distillation -   Global step: 1050, epoch step:172
05/05/2022 21:00:15 - INFO - Distillation -   Global step: 1071, epoch step:193
05/05/2022 21:00:18 - INFO - Distillation -   Global step: 1092, epoch step:214
05/05/2022 21:00:22 - INFO - Distillation -   Global step: 1113, epoch step:235
05/05/2022 21:00:26 - INFO - Distillation -   Global step: 1134, epoch step:256
05/05/2022 21:00:30 - INFO - Distillation -   Global step: 1155, epoch step:277
05/05/2022 21:00:34 - INFO - Distillation -   Global step: 1176, epoch step:298
05/05/2022 21:00:37 - INFO - Distillation -   Global step: 1197, epoch step:319
05/05/2022 21:00:41 - INFO - Distillation -   Global step: 1218, epoch step:340
05/05/2022 21:00:45 - INFO - Distillation -   Global step: 1239, epoch step:361
05/05/2022 21:00:49 - INFO - Distillation -   Global step: 1260, epoch step:382
05/05/2022 21:00:53 - INFO - Distillation -   Global step: 1281, epoch step:403
05/05/2022 21:00:56 - INFO - Distillation -   Global step: 1302, epoch step:424
05/05/2022 21:00:59 - INFO - Distillation -   Saving at global step 1317, epoch step 439 epoch 3
05/05/2022 21:01:01 - INFO - Distillation -   Running callback function...
05/05/2022 21:01:01 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' is a path or url to a directory containing tokenizer files.
05/05/2022 21:01:01 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/vocab.txt
05/05/2022 21:01:01 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/added_tokens.json
05/05/2022 21:01:01 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/special_tokens_map.json
05/05/2022 21:01:01 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/tokenizer_config.json
05/05/2022 21:01:01 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_raw_128
05/05/2022 21:01:01 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 21:01:01 - INFO - __main__ -     Num examples = 3453
05/05/2022 21:01:01 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 102.20it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:04, 101.61it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 101.54it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 100.52it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 99.50it/s] Evaluating:  15%|█▌        | 65/432 [00:00<00:03, 98.85it/s]Evaluating:  17%|█▋        | 75/432 [00:00<00:03, 98.27it/s]Evaluating:  20%|█▉        | 85/432 [00:00<00:03, 97.94it/s]Evaluating:  22%|██▏       | 95/432 [00:00<00:03, 97.99it/s]Evaluating:  24%|██▍       | 105/432 [00:01<00:03, 97.86it/s]Evaluating:  27%|██▋       | 115/432 [00:01<00:03, 97.02it/s]Evaluating:  29%|██▉       | 125/432 [00:01<00:03, 96.83it/s]Evaluating:  31%|███▏      | 135/432 [00:01<00:03, 95.85it/s]Evaluating:  34%|███▎      | 145/432 [00:01<00:03, 94.81it/s]Evaluating:  36%|███▌      | 155/432 [00:01<00:02, 93.81it/s]Evaluating:  38%|███▊      | 165/432 [00:01<00:02, 93.17it/s]Evaluating:  41%|████      | 175/432 [00:01<00:02, 92.79it/s]Evaluating:  43%|████▎     | 185/432 [00:01<00:02, 92.33it/s]Evaluating:  45%|████▌     | 195/432 [00:02<00:02, 91.43it/s]Evaluating:  47%|████▋     | 205/432 [00:02<00:02, 90.64it/s]Evaluating:  50%|████▉     | 215/432 [00:02<00:02, 89.36it/s]Evaluating:  52%|█████▏    | 224/432 [00:02<00:02, 88.23it/s]Evaluating:  54%|█████▍    | 233/432 [00:02<00:02, 87.33it/s]Evaluating:  56%|█████▌    | 242/432 [00:02<00:02, 86.40it/s]Evaluating:  58%|█████▊    | 251/432 [00:02<00:02, 85.86it/s]Evaluating:  60%|██████    | 260/432 [00:02<00:02, 85.25it/s]Evaluating:  62%|██████▏   | 269/432 [00:02<00:01, 84.73it/s]Evaluating:  64%|██████▍   | 278/432 [00:03<00:01, 84.27it/s]Evaluating:  66%|██████▋   | 287/432 [00:03<00:01, 83.47it/s]Evaluating:  69%|██████▊   | 296/432 [00:03<00:01, 83.02it/s]Evaluating:  71%|███████   | 305/432 [00:03<00:01, 82.60it/s]Evaluating:  73%|███████▎  | 314/432 [00:03<00:01, 81.92it/s]Evaluating:  75%|███████▍  | 323/432 [00:03<00:01, 81.61it/s]Evaluating:  77%|███████▋  | 332/432 [00:03<00:01, 81.13it/s]Evaluating:  79%|███████▉  | 341/432 [00:03<00:01, 80.68it/s]Evaluating:  81%|████████  | 350/432 [00:03<00:01, 80.09it/s]Evaluating:  83%|████████▎ | 359/432 [00:04<00:00, 79.64it/s]Evaluating:  85%|████████▍ | 367/432 [00:04<00:00, 79.34it/s]Evaluating:  87%|████████▋ | 375/432 [00:04<00:00, 78.88it/s]Evaluating:  89%|████████▊ | 383/432 [00:04<00:00, 78.14it/s]Evaluating:  91%|█████████ | 391/432 [00:04<00:00, 77.45it/s]Evaluating:  92%|█████████▏| 399/432 [00:04<00:00, 76.81it/s]Evaluating:  94%|█████████▍| 407/432 [00:04<00:00, 76.33it/s]Evaluating:  96%|█████████▌| 415/432 [00:04<00:00, 75.82it/s]Evaluating:  98%|█████████▊| 423/432 [00:04<00:00, 75.19it/s]Evaluating: 100%|█████████▉| 431/432 [00:04<00:00, 74.66it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 86.74it/s]
05/05/2022 21:01:07 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 21:01:07 - INFO - __main__ -     f1 = 0.8905647141353911
05/05/2022 21:01:07 - INFO - __main__ -     loss = 0.16415802653171307
05/05/2022 21:01:07 - INFO - __main__ -     precision = 0.8815972222222223
05/05/2022 21:01:07 - INFO - __main__ -     recall = 0.8997165131112687
05/05/2022 21:01:07 - INFO - Distillation -   Epoch 3 finished
05/05/2022 21:01:07 - INFO - Distillation -   Epoch 4
05/05/2022 21:01:07 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 21:01:08 - INFO - Distillation -   Global step: 1323, epoch step:6
05/05/2022 21:01:12 - INFO - Distillation -   Global step: 1344, epoch step:27
05/05/2022 21:01:16 - INFO - Distillation -   Global step: 1365, epoch step:48
05/05/2022 21:01:19 - INFO - Distillation -   Global step: 1386, epoch step:69
05/05/2022 21:01:23 - INFO - Distillation -   Global step: 1407, epoch step:90
05/05/2022 21:01:27 - INFO - Distillation -   Global step: 1428, epoch step:111
05/05/2022 21:01:31 - INFO - Distillation -   Global step: 1449, epoch step:132
05/05/2022 21:01:35 - INFO - Distillation -   Global step: 1470, epoch step:153
05/05/2022 21:01:38 - INFO - Distillation -   Global step: 1491, epoch step:174
05/05/2022 21:01:42 - INFO - Distillation -   Global step: 1512, epoch step:195
05/05/2022 21:01:46 - INFO - Distillation -   Global step: 1533, epoch step:216
05/05/2022 21:01:50 - INFO - Distillation -   Global step: 1554, epoch step:237
05/05/2022 21:01:54 - INFO - Distillation -   Global step: 1575, epoch step:258
05/05/2022 21:01:57 - INFO - Distillation -   Global step: 1596, epoch step:279
05/05/2022 21:02:01 - INFO - Distillation -   Global step: 1617, epoch step:300
05/05/2022 21:02:05 - INFO - Distillation -   Global step: 1638, epoch step:321
05/05/2022 21:02:09 - INFO - Distillation -   Global step: 1659, epoch step:342
05/05/2022 21:02:13 - INFO - Distillation -   Global step: 1680, epoch step:363
05/05/2022 21:02:16 - INFO - Distillation -   Global step: 1701, epoch step:384
05/05/2022 21:02:20 - INFO - Distillation -   Global step: 1722, epoch step:405
05/05/2022 21:02:24 - INFO - Distillation -   Global step: 1743, epoch step:426
05/05/2022 21:02:26 - INFO - Distillation -   Saving at global step 1756, epoch step 439 epoch 4
05/05/2022 21:02:27 - INFO - Distillation -   Running callback function...
05/05/2022 21:02:27 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' is a path or url to a directory containing tokenizer files.
05/05/2022 21:02:27 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/vocab.txt
05/05/2022 21:02:27 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/added_tokens.json
05/05/2022 21:02:27 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/special_tokens_map.json
05/05/2022 21:02:27 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/tokenizer_config.json
05/05/2022 21:02:27 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_raw_128
05/05/2022 21:02:27 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 21:02:27 - INFO - __main__ -     Num examples = 3453
05/05/2022 21:02:27 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 102.74it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:04, 102.36it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 102.29it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 101.82it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 100.91it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 99.95it/s] Evaluating:  18%|█▊        | 76/432 [00:00<00:03, 99.80it/s]Evaluating:  20%|██        | 87/432 [00:00<00:03, 100.08it/s]Evaluating:  23%|██▎       | 98/432 [00:00<00:03, 99.95it/s] Evaluating:  25%|██▌       | 108/432 [00:01<00:03, 99.62it/s]Evaluating:  27%|██▋       | 118/432 [00:01<00:03, 98.50it/s]Evaluating:  30%|██▉       | 128/432 [00:01<00:03, 97.25it/s]Evaluating:  32%|███▏      | 138/432 [00:01<00:03, 96.11it/s]Evaluating:  34%|███▍      | 148/432 [00:01<00:02, 95.18it/s]Evaluating:  37%|███▋      | 158/432 [00:01<00:02, 94.62it/s]Evaluating:  39%|███▉      | 168/432 [00:01<00:02, 93.66it/s]Evaluating:  41%|████      | 178/432 [00:01<00:02, 92.82it/s]Evaluating:  44%|████▎     | 188/432 [00:01<00:02, 91.96it/s]Evaluating:  46%|████▌     | 198/432 [00:02<00:02, 90.92it/s]Evaluating:  48%|████▊     | 208/432 [00:02<00:02, 89.94it/s]Evaluating:  50%|█████     | 217/432 [00:02<00:02, 88.74it/s]Evaluating:  52%|█████▏    | 226/432 [00:02<00:02, 87.86it/s]Evaluating:  54%|█████▍    | 235/432 [00:02<00:02, 87.12it/s]Evaluating:  56%|█████▋    | 244/432 [00:02<00:02, 86.59it/s]Evaluating:  59%|█████▊    | 253/432 [00:02<00:02, 85.68it/s]Evaluating:  61%|██████    | 262/432 [00:02<00:01, 85.05it/s]Evaluating:  63%|██████▎   | 271/432 [00:02<00:01, 84.38it/s]Evaluating:  65%|██████▍   | 280/432 [00:03<00:01, 83.67it/s]Evaluating:  67%|██████▋   | 289/432 [00:03<00:01, 83.18it/s]Evaluating:  69%|██████▉   | 298/432 [00:03<00:01, 82.58it/s]Evaluating:  71%|███████   | 307/432 [00:03<00:01, 81.98it/s]Evaluating:  73%|███████▎  | 316/432 [00:03<00:01, 81.25it/s]Evaluating:  75%|███████▌  | 325/432 [00:03<00:01, 80.88it/s]Evaluating:  77%|███████▋  | 334/432 [00:03<00:01, 80.41it/s]Evaluating:  79%|███████▉  | 343/432 [00:03<00:01, 79.92it/s]Evaluating:  81%|████████▏ | 351/432 [00:03<00:01, 79.55it/s]Evaluating:  83%|████████▎ | 359/432 [00:04<00:00, 79.16it/s]Evaluating:  85%|████████▍ | 367/432 [00:04<00:00, 78.93it/s]Evaluating:  87%|████████▋ | 375/432 [00:04<00:00, 78.59it/s]Evaluating:  89%|████████▊ | 383/432 [00:04<00:00, 78.14it/s]Evaluating:  91%|█████████ | 391/432 [00:04<00:00, 77.73it/s]Evaluating:  92%|█████████▏| 399/432 [00:04<00:00, 77.09it/s]Evaluating:  94%|█████████▍| 407/432 [00:04<00:00, 76.72it/s]Evaluating:  96%|█████████▌| 415/432 [00:04<00:00, 76.21it/s]Evaluating:  98%|█████████▊| 423/432 [00:04<00:00, 75.67it/s]Evaluating: 100%|█████████▉| 431/432 [00:04<00:00, 75.24it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 87.01it/s]
05/05/2022 21:02:33 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 21:02:33 - INFO - __main__ -     f1 = 0.8938929291159206
05/05/2022 21:02:33 - INFO - __main__ -     loss = 0.1563947630818937
05/05/2022 21:02:33 - INFO - __main__ -     precision = 0.8842087016814006
05/05/2022 21:02:33 - INFO - __main__ -     recall = 0.9037916371367825
05/05/2022 21:02:33 - INFO - Distillation -   Epoch 4 finished
05/05/2022 21:02:33 - INFO - Distillation -   Epoch 5
05/05/2022 21:02:33 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 21:02:34 - INFO - Distillation -   Global step: 1764, epoch step:8
05/05/2022 21:02:38 - INFO - Distillation -   Global step: 1785, epoch step:29
05/05/2022 21:02:42 - INFO - Distillation -   Global step: 1806, epoch step:50
05/05/2022 21:02:46 - INFO - Distillation -   Global step: 1827, epoch step:71
05/05/2022 21:02:49 - INFO - Distillation -   Global step: 1848, epoch step:92
05/05/2022 21:02:53 - INFO - Distillation -   Global step: 1869, epoch step:113
05/05/2022 21:02:57 - INFO - Distillation -   Global step: 1890, epoch step:134
05/05/2022 21:03:01 - INFO - Distillation -   Global step: 1911, epoch step:155
05/05/2022 21:03:05 - INFO - Distillation -   Global step: 1932, epoch step:176
05/05/2022 21:03:08 - INFO - Distillation -   Global step: 1953, epoch step:197
05/05/2022 21:03:12 - INFO - Distillation -   Global step: 1974, epoch step:218
05/05/2022 21:03:16 - INFO - Distillation -   Global step: 1995, epoch step:239
05/05/2022 21:03:20 - INFO - Distillation -   Global step: 2016, epoch step:260
05/05/2022 21:03:24 - INFO - Distillation -   Global step: 2037, epoch step:281
05/05/2022 21:03:27 - INFO - Distillation -   Global step: 2058, epoch step:302
05/05/2022 21:03:31 - INFO - Distillation -   Global step: 2079, epoch step:323
05/05/2022 21:03:35 - INFO - Distillation -   Global step: 2100, epoch step:344
05/05/2022 21:03:39 - INFO - Distillation -   Global step: 2121, epoch step:365
05/05/2022 21:03:43 - INFO - Distillation -   Global step: 2142, epoch step:386
05/05/2022 21:03:46 - INFO - Distillation -   Global step: 2163, epoch step:407
05/05/2022 21:03:50 - INFO - Distillation -   Global step: 2184, epoch step:428
05/05/2022 21:03:52 - INFO - Distillation -   Saving at global step 2195, epoch step 439 epoch 5
05/05/2022 21:03:53 - INFO - Distillation -   Running callback function...
05/05/2022 21:03:53 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' is a path or url to a directory containing tokenizer files.
05/05/2022 21:03:53 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/vocab.txt
05/05/2022 21:03:53 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/added_tokens.json
05/05/2022 21:03:53 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/special_tokens_map.json
05/05/2022 21:03:53 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/tokenizer_config.json
05/05/2022 21:03:53 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_raw_128
05/05/2022 21:03:53 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 21:03:53 - INFO - __main__ -     Num examples = 3453
05/05/2022 21:03:53 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 103.56it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:03, 103.28it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 102.54it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 102.14it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 101.41it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 101.00it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 100.43it/s]Evaluating:  20%|██        | 88/432 [00:00<00:03, 100.11it/s]Evaluating:  23%|██▎       | 99/432 [00:00<00:03, 99.62it/s] Evaluating:  25%|██▌       | 109/432 [00:01<00:03, 98.89it/s]Evaluating:  28%|██▊       | 119/432 [00:01<00:03, 98.10it/s]Evaluating:  30%|██▉       | 129/432 [00:01<00:03, 97.28it/s]Evaluating:  32%|███▏      | 139/432 [00:01<00:03, 96.20it/s]Evaluating:  34%|███▍      | 149/432 [00:01<00:02, 95.61it/s]Evaluating:  37%|███▋      | 159/432 [00:01<00:02, 94.90it/s]Evaluating:  39%|███▉      | 169/432 [00:01<00:02, 94.15it/s]Evaluating:  41%|████▏     | 179/432 [00:01<00:02, 93.09it/s]Evaluating:  44%|████▍     | 189/432 [00:01<00:02, 91.79it/s]Evaluating:  46%|████▌     | 199/432 [00:02<00:02, 90.67it/s]Evaluating:  48%|████▊     | 209/432 [00:02<00:02, 89.31it/s]Evaluating:  50%|█████     | 218/432 [00:02<00:02, 88.26it/s]Evaluating:  53%|█████▎    | 227/432 [00:02<00:02, 87.38it/s]Evaluating:  55%|█████▍    | 236/432 [00:02<00:02, 86.78it/s]Evaluating:  57%|█████▋    | 245/432 [00:02<00:02, 86.23it/s]Evaluating:  59%|█████▉    | 254/432 [00:02<00:02, 85.61it/s]Evaluating:  61%|██████    | 263/432 [00:02<00:01, 84.99it/s]Evaluating:  63%|██████▎   | 272/432 [00:02<00:01, 84.40it/s]Evaluating:  65%|██████▌   | 281/432 [00:03<00:01, 83.85it/s]Evaluating:  67%|██████▋   | 290/432 [00:03<00:01, 83.15it/s]Evaluating:  69%|██████▉   | 299/432 [00:03<00:01, 82.60it/s]Evaluating:  71%|███████▏  | 308/432 [00:03<00:01, 81.93it/s]Evaluating:  73%|███████▎  | 317/432 [00:03<00:01, 81.24it/s]Evaluating:  75%|███████▌  | 326/432 [00:03<00:01, 80.85it/s]Evaluating:  78%|███████▊  | 335/432 [00:03<00:01, 80.19it/s]Evaluating:  80%|███████▉  | 344/432 [00:03<00:01, 79.64it/s]Evaluating:  81%|████████▏ | 352/432 [00:03<00:01, 79.63it/s]Evaluating:  83%|████████▎ | 360/432 [00:04<00:00, 79.55it/s]Evaluating:  85%|████████▌ | 368/432 [00:04<00:00, 79.19it/s]Evaluating:  87%|████████▋ | 376/432 [00:04<00:00, 79.08it/s]Evaluating:  89%|████████▉ | 384/432 [00:04<00:00, 78.65it/s]Evaluating:  91%|█████████ | 392/432 [00:04<00:00, 77.52it/s]Evaluating:  93%|█████████▎| 400/432 [00:04<00:00, 76.80it/s]Evaluating:  94%|█████████▍| 408/432 [00:04<00:00, 76.71it/s]Evaluating:  96%|█████████▋| 416/432 [00:04<00:00, 76.28it/s]Evaluating:  98%|█████████▊| 424/432 [00:04<00:00, 75.81it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 76.18it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 87.11it/s]
05/05/2022 21:03:59 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 21:03:59 - INFO - __main__ -     f1 = 0.8962016453702084
05/05/2022 21:03:59 - INFO - __main__ -     loss = 0.15179528876227003
05/05/2022 21:03:59 - INFO - __main__ -     precision = 0.8855067450709098
05/05/2022 21:03:59 - INFO - __main__ -     recall = 0.9071580439404677
05/05/2022 21:03:59 - INFO - Distillation -   Epoch 5 finished
05/05/2022 21:03:59 - INFO - Distillation -   Epoch 6
05/05/2022 21:03:59 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 21:04:00 - INFO - Distillation -   Global step: 2205, epoch step:10
05/05/2022 21:04:04 - INFO - Distillation -   Global step: 2226, epoch step:31
05/05/2022 21:04:08 - INFO - Distillation -   Global step: 2247, epoch step:52
05/05/2022 21:04:12 - INFO - Distillation -   Global step: 2268, epoch step:73
05/05/2022 21:04:16 - INFO - Distillation -   Global step: 2289, epoch step:94
05/05/2022 21:04:19 - INFO - Distillation -   Global step: 2310, epoch step:115
05/05/2022 21:04:23 - INFO - Distillation -   Global step: 2331, epoch step:136
05/05/2022 21:04:27 - INFO - Distillation -   Global step: 2352, epoch step:157
05/05/2022 21:04:31 - INFO - Distillation -   Global step: 2373, epoch step:178
05/05/2022 21:04:35 - INFO - Distillation -   Global step: 2394, epoch step:199
05/05/2022 21:04:38 - INFO - Distillation -   Global step: 2415, epoch step:220
05/05/2022 21:04:42 - INFO - Distillation -   Global step: 2436, epoch step:241
05/05/2022 21:04:46 - INFO - Distillation -   Global step: 2457, epoch step:262
05/05/2022 21:04:50 - INFO - Distillation -   Global step: 2478, epoch step:283
05/05/2022 21:04:54 - INFO - Distillation -   Global step: 2499, epoch step:304
05/05/2022 21:04:57 - INFO - Distillation -   Global step: 2520, epoch step:325
05/05/2022 21:05:01 - INFO - Distillation -   Global step: 2541, epoch step:346
05/05/2022 21:05:05 - INFO - Distillation -   Global step: 2562, epoch step:367
05/05/2022 21:05:09 - INFO - Distillation -   Global step: 2583, epoch step:388
05/05/2022 21:05:13 - INFO - Distillation -   Global step: 2604, epoch step:409
05/05/2022 21:05:16 - INFO - Distillation -   Global step: 2625, epoch step:430
05/05/2022 21:05:18 - INFO - Distillation -   Saving at global step 2634, epoch step 439 epoch 6
05/05/2022 21:05:18 - INFO - Distillation -   Running callback function...
05/05/2022 21:05:18 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' is a path or url to a directory containing tokenizer files.
05/05/2022 21:05:18 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/vocab.txt
05/05/2022 21:05:18 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/added_tokens.json
05/05/2022 21:05:18 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/special_tokens_map.json
05/05/2022 21:05:18 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/tokenizer_config.json
05/05/2022 21:05:18 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_raw_128
05/05/2022 21:05:19 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 21:05:19 - INFO - __main__ -     Num examples = 3453
05/05/2022 21:05:19 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 102.65it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:04, 102.48it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 102.26it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 102.01it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 101.40it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 101.00it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 100.27it/s]Evaluating:  20%|██        | 88/432 [00:00<00:03, 99.37it/s] Evaluating:  23%|██▎       | 98/432 [00:00<00:03, 98.95it/s]Evaluating:  25%|██▌       | 108/432 [00:01<00:03, 98.33it/s]Evaluating:  27%|██▋       | 118/432 [00:01<00:03, 97.91it/s]Evaluating:  30%|██▉       | 128/432 [00:01<00:03, 97.60it/s]Evaluating:  32%|███▏      | 138/432 [00:01<00:03, 96.39it/s]Evaluating:  34%|███▍      | 148/432 [00:01<00:02, 95.14it/s]Evaluating:  37%|███▋      | 158/432 [00:01<00:02, 94.05it/s]Evaluating:  39%|███▉      | 168/432 [00:01<00:02, 93.27it/s]Evaluating:  41%|████      | 178/432 [00:01<00:02, 92.61it/s]Evaluating:  44%|████▎     | 188/432 [00:01<00:02, 91.62it/s]Evaluating:  46%|████▌     | 198/432 [00:02<00:02, 90.66it/s]Evaluating:  48%|████▊     | 208/432 [00:02<00:02, 89.75it/s]Evaluating:  50%|█████     | 217/432 [00:02<00:02, 88.77it/s]Evaluating:  52%|█████▏    | 226/432 [00:02<00:02, 87.87it/s]Evaluating:  54%|█████▍    | 235/432 [00:02<00:02, 86.87it/s]Evaluating:  56%|█████▋    | 244/432 [00:02<00:02, 86.00it/s]Evaluating:  59%|█████▊    | 253/432 [00:02<00:02, 85.33it/s]Evaluating:  61%|██████    | 262/432 [00:02<00:01, 85.03it/s]Evaluating:  63%|██████▎   | 271/432 [00:02<00:01, 84.50it/s]Evaluating:  65%|██████▍   | 280/432 [00:03<00:01, 83.88it/s]Evaluating:  67%|██████▋   | 289/432 [00:03<00:01, 83.26it/s]Evaluating:  69%|██████▉   | 298/432 [00:03<00:01, 82.57it/s]Evaluating:  71%|███████   | 307/432 [00:03<00:01, 81.96it/s]Evaluating:  73%|███████▎  | 316/432 [00:03<00:01, 81.44it/s]Evaluating:  75%|███████▌  | 325/432 [00:03<00:01, 80.88it/s]Evaluating:  77%|███████▋  | 334/432 [00:03<00:01, 80.09it/s]Evaluating:  79%|███████▉  | 343/432 [00:03<00:01, 79.50it/s]Evaluating:  81%|████████▏ | 351/432 [00:03<00:01, 78.97it/s]Evaluating:  83%|████████▎ | 359/432 [00:04<00:00, 78.60it/s]Evaluating:  85%|████████▍ | 367/432 [00:04<00:00, 78.11it/s]Evaluating:  87%|████████▋ | 375/432 [00:04<00:00, 77.72it/s]Evaluating:  89%|████████▊ | 383/432 [00:04<00:00, 77.31it/s]Evaluating:  91%|█████████ | 391/432 [00:04<00:00, 77.16it/s]Evaluating:  92%|█████████▏| 399/432 [00:04<00:00, 76.56it/s]Evaluating:  94%|█████████▍| 407/432 [00:04<00:00, 76.15it/s]Evaluating:  96%|█████████▌| 415/432 [00:04<00:00, 75.61it/s]Evaluating:  98%|█████████▊| 423/432 [00:04<00:00, 75.51it/s]Evaluating: 100%|█████████▉| 431/432 [00:04<00:00, 75.17it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 86.77it/s]
05/05/2022 21:05:25 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 21:05:25 - INFO - __main__ -     f1 = 0.8981993851559068
05/05/2022 21:05:25 - INFO - __main__ -     loss = 0.1495566031574324
05/05/2022 21:05:25 - INFO - __main__ -     precision = 0.8906113917435987
05/05/2022 21:05:25 - INFO - __main__ -     recall = 0.9059177888022679
05/05/2022 21:05:25 - INFO - Distillation -   Epoch 6 finished
05/05/2022 21:05:25 - INFO - Distillation -   Epoch 7
05/05/2022 21:05:25 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 21:05:27 - INFO - Distillation -   Global step: 2646, epoch step:12
05/05/2022 21:05:30 - INFO - Distillation -   Global step: 2667, epoch step:33
05/05/2022 21:05:34 - INFO - Distillation -   Global step: 2688, epoch step:54
05/05/2022 21:05:38 - INFO - Distillation -   Global step: 2709, epoch step:75
05/05/2022 21:05:42 - INFO - Distillation -   Global step: 2730, epoch step:96
05/05/2022 21:05:46 - INFO - Distillation -   Global step: 2751, epoch step:117
05/05/2022 21:05:49 - INFO - Distillation -   Global step: 2772, epoch step:138
05/05/2022 21:05:53 - INFO - Distillation -   Global step: 2793, epoch step:159
05/05/2022 21:05:57 - INFO - Distillation -   Global step: 2814, epoch step:180
05/05/2022 21:06:01 - INFO - Distillation -   Global step: 2835, epoch step:201
05/05/2022 21:06:05 - INFO - Distillation -   Global step: 2856, epoch step:222
05/05/2022 21:06:08 - INFO - Distillation -   Global step: 2877, epoch step:243
05/05/2022 21:06:12 - INFO - Distillation -   Global step: 2898, epoch step:264
05/05/2022 21:06:16 - INFO - Distillation -   Global step: 2919, epoch step:285
05/05/2022 21:06:20 - INFO - Distillation -   Global step: 2940, epoch step:306
05/05/2022 21:06:24 - INFO - Distillation -   Global step: 2961, epoch step:327
05/05/2022 21:06:27 - INFO - Distillation -   Global step: 2982, epoch step:348
05/05/2022 21:06:31 - INFO - Distillation -   Global step: 3003, epoch step:369
05/05/2022 21:06:35 - INFO - Distillation -   Global step: 3024, epoch step:390
05/05/2022 21:06:39 - INFO - Distillation -   Global step: 3045, epoch step:411
05/05/2022 21:06:43 - INFO - Distillation -   Global step: 3066, epoch step:432
05/05/2022 21:06:44 - INFO - Distillation -   Saving at global step 3073, epoch step 439 epoch 7
05/05/2022 21:06:44 - INFO - Distillation -   Running callback function...
05/05/2022 21:06:44 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' is a path or url to a directory containing tokenizer files.
05/05/2022 21:06:44 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/vocab.txt
05/05/2022 21:06:44 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/added_tokens.json
05/05/2022 21:06:44 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/special_tokens_map.json
05/05/2022 21:06:44 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/tokenizer_config.json
05/05/2022 21:06:44 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_raw_128
05/05/2022 21:06:45 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 21:06:45 - INFO - __main__ -     Num examples = 3453
05/05/2022 21:06:45 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 102.47it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:03, 102.91it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 102.38it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 101.79it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 100.80it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 100.21it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 100.05it/s]Evaluating:  20%|██        | 88/432 [00:00<00:03, 99.93it/s] Evaluating:  23%|██▎       | 98/432 [00:00<00:03, 99.61it/s]Evaluating:  25%|██▌       | 108/432 [00:01<00:03, 98.81it/s]Evaluating:  27%|██▋       | 118/432 [00:01<00:03, 97.78it/s]Evaluating:  30%|██▉       | 128/432 [00:01<00:03, 97.28it/s]Evaluating:  32%|███▏      | 138/432 [00:01<00:03, 95.78it/s]Evaluating:  34%|███▍      | 148/432 [00:01<00:02, 94.94it/s]Evaluating:  37%|███▋      | 158/432 [00:01<00:02, 94.24it/s]Evaluating:  39%|███▉      | 168/432 [00:01<00:02, 93.22it/s]Evaluating:  41%|████      | 178/432 [00:01<00:02, 91.97it/s]Evaluating:  44%|████▎     | 188/432 [00:01<00:02, 91.16it/s]Evaluating:  46%|████▌     | 198/432 [00:02<00:02, 90.45it/s]Evaluating:  48%|████▊     | 208/432 [00:02<00:02, 89.22it/s]Evaluating:  50%|█████     | 217/432 [00:02<00:02, 88.51it/s]Evaluating:  52%|█████▏    | 226/432 [00:02<00:02, 87.85it/s]Evaluating:  54%|█████▍    | 235/432 [00:02<00:02, 87.16it/s]Evaluating:  56%|█████▋    | 244/432 [00:02<00:02, 86.43it/s]Evaluating:  59%|█████▊    | 253/432 [00:02<00:02, 85.41it/s]Evaluating:  61%|██████    | 262/432 [00:02<00:02, 84.69it/s]Evaluating:  63%|██████▎   | 271/432 [00:02<00:01, 83.90it/s]Evaluating:  65%|██████▍   | 280/432 [00:03<00:01, 83.19it/s]Evaluating:  67%|██████▋   | 289/432 [00:03<00:01, 82.40it/s]Evaluating:  69%|██████▉   | 298/432 [00:03<00:01, 81.81it/s]Evaluating:  71%|███████   | 307/432 [00:03<00:01, 81.45it/s]Evaluating:  73%|███████▎  | 316/432 [00:03<00:01, 80.95it/s]Evaluating:  75%|███████▌  | 325/432 [00:03<00:01, 80.52it/s]Evaluating:  77%|███████▋  | 334/432 [00:03<00:01, 80.07it/s]Evaluating:  79%|███████▉  | 343/432 [00:03<00:01, 79.40it/s]Evaluating:  81%|████████▏ | 351/432 [00:03<00:01, 79.02it/s]Evaluating:  83%|████████▎ | 359/432 [00:04<00:00, 78.84it/s]Evaluating:  85%|████████▍ | 367/432 [00:04<00:00, 78.37it/s]Evaluating:  87%|████████▋ | 375/432 [00:04<00:00, 77.86it/s]Evaluating:  89%|████████▊ | 383/432 [00:04<00:00, 77.56it/s]Evaluating:  91%|█████████ | 391/432 [00:04<00:00, 77.24it/s]Evaluating:  92%|█████████▏| 399/432 [00:04<00:00, 76.85it/s]Evaluating:  94%|█████████▍| 407/432 [00:04<00:00, 76.39it/s]Evaluating:  96%|█████████▌| 415/432 [00:04<00:00, 75.77it/s]Evaluating:  98%|█████████▊| 423/432 [00:04<00:00, 75.39it/s]Evaluating: 100%|█████████▉| 431/432 [00:04<00:00, 75.02it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 86.64it/s]
05/05/2022 21:06:50 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 21:06:50 - INFO - __main__ -     f1 = 0.8994895264918149
05/05/2022 21:06:50 - INFO - __main__ -     loss = 0.1471182390108778
05/05/2022 21:06:50 - INFO - __main__ -     precision = 0.8936691150752011
05/05/2022 21:06:50 - INFO - __main__ -     recall = 0.9053862508858965
05/05/2022 21:06:50 - INFO - Distillation -   Epoch 7 finished
05/05/2022 21:06:50 - INFO - Distillation -   Epoch 8
05/05/2022 21:06:50 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 21:06:53 - INFO - Distillation -   Global step: 3087, epoch step:14
05/05/2022 21:06:57 - INFO - Distillation -   Global step: 3108, epoch step:35
05/05/2022 21:07:00 - INFO - Distillation -   Global step: 3129, epoch step:56
05/05/2022 21:07:04 - INFO - Distillation -   Global step: 3150, epoch step:77
05/05/2022 21:07:08 - INFO - Distillation -   Global step: 3171, epoch step:98
05/05/2022 21:07:12 - INFO - Distillation -   Global step: 3192, epoch step:119
05/05/2022 21:07:16 - INFO - Distillation -   Global step: 3213, epoch step:140
05/05/2022 21:07:20 - INFO - Distillation -   Global step: 3234, epoch step:161
05/05/2022 21:07:23 - INFO - Distillation -   Global step: 3255, epoch step:182
05/05/2022 21:07:27 - INFO - Distillation -   Global step: 3276, epoch step:203
05/05/2022 21:07:31 - INFO - Distillation -   Global step: 3297, epoch step:224
05/05/2022 21:07:35 - INFO - Distillation -   Global step: 3318, epoch step:245
05/05/2022 21:07:39 - INFO - Distillation -   Global step: 3339, epoch step:266
05/05/2022 21:07:42 - INFO - Distillation -   Global step: 3360, epoch step:287
05/05/2022 21:07:46 - INFO - Distillation -   Global step: 3381, epoch step:308
05/05/2022 21:07:50 - INFO - Distillation -   Global step: 3402, epoch step:329
05/05/2022 21:07:54 - INFO - Distillation -   Global step: 3423, epoch step:350
05/05/2022 21:07:58 - INFO - Distillation -   Global step: 3444, epoch step:371
05/05/2022 21:08:01 - INFO - Distillation -   Global step: 3465, epoch step:392
05/05/2022 21:08:05 - INFO - Distillation -   Global step: 3486, epoch step:413
05/05/2022 21:08:09 - INFO - Distillation -   Global step: 3507, epoch step:434
05/05/2022 21:08:10 - INFO - Distillation -   Saving at global step 3512, epoch step 439 epoch 8
05/05/2022 21:08:10 - INFO - Distillation -   Running callback function...
05/05/2022 21:08:10 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' is a path or url to a directory containing tokenizer files.
05/05/2022 21:08:10 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/vocab.txt
05/05/2022 21:08:10 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/added_tokens.json
05/05/2022 21:08:10 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/special_tokens_map.json
05/05/2022 21:08:10 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/tokenizer_config.json
05/05/2022 21:08:10 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_raw_128
05/05/2022 21:08:11 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 21:08:11 - INFO - __main__ -     Num examples = 3453
05/05/2022 21:08:11 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 102.31it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:04, 101.86it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 101.80it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 101.48it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 101.00it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 100.10it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 99.64it/s] Evaluating:  20%|██        | 87/432 [00:00<00:03, 99.60it/s]Evaluating:  22%|██▏       | 97/432 [00:00<00:03, 99.47it/s]Evaluating:  25%|██▍       | 107/432 [00:01<00:03, 99.13it/s]Evaluating:  27%|██▋       | 117/432 [00:01<00:03, 98.53it/s]Evaluating:  29%|██▉       | 127/432 [00:01<00:03, 97.61it/s]Evaluating:  32%|███▏      | 137/432 [00:01<00:03, 96.20it/s]Evaluating:  34%|███▍      | 147/432 [00:01<00:02, 95.20it/s]Evaluating:  36%|███▋      | 157/432 [00:01<00:02, 94.23it/s]Evaluating:  39%|███▊      | 167/432 [00:01<00:02, 92.99it/s]Evaluating:  41%|████      | 177/432 [00:01<00:02, 92.25it/s]Evaluating:  43%|████▎     | 187/432 [00:01<00:02, 90.89it/s]Evaluating:  46%|████▌     | 197/432 [00:02<00:02, 90.11it/s]Evaluating:  48%|████▊     | 207/432 [00:02<00:02, 89.28it/s]Evaluating:  50%|█████     | 216/432 [00:02<00:02, 88.07it/s]Evaluating:  52%|█████▏    | 225/432 [00:02<00:02, 87.08it/s]Evaluating:  54%|█████▍    | 234/432 [00:02<00:02, 86.22it/s]Evaluating:  56%|█████▋    | 243/432 [00:02<00:02, 85.45it/s]Evaluating:  58%|█████▊    | 252/432 [00:02<00:02, 84.89it/s]Evaluating:  60%|██████    | 261/432 [00:02<00:02, 84.64it/s]Evaluating:  62%|██████▎   | 270/432 [00:02<00:01, 84.19it/s]Evaluating:  65%|██████▍   | 279/432 [00:03<00:01, 83.75it/s]Evaluating:  67%|██████▋   | 288/432 [00:03<00:01, 83.33it/s]Evaluating:  69%|██████▉   | 297/432 [00:03<00:01, 82.88it/s]Evaluating:  71%|███████   | 306/432 [00:03<00:01, 82.19it/s]Evaluating:  73%|███████▎  | 315/432 [00:03<00:01, 81.51it/s]Evaluating:  75%|███████▌  | 324/432 [00:03<00:01, 81.08it/s]Evaluating:  77%|███████▋  | 333/432 [00:03<00:01, 80.48it/s]Evaluating:  79%|███████▉  | 342/432 [00:03<00:01, 80.02it/s]Evaluating:  81%|████████▏ | 351/432 [00:03<00:01, 79.57it/s]Evaluating:  83%|████████▎ | 359/432 [00:04<00:00, 79.28it/s]Evaluating:  85%|████████▍ | 367/432 [00:04<00:00, 78.75it/s]Evaluating:  87%|████████▋ | 375/432 [00:04<00:00, 78.58it/s]Evaluating:  89%|████████▊ | 383/432 [00:04<00:00, 77.99it/s]Evaluating:  91%|█████████ | 391/432 [00:04<00:00, 77.45it/s]Evaluating:  92%|█████████▏| 399/432 [00:04<00:00, 76.84it/s]Evaluating:  94%|█████████▍| 407/432 [00:04<00:00, 76.42it/s]Evaluating:  96%|█████████▌| 415/432 [00:04<00:00, 75.72it/s]Evaluating:  98%|█████████▊| 423/432 [00:04<00:00, 75.38it/s]Evaluating: 100%|█████████▉| 431/432 [00:04<00:00, 74.91it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 86.69it/s]
05/05/2022 21:08:16 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 21:08:16 - INFO - __main__ -     f1 = 0.8969398522687302
05/05/2022 21:08:16 - INFO - __main__ -     loss = 0.14584889084816227
05/05/2022 21:08:16 - INFO - __main__ -     precision = 0.8903631284916201
05/05/2022 21:08:16 - INFO - __main__ -     recall = 0.9036144578313253
05/05/2022 21:08:16 - INFO - Distillation -   Epoch 8 finished
05/05/2022 21:08:16 - INFO - Distillation -   Epoch 9
05/05/2022 21:08:16 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 21:08:19 - INFO - Distillation -   Global step: 3528, epoch step:16
05/05/2022 21:08:23 - INFO - Distillation -   Global step: 3549, epoch step:37
05/05/2022 21:08:27 - INFO - Distillation -   Global step: 3570, epoch step:58
05/05/2022 21:08:31 - INFO - Distillation -   Global step: 3591, epoch step:79
05/05/2022 21:08:35 - INFO - Distillation -   Global step: 3612, epoch step:100
05/05/2022 21:08:38 - INFO - Distillation -   Global step: 3633, epoch step:121
05/05/2022 21:08:42 - INFO - Distillation -   Global step: 3654, epoch step:142
05/05/2022 21:08:46 - INFO - Distillation -   Global step: 3675, epoch step:163
05/05/2022 21:08:50 - INFO - Distillation -   Global step: 3696, epoch step:184
05/05/2022 21:08:54 - INFO - Distillation -   Global step: 3717, epoch step:205
05/05/2022 21:08:57 - INFO - Distillation -   Global step: 3738, epoch step:226
05/05/2022 21:09:01 - INFO - Distillation -   Global step: 3759, epoch step:247
05/05/2022 21:09:05 - INFO - Distillation -   Global step: 3780, epoch step:268
05/05/2022 21:09:09 - INFO - Distillation -   Global step: 3801, epoch step:289
05/05/2022 21:09:13 - INFO - Distillation -   Global step: 3822, epoch step:310
05/05/2022 21:09:16 - INFO - Distillation -   Global step: 3843, epoch step:331
05/05/2022 21:09:20 - INFO - Distillation -   Global step: 3864, epoch step:352
05/05/2022 21:09:24 - INFO - Distillation -   Global step: 3885, epoch step:373
05/05/2022 21:09:28 - INFO - Distillation -   Global step: 3906, epoch step:394
05/05/2022 21:09:32 - INFO - Distillation -   Global step: 3927, epoch step:415
05/05/2022 21:09:35 - INFO - Distillation -   Global step: 3948, epoch step:436
05/05/2022 21:09:36 - INFO - Distillation -   Saving at global step 3951, epoch step 439 epoch 9
05/05/2022 21:09:36 - INFO - Distillation -   Running callback function...
05/05/2022 21:09:36 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' is a path or url to a directory containing tokenizer files.
05/05/2022 21:09:36 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/vocab.txt
05/05/2022 21:09:36 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/added_tokens.json
05/05/2022 21:09:36 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/special_tokens_map.json
05/05/2022 21:09:36 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/tokenizer_config.json
05/05/2022 21:09:36 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_raw_128
05/05/2022 21:09:37 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 21:09:37 - INFO - __main__ -     Num examples = 3453
05/05/2022 21:09:37 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 103.44it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:03, 102.98it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 102.59it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 102.02it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 101.19it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 100.51it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 100.20it/s]Evaluating:  20%|██        | 88/432 [00:00<00:03, 99.68it/s] Evaluating:  23%|██▎       | 98/432 [00:00<00:03, 99.07it/s]Evaluating:  25%|██▌       | 108/432 [00:01<00:03, 98.36it/s]Evaluating:  27%|██▋       | 118/432 [00:01<00:03, 97.52it/s]Evaluating:  30%|██▉       | 128/432 [00:01<00:03, 96.99it/s]Evaluating:  32%|███▏      | 138/432 [00:01<00:03, 96.36it/s]Evaluating:  34%|███▍      | 148/432 [00:01<00:02, 94.94it/s]Evaluating:  37%|███▋      | 158/432 [00:01<00:02, 93.65it/s]Evaluating:  39%|███▉      | 168/432 [00:01<00:02, 92.58it/s]Evaluating:  41%|████      | 178/432 [00:01<00:02, 91.77it/s]Evaluating:  44%|████▎     | 188/432 [00:01<00:02, 91.09it/s]Evaluating:  46%|████▌     | 198/432 [00:02<00:02, 90.22it/s]Evaluating:  48%|████▊     | 208/432 [00:02<00:02, 89.55it/s]Evaluating:  50%|█████     | 217/432 [00:02<00:02, 88.75it/s]Evaluating:  52%|█████▏    | 226/432 [00:02<00:02, 87.81it/s]Evaluating:  54%|█████▍    | 235/432 [00:02<00:02, 87.00it/s]Evaluating:  56%|█████▋    | 244/432 [00:02<00:02, 85.75it/s]Evaluating:  59%|█████▊    | 253/432 [00:02<00:02, 84.66it/s]Evaluating:  61%|██████    | 262/432 [00:02<00:02, 84.09it/s]Evaluating:  63%|██████▎   | 271/432 [00:02<00:01, 83.54it/s]Evaluating:  65%|██████▍   | 280/432 [00:03<00:01, 83.22it/s]Evaluating:  67%|██████▋   | 289/432 [00:03<00:01, 82.79it/s]Evaluating:  69%|██████▉   | 298/432 [00:03<00:01, 82.36it/s]Evaluating:  71%|███████   | 307/432 [00:03<00:01, 81.85it/s]Evaluating:  73%|███████▎  | 316/432 [00:03<00:01, 81.19it/s]Evaluating:  75%|███████▌  | 325/432 [00:03<00:01, 80.62it/s]Evaluating:  77%|███████▋  | 334/432 [00:03<00:01, 80.05it/s]Evaluating:  79%|███████▉  | 343/432 [00:03<00:01, 79.88it/s]Evaluating:  81%|████████▏ | 351/432 [00:03<00:01, 79.53it/s]Evaluating:  83%|████████▎ | 359/432 [00:04<00:00, 79.24it/s]Evaluating:  85%|████████▍ | 367/432 [00:04<00:00, 79.10it/s]Evaluating:  87%|████████▋ | 375/432 [00:04<00:00, 78.73it/s]Evaluating:  89%|████████▊ | 383/432 [00:04<00:00, 78.32it/s]Evaluating:  91%|█████████ | 391/432 [00:04<00:00, 77.91it/s]Evaluating:  92%|█████████▏| 399/432 [00:04<00:00, 77.36it/s]Evaluating:  94%|█████████▍| 407/432 [00:04<00:00, 76.83it/s]Evaluating:  96%|█████████▌| 415/432 [00:04<00:00, 76.06it/s]Evaluating:  98%|█████████▊| 423/432 [00:04<00:00, 75.62it/s]Evaluating: 100%|█████████▉| 431/432 [00:04<00:00, 74.88it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 86.73it/s]
05/05/2022 21:09:42 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 21:09:42 - INFO - __main__ -     f1 = 0.8981888517671883
05/05/2022 21:09:42 - INFO - __main__ -     loss = 0.14906670671628794
05/05/2022 21:09:42 - INFO - __main__ -     precision = 0.8914485165794066
05/05/2022 21:09:42 - INFO - __main__ -     recall = 0.9050318922749823
05/05/2022 21:09:42 - INFO - Distillation -   Epoch 9 finished
05/05/2022 21:09:42 - INFO - Distillation -   Epoch 10
05/05/2022 21:09:42 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 21:09:46 - INFO - Distillation -   Global step: 3969, epoch step:18
05/05/2022 21:09:49 - INFO - Distillation -   Global step: 3990, epoch step:39
05/05/2022 21:09:53 - INFO - Distillation -   Global step: 4011, epoch step:60
05/05/2022 21:09:57 - INFO - Distillation -   Global step: 4032, epoch step:81
05/05/2022 21:10:01 - INFO - Distillation -   Global step: 4053, epoch step:102
05/05/2022 21:10:05 - INFO - Distillation -   Global step: 4074, epoch step:123
05/05/2022 21:10:08 - INFO - Distillation -   Global step: 4095, epoch step:144
05/05/2022 21:10:12 - INFO - Distillation -   Global step: 4116, epoch step:165
05/05/2022 21:10:16 - INFO - Distillation -   Global step: 4137, epoch step:186
05/05/2022 21:10:20 - INFO - Distillation -   Global step: 4158, epoch step:207
05/05/2022 21:10:24 - INFO - Distillation -   Global step: 4179, epoch step:228
05/05/2022 21:10:28 - INFO - Distillation -   Global step: 4200, epoch step:249
05/05/2022 21:10:31 - INFO - Distillation -   Global step: 4221, epoch step:270
05/05/2022 21:10:35 - INFO - Distillation -   Global step: 4242, epoch step:291
05/05/2022 21:10:39 - INFO - Distillation -   Global step: 4263, epoch step:312
05/05/2022 21:10:43 - INFO - Distillation -   Global step: 4284, epoch step:333
05/05/2022 21:10:47 - INFO - Distillation -   Global step: 4305, epoch step:354
05/05/2022 21:10:50 - INFO - Distillation -   Global step: 4326, epoch step:375
05/05/2022 21:10:54 - INFO - Distillation -   Global step: 4347, epoch step:396
05/05/2022 21:10:58 - INFO - Distillation -   Global step: 4368, epoch step:417
05/05/2022 21:11:02 - INFO - Distillation -   Global step: 4389, epoch step:438
05/05/2022 21:11:02 - INFO - Distillation -   Saving at global step 4390, epoch step 439 epoch 10
05/05/2022 21:11:02 - INFO - Distillation -   Running callback function...
05/05/2022 21:11:02 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' is a path or url to a directory containing tokenizer files.
05/05/2022 21:11:02 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/vocab.txt
05/05/2022 21:11:02 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/added_tokens.json
05/05/2022 21:11:02 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/special_tokens_map.json
05/05/2022 21:11:02 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/tokenizer_config.json
05/05/2022 21:11:02 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_raw_128
05/05/2022 21:11:03 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 21:11:03 - INFO - __main__ -     Num examples = 3453
05/05/2022 21:11:03 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 103.80it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:03, 103.15it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 102.62it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 102.09it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 101.22it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 100.37it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 99.92it/s] Evaluating:  20%|██        | 87/432 [00:00<00:03, 99.45it/s]Evaluating:  22%|██▏       | 97/432 [00:00<00:03, 98.78it/s]Evaluating:  25%|██▍       | 107/432 [00:01<00:03, 98.43it/s]Evaluating:  27%|██▋       | 117/432 [00:01<00:03, 97.94it/s]Evaluating:  29%|██▉       | 127/432 [00:01<00:03, 97.02it/s]Evaluating:  32%|███▏      | 137/432 [00:01<00:03, 96.17it/s]Evaluating:  34%|███▍      | 147/432 [00:01<00:02, 95.18it/s]Evaluating:  36%|███▋      | 157/432 [00:01<00:02, 94.17it/s]Evaluating:  39%|███▊      | 167/432 [00:01<00:02, 93.41it/s]Evaluating:  41%|████      | 177/432 [00:01<00:02, 92.65it/s]Evaluating:  43%|████▎     | 187/432 [00:01<00:02, 91.83it/s]Evaluating:  46%|████▌     | 197/432 [00:02<00:02, 90.84it/s]Evaluating:  48%|████▊     | 207/432 [00:02<00:02, 89.95it/s]Evaluating:  50%|█████     | 217/432 [00:02<00:02, 88.68it/s]Evaluating:  52%|█████▏    | 226/432 [00:02<00:02, 87.69it/s]Evaluating:  54%|█████▍    | 235/432 [00:02<00:02, 86.84it/s]Evaluating:  56%|█████▋    | 244/432 [00:02<00:02, 86.10it/s]Evaluating:  59%|█████▊    | 253/432 [00:02<00:02, 85.34it/s]Evaluating:  61%|██████    | 262/432 [00:02<00:02, 84.71it/s]Evaluating:  63%|██████▎   | 271/432 [00:02<00:01, 84.05it/s]Evaluating:  65%|██████▍   | 280/432 [00:03<00:01, 83.45it/s]Evaluating:  67%|██████▋   | 289/432 [00:03<00:01, 82.96it/s]Evaluating:  69%|██████▉   | 298/432 [00:03<00:01, 82.27it/s]Evaluating:  71%|███████   | 307/432 [00:03<00:01, 81.64it/s]Evaluating:  73%|███████▎  | 316/432 [00:03<00:01, 81.41it/s]Evaluating:  75%|███████▌  | 325/432 [00:03<00:01, 81.11it/s]Evaluating:  77%|███████▋  | 334/432 [00:03<00:01, 80.79it/s]Evaluating:  79%|███████▉  | 343/432 [00:03<00:01, 80.13it/s]Evaluating:  81%|████████▏ | 352/432 [00:03<00:01, 79.76it/s]Evaluating:  83%|████████▎ | 360/432 [00:04<00:00, 79.23it/s]Evaluating:  85%|████████▌ | 368/432 [00:04<00:00, 78.57it/s]Evaluating:  87%|████████▋ | 376/432 [00:04<00:00, 78.29it/s]Evaluating:  89%|████████▉ | 384/432 [00:04<00:00, 77.62it/s]Evaluating:  91%|█████████ | 392/432 [00:04<00:00, 77.21it/s]Evaluating:  93%|█████████▎| 400/432 [00:04<00:00, 76.82it/s]Evaluating:  94%|█████████▍| 408/432 [00:04<00:00, 76.63it/s]Evaluating:  96%|█████████▋| 416/432 [00:04<00:00, 75.90it/s]Evaluating:  98%|█████████▊| 424/432 [00:04<00:00, 75.30it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 75.72it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 86.82it/s]
05/05/2022 21:11:08 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 21:11:08 - INFO - __main__ -     f1 = 0.8962181178540017
05/05/2022 21:11:08 - INFO - __main__ -     loss = 0.1436702562034666
05/05/2022 21:11:08 - INFO - __main__ -     precision = 0.8898009081383165
05/05/2022 21:11:08 - INFO - __main__ -     recall = 0.9027285613040397
05/05/2022 21:11:08 - INFO - Distillation -   Epoch 10 finished
05/05/2022 21:11:08 - INFO - Distillation -   Epoch 11
05/05/2022 21:11:08 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 21:11:12 - INFO - Distillation -   Global step: 4410, epoch step:20
05/05/2022 21:11:16 - INFO - Distillation -   Global step: 4431, epoch step:41
05/05/2022 21:11:20 - INFO - Distillation -   Global step: 4452, epoch step:62
05/05/2022 21:11:23 - INFO - Distillation -   Global step: 4473, epoch step:83
05/05/2022 21:11:27 - INFO - Distillation -   Global step: 4494, epoch step:104
05/05/2022 21:11:31 - INFO - Distillation -   Global step: 4515, epoch step:125
05/05/2022 21:11:35 - INFO - Distillation -   Global step: 4536, epoch step:146
05/05/2022 21:11:39 - INFO - Distillation -   Global step: 4557, epoch step:167
05/05/2022 21:11:42 - INFO - Distillation -   Global step: 4578, epoch step:188
05/05/2022 21:11:46 - INFO - Distillation -   Global step: 4599, epoch step:209
05/05/2022 21:11:50 - INFO - Distillation -   Global step: 4620, epoch step:230
05/05/2022 21:11:54 - INFO - Distillation -   Global step: 4641, epoch step:251
05/05/2022 21:11:58 - INFO - Distillation -   Global step: 4662, epoch step:272
05/05/2022 21:12:01 - INFO - Distillation -   Global step: 4683, epoch step:293
05/05/2022 21:12:05 - INFO - Distillation -   Global step: 4704, epoch step:314
05/05/2022 21:12:09 - INFO - Distillation -   Global step: 4725, epoch step:335
05/05/2022 21:12:13 - INFO - Distillation -   Global step: 4746, epoch step:356
05/05/2022 21:12:17 - INFO - Distillation -   Global step: 4767, epoch step:377
05/05/2022 21:12:20 - INFO - Distillation -   Global step: 4788, epoch step:398
05/05/2022 21:12:24 - INFO - Distillation -   Global step: 4809, epoch step:419
05/05/2022 21:12:28 - INFO - Distillation -   Saving at global step 4829, epoch step 439 epoch 11
05/05/2022 21:12:28 - INFO - Distillation -   Running callback function...
05/05/2022 21:12:28 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' is a path or url to a directory containing tokenizer files.
05/05/2022 21:12:28 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/vocab.txt
05/05/2022 21:12:28 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/added_tokens.json
05/05/2022 21:12:28 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/special_tokens_map.json
05/05/2022 21:12:28 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/tokenizer_config.json
05/05/2022 21:12:28 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_raw_128
05/05/2022 21:12:29 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 21:12:29 - INFO - __main__ -     Num examples = 3453
05/05/2022 21:12:29 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 102.79it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:03, 102.87it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 102.39it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 101.77it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 101.03it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 100.60it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 100.33it/s]Evaluating:  20%|██        | 88/432 [00:00<00:03, 99.91it/s] Evaluating:  23%|██▎       | 98/432 [00:00<00:03, 98.91it/s]Evaluating:  25%|██▌       | 108/432 [00:01<00:03, 97.92it/s]Evaluating:  27%|██▋       | 118/432 [00:01<00:03, 97.82it/s]Evaluating:  30%|██▉       | 128/432 [00:01<00:03, 97.55it/s]Evaluating:  32%|███▏      | 138/432 [00:01<00:03, 96.43it/s]Evaluating:  34%|███▍      | 148/432 [00:01<00:02, 95.60it/s]Evaluating:  37%|███▋      | 158/432 [00:01<00:02, 94.56it/s]Evaluating:  39%|███▉      | 168/432 [00:01<00:02, 93.55it/s]Evaluating:  41%|████      | 178/432 [00:01<00:02, 92.61it/s]Evaluating:  44%|████▎     | 188/432 [00:01<00:02, 91.46it/s]Evaluating:  46%|████▌     | 198/432 [00:02<00:02, 90.21it/s]Evaluating:  48%|████▊     | 208/432 [00:02<00:02, 89.16it/s]Evaluating:  50%|█████     | 217/432 [00:02<00:02, 88.05it/s]Evaluating:  52%|█████▏    | 226/432 [00:02<00:02, 87.02it/s]Evaluating:  54%|█████▍    | 235/432 [00:02<00:02, 86.22it/s]Evaluating:  56%|█████▋    | 244/432 [00:02<00:02, 85.59it/s]Evaluating:  59%|█████▊    | 253/432 [00:02<00:02, 84.78it/s]Evaluating:  61%|██████    | 262/432 [00:02<00:02, 84.32it/s]Evaluating:  63%|██████▎   | 271/432 [00:02<00:01, 84.17it/s]Evaluating:  65%|██████▍   | 280/432 [00:03<00:01, 83.53it/s]Evaluating:  67%|██████▋   | 289/432 [00:03<00:01, 83.03it/s]Evaluating:  69%|██████▉   | 298/432 [00:03<00:01, 82.50it/s]Evaluating:  71%|███████   | 307/432 [00:03<00:01, 81.69it/s]Evaluating:  73%|███████▎  | 316/432 [00:03<00:01, 81.10it/s]Evaluating:  75%|███████▌  | 325/432 [00:03<00:01, 80.70it/s]Evaluating:  77%|███████▋  | 334/432 [00:03<00:01, 80.41it/s]Evaluating:  79%|███████▉  | 343/432 [00:03<00:01, 79.81it/s]Evaluating:  81%|████████▏ | 351/432 [00:03<00:01, 79.33it/s]Evaluating:  83%|████████▎ | 359/432 [00:04<00:00, 78.83it/s]Evaluating:  85%|████████▍ | 367/432 [00:04<00:00, 78.29it/s]Evaluating:  87%|████████▋ | 375/432 [00:04<00:00, 77.99it/s]Evaluating:  89%|████████▊ | 383/432 [00:04<00:00, 77.51it/s]Evaluating:  91%|█████████ | 391/432 [00:04<00:00, 77.17it/s]Evaluating:  92%|█████████▏| 399/432 [00:04<00:00, 76.88it/s]Evaluating:  94%|█████████▍| 407/432 [00:04<00:00, 76.66it/s]Evaluating:  96%|█████████▌| 415/432 [00:04<00:00, 76.28it/s]Evaluating:  98%|█████████▊| 423/432 [00:04<00:00, 75.82it/s]Evaluating: 100%|█████████▉| 431/432 [00:04<00:00, 75.25it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 86.73it/s]
05/05/2022 21:12:34 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 21:12:34 - INFO - __main__ -     f1 = 0.9018156178388859
05/05/2022 21:12:34 - INFO - __main__ -     loss = 0.14375863103855072
05/05/2022 21:12:34 - INFO - __main__ -     precision = 0.8972290424412487
05/05/2022 21:12:34 - INFO - __main__ -     recall = 0.9064493267186393
05/05/2022 21:12:34 - INFO - Distillation -   Epoch 11 finished
05/05/2022 21:12:34 - INFO - Distillation -   Epoch 12
05/05/2022 21:12:34 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 21:12:34 - INFO - Distillation -   Global step: 4830, epoch step:1
05/05/2022 21:12:38 - INFO - Distillation -   Global step: 4851, epoch step:22
05/05/2022 21:12:42 - INFO - Distillation -   Global step: 4872, epoch step:43
05/05/2022 21:12:46 - INFO - Distillation -   Global step: 4893, epoch step:64
05/05/2022 21:12:50 - INFO - Distillation -   Global step: 4914, epoch step:85
05/05/2022 21:12:53 - INFO - Distillation -   Global step: 4935, epoch step:106
05/05/2022 21:12:57 - INFO - Distillation -   Global step: 4956, epoch step:127
05/05/2022 21:13:01 - INFO - Distillation -   Global step: 4977, epoch step:148
05/05/2022 21:13:05 - INFO - Distillation -   Global step: 4998, epoch step:169
05/05/2022 21:13:09 - INFO - Distillation -   Global step: 5019, epoch step:190
05/05/2022 21:13:13 - INFO - Distillation -   Global step: 5040, epoch step:211
05/05/2022 21:13:16 - INFO - Distillation -   Global step: 5061, epoch step:232
05/05/2022 21:13:20 - INFO - Distillation -   Global step: 5082, epoch step:253
05/05/2022 21:13:24 - INFO - Distillation -   Global step: 5103, epoch step:274
05/05/2022 21:13:28 - INFO - Distillation -   Global step: 5124, epoch step:295
05/05/2022 21:13:32 - INFO - Distillation -   Global step: 5145, epoch step:316
05/05/2022 21:13:35 - INFO - Distillation -   Global step: 5166, epoch step:337
05/05/2022 21:13:39 - INFO - Distillation -   Global step: 5187, epoch step:358
05/05/2022 21:13:43 - INFO - Distillation -   Global step: 5208, epoch step:379
05/05/2022 21:13:47 - INFO - Distillation -   Global step: 5229, epoch step:400
05/05/2022 21:13:51 - INFO - Distillation -   Global step: 5250, epoch step:421
05/05/2022 21:13:54 - INFO - Distillation -   Saving at global step 5268, epoch step 439 epoch 12
05/05/2022 21:13:54 - INFO - Distillation -   Running callback function...
05/05/2022 21:13:54 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' is a path or url to a directory containing tokenizer files.
05/05/2022 21:13:54 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/vocab.txt
05/05/2022 21:13:54 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/added_tokens.json
05/05/2022 21:13:54 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/special_tokens_map.json
05/05/2022 21:13:54 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/tokenizer_config.json
05/05/2022 21:13:54 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_raw_128
05/05/2022 21:13:55 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 21:13:55 - INFO - __main__ -     Num examples = 3453
05/05/2022 21:13:55 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 102.36it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:04, 102.40it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 102.52it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 102.20it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 101.25it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 100.50it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 99.79it/s] Evaluating:  20%|██        | 87/432 [00:00<00:03, 99.60it/s]Evaluating:  22%|██▏       | 97/432 [00:00<00:03, 99.21it/s]Evaluating:  25%|██▍       | 107/432 [00:01<00:03, 98.55it/s]Evaluating:  27%|██▋       | 117/432 [00:01<00:03, 98.18it/s]Evaluating:  29%|██▉       | 127/432 [00:01<00:03, 97.70it/s]Evaluating:  32%|███▏      | 137/432 [00:01<00:03, 97.05it/s]Evaluating:  34%|███▍      | 147/432 [00:01<00:02, 96.27it/s]Evaluating:  36%|███▋      | 157/432 [00:01<00:02, 95.13it/s]Evaluating:  39%|███▊      | 167/432 [00:01<00:02, 93.98it/s]Evaluating:  41%|████      | 177/432 [00:01<00:02, 92.94it/s]Evaluating:  43%|████▎     | 187/432 [00:01<00:02, 91.70it/s]Evaluating:  46%|████▌     | 197/432 [00:02<00:02, 90.53it/s]Evaluating:  48%|████▊     | 207/432 [00:02<00:02, 89.30it/s]Evaluating:  50%|█████     | 216/432 [00:02<00:02, 88.32it/s]Evaluating:  52%|█████▏    | 225/432 [00:02<00:02, 87.61it/s]Evaluating:  54%|█████▍    | 234/432 [00:02<00:02, 86.74it/s]Evaluating:  56%|█████▋    | 243/432 [00:02<00:02, 86.06it/s]Evaluating:  58%|█████▊    | 252/432 [00:02<00:02, 85.49it/s]Evaluating:  60%|██████    | 261/432 [00:02<00:02, 84.73it/s]Evaluating:  62%|██████▎   | 270/432 [00:02<00:01, 84.35it/s]Evaluating:  65%|██████▍   | 279/432 [00:03<00:01, 83.80it/s]Evaluating:  67%|██████▋   | 288/432 [00:03<00:01, 83.08it/s]Evaluating:  69%|██████▉   | 297/432 [00:03<00:01, 82.56it/s]Evaluating:  71%|███████   | 306/432 [00:03<00:01, 82.03it/s]Evaluating:  73%|███████▎  | 315/432 [00:03<00:01, 81.54it/s]Evaluating:  75%|███████▌  | 324/432 [00:03<00:01, 81.02it/s]Evaluating:  77%|███████▋  | 333/432 [00:03<00:01, 80.49it/s]Evaluating:  79%|███████▉  | 342/432 [00:03<00:01, 79.87it/s]Evaluating:  81%|████████  | 350/432 [00:03<00:01, 79.45it/s]Evaluating:  83%|████████▎ | 358/432 [00:03<00:00, 79.10it/s]Evaluating:  85%|████████▍ | 366/432 [00:04<00:00, 78.31it/s]Evaluating:  87%|████████▋ | 374/432 [00:04<00:00, 77.93it/s]Evaluating:  88%|████████▊ | 382/432 [00:04<00:00, 77.78it/s]Evaluating:  90%|█████████ | 390/432 [00:04<00:00, 77.65it/s]Evaluating:  92%|█████████▏| 398/432 [00:04<00:00, 77.23it/s]Evaluating:  94%|█████████▍| 406/432 [00:04<00:00, 76.75it/s]Evaluating:  96%|█████████▌| 414/432 [00:04<00:00, 76.16it/s]Evaluating:  98%|█████████▊| 422/432 [00:04<00:00, 75.49it/s]Evaluating: 100%|█████████▉| 430/432 [00:04<00:00, 75.16it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 86.87it/s]
05/05/2022 21:14:00 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 21:14:00 - INFO - __main__ -     f1 = 0.8995248988210451
05/05/2022 21:14:00 - INFO - __main__ -     loss = 0.1430877897796984
05/05/2022 21:14:00 - INFO - __main__ -     precision = 0.8933939182104159
05/05/2022 21:14:00 - INFO - __main__ -     recall = 0.9057406094968108
05/05/2022 21:14:00 - INFO - Distillation -   Epoch 12 finished
05/05/2022 21:14:00 - INFO - Distillation -   Epoch 13
05/05/2022 21:14:00 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 21:14:01 - INFO - Distillation -   Global step: 5271, epoch step:3
05/05/2022 21:14:05 - INFO - Distillation -   Global step: 5292, epoch step:24
05/05/2022 21:14:08 - INFO - Distillation -   Global step: 5313, epoch step:45
05/05/2022 21:14:12 - INFO - Distillation -   Global step: 5334, epoch step:66
05/05/2022 21:14:16 - INFO - Distillation -   Global step: 5355, epoch step:87
05/05/2022 21:14:20 - INFO - Distillation -   Global step: 5376, epoch step:108
05/05/2022 21:14:24 - INFO - Distillation -   Global step: 5397, epoch step:129
05/05/2022 21:14:28 - INFO - Distillation -   Global step: 5418, epoch step:150
05/05/2022 21:14:31 - INFO - Distillation -   Global step: 5439, epoch step:171
05/05/2022 21:14:35 - INFO - Distillation -   Global step: 5460, epoch step:192
05/05/2022 21:14:39 - INFO - Distillation -   Global step: 5481, epoch step:213
05/05/2022 21:14:43 - INFO - Distillation -   Global step: 5502, epoch step:234
05/05/2022 21:14:47 - INFO - Distillation -   Global step: 5523, epoch step:255
05/05/2022 21:14:50 - INFO - Distillation -   Global step: 5544, epoch step:276
05/05/2022 21:14:54 - INFO - Distillation -   Global step: 5565, epoch step:297
05/05/2022 21:14:58 - INFO - Distillation -   Global step: 5586, epoch step:318
05/05/2022 21:15:02 - INFO - Distillation -   Global step: 5607, epoch step:339
05/05/2022 21:15:06 - INFO - Distillation -   Global step: 5628, epoch step:360
05/05/2022 21:15:09 - INFO - Distillation -   Global step: 5649, epoch step:381
05/05/2022 21:15:13 - INFO - Distillation -   Global step: 5670, epoch step:402
05/05/2022 21:15:17 - INFO - Distillation -   Global step: 5691, epoch step:423
05/05/2022 21:15:20 - INFO - Distillation -   Saving at global step 5707, epoch step 439 epoch 13
05/05/2022 21:15:20 - INFO - Distillation -   Running callback function...
05/05/2022 21:15:20 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' is a path or url to a directory containing tokenizer files.
05/05/2022 21:15:20 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/vocab.txt
05/05/2022 21:15:20 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/added_tokens.json
05/05/2022 21:15:20 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/special_tokens_map.json
05/05/2022 21:15:20 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/tokenizer_config.json
05/05/2022 21:15:20 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_raw_128
05/05/2022 21:15:21 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 21:15:21 - INFO - __main__ -     Num examples = 3453
05/05/2022 21:15:21 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 101.82it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:04, 102.23it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 101.44it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 100.58it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 99.77it/s] Evaluating:  15%|█▌        | 65/432 [00:00<00:03, 99.20it/s]Evaluating:  17%|█▋        | 75/432 [00:00<00:03, 99.06it/s]Evaluating:  20%|█▉        | 85/432 [00:00<00:03, 98.94it/s]Evaluating:  22%|██▏       | 95/432 [00:00<00:03, 98.77it/s]Evaluating:  24%|██▍       | 105/432 [00:01<00:03, 98.53it/s]Evaluating:  27%|██▋       | 115/432 [00:01<00:03, 98.00it/s]Evaluating:  29%|██▉       | 125/432 [00:01<00:03, 97.18it/s]Evaluating:  31%|███▏      | 135/432 [00:01<00:03, 96.39it/s]Evaluating:  34%|███▎      | 145/432 [00:01<00:03, 95.23it/s]Evaluating:  36%|███▌      | 155/432 [00:01<00:02, 94.02it/s]Evaluating:  38%|███▊      | 165/432 [00:01<00:02, 93.11it/s]Evaluating:  41%|████      | 175/432 [00:01<00:02, 92.28it/s]Evaluating:  43%|████▎     | 185/432 [00:01<00:02, 91.07it/s]Evaluating:  45%|████▌     | 195/432 [00:02<00:02, 90.29it/s]Evaluating:  47%|████▋     | 205/432 [00:02<00:02, 89.52it/s]Evaluating:  50%|████▉     | 214/432 [00:02<00:02, 88.41it/s]Evaluating:  52%|█████▏    | 223/432 [00:02<00:02, 87.60it/s]Evaluating:  54%|█████▎    | 232/432 [00:02<00:02, 86.92it/s]Evaluating:  56%|█████▌    | 241/432 [00:02<00:02, 86.12it/s]Evaluating:  58%|█████▊    | 250/432 [00:02<00:02, 85.61it/s]Evaluating:  60%|█████▉    | 259/432 [00:02<00:02, 84.97it/s]Evaluating:  62%|██████▏   | 268/432 [00:02<00:01, 84.34it/s]Evaluating:  64%|██████▍   | 277/432 [00:03<00:01, 83.97it/s]Evaluating:  66%|██████▌   | 286/432 [00:03<00:01, 83.41it/s]Evaluating:  68%|██████▊   | 295/432 [00:03<00:01, 82.77it/s]Evaluating:  70%|███████   | 304/432 [00:03<00:01, 82.06it/s]Evaluating:  72%|███████▏  | 313/432 [00:03<00:01, 81.54it/s]Evaluating:  75%|███████▍  | 322/432 [00:03<00:01, 80.74it/s]Evaluating:  77%|███████▋  | 331/432 [00:03<00:01, 80.28it/s]Evaluating:  79%|███████▊  | 340/432 [00:03<00:01, 79.81it/s]Evaluating:  81%|████████  | 348/432 [00:03<00:01, 79.49it/s]Evaluating:  82%|████████▏ | 356/432 [00:03<00:00, 79.06it/s]Evaluating:  84%|████████▍ | 364/432 [00:04<00:00, 79.02it/s]Evaluating:  86%|████████▌ | 372/432 [00:04<00:00, 78.79it/s]Evaluating:  88%|████████▊ | 380/432 [00:04<00:00, 78.31it/s]Evaluating:  90%|████████▉ | 388/432 [00:04<00:00, 77.81it/s]Evaluating:  92%|█████████▏| 396/432 [00:04<00:00, 77.48it/s]Evaluating:  94%|█████████▎| 404/432 [00:04<00:00, 76.68it/s]Evaluating:  95%|█████████▌| 412/432 [00:04<00:00, 76.31it/s]Evaluating:  97%|█████████▋| 420/432 [00:04<00:00, 75.75it/s]Evaluating:  99%|█████████▉| 428/432 [00:04<00:00, 75.28it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 86.63it/s]
05/05/2022 21:15:26 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 21:15:26 - INFO - __main__ -     f1 = 0.8997882851093861
05/05/2022 21:15:26 - INFO - __main__ -     loss = 0.14813124869227035
05/05/2022 21:15:26 - INFO - __main__ -     precision = 0.8959943780744906
05/05/2022 21:15:26 - INFO - __main__ -     recall = 0.9036144578313253
05/05/2022 21:15:26 - INFO - Distillation -   Epoch 13 finished
05/05/2022 21:15:26 - INFO - Distillation -   Epoch 14
05/05/2022 21:15:26 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 21:15:27 - INFO - Distillation -   Global step: 5712, epoch step:5
05/05/2022 21:15:31 - INFO - Distillation -   Global step: 5733, epoch step:26
05/05/2022 21:15:35 - INFO - Distillation -   Global step: 5754, epoch step:47
05/05/2022 21:15:39 - INFO - Distillation -   Global step: 5775, epoch step:68
05/05/2022 21:15:42 - INFO - Distillation -   Global step: 5796, epoch step:89
05/05/2022 21:15:46 - INFO - Distillation -   Global step: 5817, epoch step:110
05/05/2022 21:15:50 - INFO - Distillation -   Global step: 5838, epoch step:131
05/05/2022 21:15:54 - INFO - Distillation -   Global step: 5859, epoch step:152
05/05/2022 21:15:58 - INFO - Distillation -   Global step: 5880, epoch step:173
05/05/2022 21:16:01 - INFO - Distillation -   Global step: 5901, epoch step:194
05/05/2022 21:16:05 - INFO - Distillation -   Global step: 5922, epoch step:215
05/05/2022 21:16:09 - INFO - Distillation -   Global step: 5943, epoch step:236
05/05/2022 21:16:13 - INFO - Distillation -   Global step: 5964, epoch step:257
05/05/2022 21:16:17 - INFO - Distillation -   Global step: 5985, epoch step:278
05/05/2022 21:16:20 - INFO - Distillation -   Global step: 6006, epoch step:299
05/05/2022 21:16:24 - INFO - Distillation -   Global step: 6027, epoch step:320
05/05/2022 21:16:28 - INFO - Distillation -   Global step: 6048, epoch step:341
05/05/2022 21:16:32 - INFO - Distillation -   Global step: 6069, epoch step:362
05/05/2022 21:16:36 - INFO - Distillation -   Global step: 6090, epoch step:383
05/05/2022 21:16:40 - INFO - Distillation -   Global step: 6111, epoch step:404
05/05/2022 21:16:43 - INFO - Distillation -   Global step: 6132, epoch step:425
05/05/2022 21:16:46 - INFO - Distillation -   Saving at global step 6146, epoch step 439 epoch 14
05/05/2022 21:16:46 - INFO - Distillation -   Running callback function...
05/05/2022 21:16:46 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' is a path or url to a directory containing tokenizer files.
05/05/2022 21:16:46 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/vocab.txt
05/05/2022 21:16:46 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/added_tokens.json
05/05/2022 21:16:46 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/special_tokens_map.json
05/05/2022 21:16:46 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/tokenizer_config.json
05/05/2022 21:16:46 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_raw_128
05/05/2022 21:16:47 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 21:16:47 - INFO - __main__ -     Num examples = 3453
05/05/2022 21:16:47 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 102.81it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:04, 102.32it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 101.79it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 101.27it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 100.61it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 99.62it/s] Evaluating:  18%|█▊        | 76/432 [00:00<00:03, 98.92it/s]Evaluating:  20%|█▉        | 86/432 [00:00<00:03, 97.76it/s]Evaluating:  22%|██▏       | 96/432 [00:00<00:03, 97.08it/s]Evaluating:  25%|██▍       | 106/432 [00:01<00:03, 96.56it/s]Evaluating:  27%|██▋       | 116/432 [00:01<00:03, 95.77it/s]Evaluating:  29%|██▉       | 126/432 [00:01<00:03, 94.83it/s]Evaluating:  31%|███▏      | 136/432 [00:01<00:03, 94.59it/s]Evaluating:  34%|███▍      | 146/432 [00:01<00:03, 93.90it/s]Evaluating:  36%|███▌      | 156/432 [00:01<00:02, 93.08it/s]Evaluating:  38%|███▊      | 166/432 [00:01<00:02, 92.40it/s]Evaluating:  41%|████      | 176/432 [00:01<00:02, 91.73it/s]Evaluating:  43%|████▎     | 186/432 [00:01<00:02, 91.12it/s]Evaluating:  45%|████▌     | 196/432 [00:02<00:02, 90.52it/s]Evaluating:  48%|████▊     | 206/432 [00:02<00:02, 89.54it/s]Evaluating:  50%|████▉     | 215/432 [00:02<00:02, 88.57it/s]Evaluating:  52%|█████▏    | 224/432 [00:02<00:02, 87.49it/s]Evaluating:  54%|█████▍    | 233/432 [00:02<00:02, 86.64it/s]Evaluating:  56%|█████▌    | 242/432 [00:02<00:02, 85.91it/s]Evaluating:  58%|█████▊    | 251/432 [00:02<00:02, 85.28it/s]Evaluating:  60%|██████    | 260/432 [00:02<00:02, 84.89it/s]Evaluating:  62%|██████▏   | 269/432 [00:02<00:01, 84.27it/s]Evaluating:  64%|██████▍   | 278/432 [00:03<00:01, 83.76it/s]Evaluating:  66%|██████▋   | 287/432 [00:03<00:01, 83.11it/s]Evaluating:  69%|██████▊   | 296/432 [00:03<00:01, 82.07it/s]Evaluating:  71%|███████   | 305/432 [00:03<00:01, 81.33it/s]Evaluating:  73%|███████▎  | 314/432 [00:03<00:01, 80.90it/s]Evaluating:  75%|███████▍  | 323/432 [00:03<00:01, 80.61it/s]Evaluating:  77%|███████▋  | 332/432 [00:03<00:01, 80.45it/s]Evaluating:  79%|███████▉  | 341/432 [00:03<00:01, 80.11it/s]Evaluating:  81%|████████  | 350/432 [00:03<00:01, 79.66it/s]Evaluating:  83%|████████▎ | 358/432 [00:04<00:00, 79.33it/s]Evaluating:  85%|████████▍ | 366/432 [00:04<00:00, 78.96it/s]Evaluating:  87%|████████▋ | 374/432 [00:04<00:00, 78.37it/s]Evaluating:  88%|████████▊ | 382/432 [00:04<00:00, 77.79it/s]Evaluating:  90%|█████████ | 390/432 [00:04<00:00, 77.35it/s]Evaluating:  92%|█████████▏| 398/432 [00:04<00:00, 76.93it/s]Evaluating:  94%|█████████▍| 406/432 [00:04<00:00, 76.59it/s]Evaluating:  96%|█████████▌| 414/432 [00:04<00:00, 76.21it/s]Evaluating:  98%|█████████▊| 422/432 [00:04<00:00, 75.41it/s]Evaluating: 100%|█████████▉| 430/432 [00:04<00:00, 74.71it/s]Evaluating: 100%|██████████| 432/432 [00:05<00:00, 86.29it/s]
05/05/2022 21:16:52 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 21:16:52 - INFO - __main__ -     f1 = 0.9018745049722785
05/05/2022 21:16:52 - INFO - __main__ -     loss = 0.14134401459671575
05/05/2022 21:16:52 - INFO - __main__ -     precision = 0.8959608323133414
05/05/2022 21:16:52 - INFO - __main__ -     recall = 0.9078667611622963
05/05/2022 21:16:52 - INFO - Distillation -   Epoch 14 finished
05/05/2022 21:16:52 - INFO - Distillation -   Epoch 15
05/05/2022 21:16:52 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 21:16:54 - INFO - Distillation -   Global step: 6153, epoch step:7
05/05/2022 21:16:57 - INFO - Distillation -   Global step: 6174, epoch step:28
05/05/2022 21:17:01 - INFO - Distillation -   Global step: 6195, epoch step:49
05/05/2022 21:17:05 - INFO - Distillation -   Global step: 6216, epoch step:70
05/05/2022 21:17:09 - INFO - Distillation -   Global step: 6237, epoch step:91
05/05/2022 21:17:13 - INFO - Distillation -   Global step: 6258, epoch step:112
05/05/2022 21:17:17 - INFO - Distillation -   Global step: 6279, epoch step:133
05/05/2022 21:17:20 - INFO - Distillation -   Global step: 6300, epoch step:154
05/05/2022 21:17:24 - INFO - Distillation -   Global step: 6321, epoch step:175
05/05/2022 21:17:28 - INFO - Distillation -   Global step: 6342, epoch step:196
05/05/2022 21:17:32 - INFO - Distillation -   Global step: 6363, epoch step:217
05/05/2022 21:17:36 - INFO - Distillation -   Global step: 6384, epoch step:238
05/05/2022 21:17:39 - INFO - Distillation -   Global step: 6405, epoch step:259
05/05/2022 21:17:43 - INFO - Distillation -   Global step: 6426, epoch step:280
05/05/2022 21:17:47 - INFO - Distillation -   Global step: 6447, epoch step:301
05/05/2022 21:17:51 - INFO - Distillation -   Global step: 6468, epoch step:322
05/05/2022 21:17:55 - INFO - Distillation -   Global step: 6489, epoch step:343
05/05/2022 21:17:58 - INFO - Distillation -   Global step: 6510, epoch step:364
05/05/2022 21:18:02 - INFO - Distillation -   Global step: 6531, epoch step:385
05/05/2022 21:18:06 - INFO - Distillation -   Global step: 6552, epoch step:406
05/05/2022 21:18:10 - INFO - Distillation -   Global step: 6573, epoch step:427
05/05/2022 21:18:12 - INFO - Distillation -   Saving at global step 6585, epoch step 439 epoch 15
05/05/2022 21:18:12 - INFO - Distillation -   Running callback function...
05/05/2022 21:18:12 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' is a path or url to a directory containing tokenizer files.
05/05/2022 21:18:12 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/vocab.txt
05/05/2022 21:18:12 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/added_tokens.json
05/05/2022 21:18:12 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/special_tokens_map.json
05/05/2022 21:18:12 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/tokenizer_config.json
05/05/2022 21:18:12 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_raw_128
05/05/2022 21:18:13 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 21:18:13 - INFO - __main__ -     Num examples = 3453
05/05/2022 21:18:13 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 102.63it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:03, 102.68it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 102.48it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 101.35it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 100.63it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 100.34it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 99.93it/s] Evaluating:  20%|██        | 87/432 [00:00<00:03, 99.26it/s]Evaluating:  22%|██▏       | 97/432 [00:00<00:03, 98.85it/s]Evaluating:  25%|██▍       | 107/432 [00:01<00:03, 98.16it/s]Evaluating:  27%|██▋       | 117/432 [00:01<00:03, 97.43it/s]Evaluating:  29%|██▉       | 127/432 [00:01<00:03, 96.43it/s]Evaluating:  32%|███▏      | 137/432 [00:01<00:03, 95.60it/s]Evaluating:  34%|███▍      | 147/432 [00:01<00:02, 95.10it/s]Evaluating:  36%|███▋      | 157/432 [00:01<00:02, 94.54it/s]Evaluating:  39%|███▊      | 167/432 [00:01<00:02, 93.64it/s]Evaluating:  41%|████      | 177/432 [00:01<00:02, 92.55it/s]Evaluating:  43%|████▎     | 187/432 [00:01<00:02, 91.14it/s]Evaluating:  46%|████▌     | 197/432 [00:02<00:02, 90.03it/s]Evaluating:  48%|████▊     | 207/432 [00:02<00:02, 89.08it/s]Evaluating:  50%|█████     | 216/432 [00:02<00:02, 88.02it/s]Evaluating:  52%|█████▏    | 225/432 [00:02<00:02, 87.07it/s]Evaluating:  54%|█████▍    | 234/432 [00:02<00:02, 86.38it/s]Evaluating:  56%|█████▋    | 243/432 [00:02<00:02, 85.70it/s]Evaluating:  58%|█████▊    | 252/432 [00:02<00:02, 85.05it/s]Evaluating:  60%|██████    | 261/432 [00:02<00:02, 84.60it/s]Evaluating:  62%|██████▎   | 270/432 [00:02<00:01, 84.09it/s]Evaluating:  65%|██████▍   | 279/432 [00:03<00:01, 83.85it/s]Evaluating:  67%|██████▋   | 288/432 [00:03<00:01, 83.43it/s]Evaluating:  69%|██████▉   | 297/432 [00:03<00:01, 82.27it/s]Evaluating:  71%|███████   | 306/432 [00:03<00:01, 81.07it/s]Evaluating:  73%|███████▎  | 315/432 [00:03<00:01, 80.58it/s]Evaluating:  75%|███████▌  | 324/432 [00:03<00:01, 80.33it/s]Evaluating:  77%|███████▋  | 333/432 [00:03<00:01, 80.10it/s]Evaluating:  79%|███████▉  | 342/432 [00:03<00:01, 79.82it/s]Evaluating:  81%|████████  | 350/432 [00:03<00:01, 79.17it/s]Evaluating:  83%|████████▎ | 358/432 [00:04<00:00, 78.45it/s]Evaluating:  85%|████████▍ | 366/432 [00:04<00:00, 77.64it/s]Evaluating:  87%|████████▋ | 374/432 [00:04<00:00, 76.87it/s]Evaluating:  88%|████████▊ | 382/432 [00:04<00:00, 76.78it/s]Evaluating:  90%|█████████ | 390/432 [00:04<00:00, 76.92it/s]Evaluating:  92%|█████████▏| 398/432 [00:04<00:00, 76.85it/s]Evaluating:  94%|█████████▍| 406/432 [00:04<00:00, 76.65it/s]Evaluating:  96%|█████████▌| 414/432 [00:04<00:00, 76.31it/s]Evaluating:  98%|█████████▊| 422/432 [00:04<00:00, 75.12it/s]Evaluating: 100%|█████████▉| 430/432 [00:04<00:00, 74.14it/s]Evaluating: 100%|██████████| 432/432 [00:05<00:00, 86.36it/s]
05/05/2022 21:18:18 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 21:18:18 - INFO - __main__ -     f1 = 0.9030324400564174
05/05/2022 21:18:18 - INFO - __main__ -     loss = 0.13967654469946764
05/05/2022 21:18:18 - INFO - __main__ -     precision = 0.8985964912280702
05/05/2022 21:18:18 - INFO - __main__ -     recall = 0.907512402551382
05/05/2022 21:18:18 - INFO - Distillation -   Epoch 15 finished
05/05/2022 21:18:18 - INFO - Distillation -   Epoch 16
05/05/2022 21:18:18 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 21:18:20 - INFO - Distillation -   Global step: 6594, epoch step:9
05/05/2022 21:18:24 - INFO - Distillation -   Global step: 6615, epoch step:30
05/05/2022 21:18:28 - INFO - Distillation -   Global step: 6636, epoch step:51
05/05/2022 21:18:32 - INFO - Distillation -   Global step: 6657, epoch step:72
05/05/2022 21:18:35 - INFO - Distillation -   Global step: 6678, epoch step:93
05/05/2022 21:18:39 - INFO - Distillation -   Global step: 6699, epoch step:114
05/05/2022 21:18:43 - INFO - Distillation -   Global step: 6720, epoch step:135
05/05/2022 21:18:47 - INFO - Distillation -   Global step: 6741, epoch step:156
05/05/2022 21:18:51 - INFO - Distillation -   Global step: 6762, epoch step:177
05/05/2022 21:18:54 - INFO - Distillation -   Global step: 6783, epoch step:198
05/05/2022 21:18:58 - INFO - Distillation -   Global step: 6804, epoch step:219
05/05/2022 21:19:02 - INFO - Distillation -   Global step: 6825, epoch step:240
05/05/2022 21:19:06 - INFO - Distillation -   Global step: 6846, epoch step:261
05/05/2022 21:19:10 - INFO - Distillation -   Global step: 6867, epoch step:282
05/05/2022 21:19:13 - INFO - Distillation -   Global step: 6888, epoch step:303
05/05/2022 21:19:17 - INFO - Distillation -   Global step: 6909, epoch step:324
05/05/2022 21:19:21 - INFO - Distillation -   Global step: 6930, epoch step:345
05/05/2022 21:19:25 - INFO - Distillation -   Global step: 6951, epoch step:366
05/05/2022 21:19:29 - INFO - Distillation -   Global step: 6972, epoch step:387
05/05/2022 21:19:33 - INFO - Distillation -   Global step: 6993, epoch step:408
05/05/2022 21:19:36 - INFO - Distillation -   Global step: 7014, epoch step:429
05/05/2022 21:19:38 - INFO - Distillation -   Saving at global step 7024, epoch step 439 epoch 16
05/05/2022 21:19:39 - INFO - Distillation -   Running callback function...
05/05/2022 21:19:39 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' is a path or url to a directory containing tokenizer files.
05/05/2022 21:19:39 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/vocab.txt
05/05/2022 21:19:39 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/added_tokens.json
05/05/2022 21:19:39 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/special_tokens_map.json
05/05/2022 21:19:39 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/tokenizer_config.json
05/05/2022 21:19:39 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_raw_128
05/05/2022 21:19:39 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 21:19:39 - INFO - __main__ -     Num examples = 3453
05/05/2022 21:19:39 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 102.33it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:04, 102.11it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 102.22it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 101.45it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 100.92it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 100.76it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 100.32it/s]Evaluating:  20%|██        | 88/432 [00:00<00:03, 99.90it/s] Evaluating:  23%|██▎       | 98/432 [00:00<00:03, 99.46it/s]Evaluating:  25%|██▌       | 108/432 [00:01<00:03, 98.68it/s]Evaluating:  27%|██▋       | 118/432 [00:01<00:03, 97.72it/s]Evaluating:  30%|██▉       | 128/432 [00:01<00:03, 96.97it/s]Evaluating:  32%|███▏      | 138/432 [00:01<00:03, 95.68it/s]Evaluating:  34%|███▍      | 148/432 [00:01<00:03, 94.33it/s]Evaluating:  37%|███▋      | 158/432 [00:01<00:02, 93.41it/s]Evaluating:  39%|███▉      | 168/432 [00:01<00:02, 92.86it/s]Evaluating:  41%|████      | 178/432 [00:01<00:02, 92.23it/s]Evaluating:  44%|████▎     | 188/432 [00:01<00:02, 91.40it/s]Evaluating:  46%|████▌     | 198/432 [00:02<00:02, 90.51it/s]Evaluating:  48%|████▊     | 208/432 [00:02<00:02, 89.44it/s]Evaluating:  50%|█████     | 217/432 [00:02<00:02, 88.39it/s]Evaluating:  52%|█████▏    | 226/432 [00:02<00:02, 87.54it/s]Evaluating:  54%|█████▍    | 235/432 [00:02<00:02, 86.23it/s]Evaluating:  56%|█████▋    | 244/432 [00:02<00:02, 85.49it/s]Evaluating:  59%|█████▊    | 253/432 [00:02<00:02, 84.87it/s]Evaluating:  61%|██████    | 262/432 [00:02<00:02, 84.34it/s]Evaluating:  63%|██████▎   | 271/432 [00:02<00:01, 84.01it/s]Evaluating:  65%|██████▍   | 280/432 [00:03<00:01, 83.20it/s]Evaluating:  67%|██████▋   | 289/432 [00:03<00:01, 82.59it/s]Evaluating:  69%|██████▉   | 298/432 [00:03<00:01, 81.92it/s]Evaluating:  71%|███████   | 307/432 [00:03<00:01, 81.62it/s]Evaluating:  73%|███████▎  | 316/432 [00:03<00:01, 80.90it/s]Evaluating:  75%|███████▌  | 325/432 [00:03<00:01, 80.59it/s]Evaluating:  77%|███████▋  | 334/432 [00:03<00:01, 79.94it/s]Evaluating:  79%|███████▉  | 342/432 [00:03<00:01, 79.58it/s]Evaluating:  81%|████████  | 350/432 [00:03<00:01, 79.07it/s]Evaluating:  83%|████████▎ | 358/432 [00:04<00:00, 78.57it/s]Evaluating:  85%|████████▍ | 366/432 [00:04<00:00, 78.15it/s]Evaluating:  87%|████████▋ | 374/432 [00:04<00:00, 77.44it/s]Evaluating:  88%|████████▊ | 382/432 [00:04<00:00, 76.95it/s]Evaluating:  90%|█████████ | 390/432 [00:04<00:00, 76.54it/s]Evaluating:  92%|█████████▏| 398/432 [00:04<00:00, 75.63it/s]Evaluating:  94%|█████████▍| 406/432 [00:04<00:00, 75.55it/s]Evaluating:  96%|█████████▌| 414/432 [00:04<00:00, 75.61it/s]Evaluating:  98%|█████████▊| 422/432 [00:04<00:00, 75.46it/s]Evaluating: 100%|█████████▉| 430/432 [00:04<00:00, 74.92it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 86.43it/s]
05/05/2022 21:19:45 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 21:19:45 - INFO - __main__ -     f1 = 0.9015198102433453
05/05/2022 21:19:45 - INFO - __main__ -     loss = 0.14133985525339726
05/05/2022 21:19:45 - INFO - __main__ -     precision = 0.8940581982923854
05/05/2022 21:19:45 - INFO - __main__ -     recall = 0.9091070163004961
05/05/2022 21:19:45 - INFO - Distillation -   Epoch 16 finished
05/05/2022 21:19:45 - INFO - Distillation -   Epoch 17
05/05/2022 21:19:45 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 21:19:47 - INFO - Distillation -   Global step: 7035, epoch step:11
05/05/2022 21:19:50 - INFO - Distillation -   Global step: 7056, epoch step:32
05/05/2022 21:19:54 - INFO - Distillation -   Global step: 7077, epoch step:53
05/05/2022 21:19:58 - INFO - Distillation -   Global step: 7098, epoch step:74
05/05/2022 21:20:02 - INFO - Distillation -   Global step: 7119, epoch step:95
05/05/2022 21:20:06 - INFO - Distillation -   Global step: 7140, epoch step:116
05/05/2022 21:20:09 - INFO - Distillation -   Global step: 7161, epoch step:137
05/05/2022 21:20:13 - INFO - Distillation -   Global step: 7182, epoch step:158
05/05/2022 21:20:17 - INFO - Distillation -   Global step: 7203, epoch step:179
05/05/2022 21:20:21 - INFO - Distillation -   Global step: 7224, epoch step:200
05/05/2022 21:20:25 - INFO - Distillation -   Global step: 7245, epoch step:221
05/05/2022 21:20:29 - INFO - Distillation -   Global step: 7266, epoch step:242
05/05/2022 21:20:32 - INFO - Distillation -   Global step: 7287, epoch step:263
05/05/2022 21:20:36 - INFO - Distillation -   Global step: 7308, epoch step:284
05/05/2022 21:20:40 - INFO - Distillation -   Global step: 7329, epoch step:305
05/05/2022 21:20:44 - INFO - Distillation -   Global step: 7350, epoch step:326
05/05/2022 21:20:48 - INFO - Distillation -   Global step: 7371, epoch step:347
05/05/2022 21:20:51 - INFO - Distillation -   Global step: 7392, epoch step:368
05/05/2022 21:20:55 - INFO - Distillation -   Global step: 7413, epoch step:389
05/05/2022 21:20:59 - INFO - Distillation -   Global step: 7434, epoch step:410
05/05/2022 21:21:03 - INFO - Distillation -   Global step: 7455, epoch step:431
05/05/2022 21:21:04 - INFO - Distillation -   Saving at global step 7463, epoch step 439 epoch 17
05/05/2022 21:21:05 - INFO - Distillation -   Running callback function...
05/05/2022 21:21:05 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' is a path or url to a directory containing tokenizer files.
05/05/2022 21:21:05 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/vocab.txt
05/05/2022 21:21:05 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/added_tokens.json
05/05/2022 21:21:05 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/special_tokens_map.json
05/05/2022 21:21:05 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/tokenizer_config.json
05/05/2022 21:21:05 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_raw_128
05/05/2022 21:21:05 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 21:21:05 - INFO - __main__ -     Num examples = 3453
05/05/2022 21:21:05 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 103.11it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:03, 102.69it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 101.57it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 100.53it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 99.90it/s] Evaluating:  15%|█▌        | 65/432 [00:00<00:03, 98.87it/s]Evaluating:  17%|█▋        | 75/432 [00:00<00:03, 98.95it/s]Evaluating:  20%|█▉        | 85/432 [00:00<00:03, 99.16it/s]Evaluating:  22%|██▏       | 95/432 [00:00<00:03, 98.89it/s]Evaluating:  24%|██▍       | 105/432 [00:01<00:03, 98.19it/s]Evaluating:  27%|██▋       | 115/432 [00:01<00:03, 97.80it/s]Evaluating:  29%|██▉       | 125/432 [00:01<00:03, 96.92it/s]Evaluating:  31%|███▏      | 135/432 [00:01<00:03, 95.03it/s]Evaluating:  34%|███▎      | 145/432 [00:01<00:03, 94.30it/s]Evaluating:  36%|███▌      | 155/432 [00:01<00:02, 93.99it/s]Evaluating:  38%|███▊      | 165/432 [00:01<00:02, 93.39it/s]Evaluating:  41%|████      | 175/432 [00:01<00:02, 92.38it/s]Evaluating:  43%|████▎     | 185/432 [00:01<00:02, 91.30it/s]Evaluating:  45%|████▌     | 195/432 [00:02<00:02, 90.32it/s]Evaluating:  47%|████▋     | 205/432 [00:02<00:02, 89.34it/s]Evaluating:  50%|████▉     | 214/432 [00:02<00:02, 88.42it/s]Evaluating:  52%|█████▏    | 223/432 [00:02<00:02, 87.74it/s]Evaluating:  54%|█████▎    | 232/432 [00:02<00:02, 86.52it/s]Evaluating:  56%|█████▌    | 241/432 [00:02<00:02, 85.46it/s]Evaluating:  58%|█████▊    | 250/432 [00:02<00:02, 84.40it/s]Evaluating:  60%|█████▉    | 259/432 [00:02<00:02, 83.65it/s]Evaluating:  62%|██████▏   | 268/432 [00:02<00:01, 82.97it/s]Evaluating:  64%|██████▍   | 277/432 [00:03<00:01, 82.25it/s]Evaluating:  66%|██████▌   | 286/432 [00:03<00:01, 81.46it/s]Evaluating:  68%|██████▊   | 295/432 [00:03<00:01, 80.80it/s]Evaluating:  70%|███████   | 304/432 [00:03<00:01, 80.21it/s]Evaluating:  72%|███████▏  | 313/432 [00:03<00:01, 79.44it/s]Evaluating:  74%|███████▍  | 321/432 [00:03<00:01, 78.72it/s]Evaluating:  76%|███████▌  | 329/432 [00:03<00:01, 78.27it/s]Evaluating:  78%|███████▊  | 337/432 [00:03<00:01, 77.99it/s]Evaluating:  80%|███████▉  | 345/432 [00:03<00:01, 77.69it/s]Evaluating:  82%|████████▏ | 353/432 [00:03<00:01, 77.27it/s]Evaluating:  84%|████████▎ | 361/432 [00:04<00:00, 77.03it/s]Evaluating:  85%|████████▌ | 369/432 [00:04<00:00, 76.62it/s]Evaluating:  87%|████████▋ | 377/432 [00:04<00:00, 76.17it/s]Evaluating:  89%|████████▉ | 385/432 [00:04<00:00, 75.77it/s]Evaluating:  91%|█████████ | 393/432 [00:04<00:00, 75.31it/s]Evaluating:  93%|█████████▎| 401/432 [00:04<00:00, 74.74it/s]Evaluating:  95%|█████████▍| 409/432 [00:04<00:00, 74.30it/s]Evaluating:  97%|█████████▋| 417/432 [00:04<00:00, 74.14it/s]Evaluating:  98%|█████████▊| 425/432 [00:04<00:00, 73.86it/s]Evaluating: 100%|██████████| 432/432 [00:05<00:00, 85.48it/s]
05/05/2022 21:21:11 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 21:21:11 - INFO - __main__ -     f1 = 0.9026751143963394
05/05/2022 21:21:11 - INFO - __main__ -     loss = 0.14124397426331192
05/05/2022 21:21:11 - INFO - __main__ -     precision = 0.8966783216783217
05/05/2022 21:21:11 - INFO - __main__ -     recall = 0.9087526576895819
05/05/2022 21:21:11 - INFO - Distillation -   Epoch 17 finished
05/05/2022 21:21:11 - INFO - Distillation -   Epoch 18
05/05/2022 21:21:11 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 21:21:13 - INFO - Distillation -   Global step: 7476, epoch step:13
05/05/2022 21:21:17 - INFO - Distillation -   Global step: 7497, epoch step:34
05/05/2022 21:21:21 - INFO - Distillation -   Global step: 7518, epoch step:55
05/05/2022 21:21:25 - INFO - Distillation -   Global step: 7539, epoch step:76
05/05/2022 21:21:28 - INFO - Distillation -   Global step: 7560, epoch step:97
05/05/2022 21:21:32 - INFO - Distillation -   Global step: 7581, epoch step:118
05/05/2022 21:21:36 - INFO - Distillation -   Global step: 7602, epoch step:139
05/05/2022 21:21:40 - INFO - Distillation -   Global step: 7623, epoch step:160
05/05/2022 21:21:44 - INFO - Distillation -   Global step: 7644, epoch step:181
05/05/2022 21:21:47 - INFO - Distillation -   Global step: 7665, epoch step:202
05/05/2022 21:21:51 - INFO - Distillation -   Global step: 7686, epoch step:223
05/05/2022 21:21:55 - INFO - Distillation -   Global step: 7707, epoch step:244
05/05/2022 21:21:59 - INFO - Distillation -   Global step: 7728, epoch step:265
05/05/2022 21:22:03 - INFO - Distillation -   Global step: 7749, epoch step:286
05/05/2022 21:22:07 - INFO - Distillation -   Global step: 7770, epoch step:307
05/05/2022 21:22:10 - INFO - Distillation -   Global step: 7791, epoch step:328
05/05/2022 21:22:14 - INFO - Distillation -   Global step: 7812, epoch step:349
05/05/2022 21:22:18 - INFO - Distillation -   Global step: 7833, epoch step:370
05/05/2022 21:22:22 - INFO - Distillation -   Global step: 7854, epoch step:391
05/05/2022 21:22:26 - INFO - Distillation -   Global step: 7875, epoch step:412
05/05/2022 21:22:29 - INFO - Distillation -   Global step: 7896, epoch step:433
05/05/2022 21:22:31 - INFO - Distillation -   Saving at global step 7902, epoch step 439 epoch 18
05/05/2022 21:22:31 - INFO - Distillation -   Running callback function...
05/05/2022 21:22:31 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' is a path or url to a directory containing tokenizer files.
05/05/2022 21:22:31 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/vocab.txt
05/05/2022 21:22:31 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/added_tokens.json
05/05/2022 21:22:31 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/special_tokens_map.json
05/05/2022 21:22:31 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/tokenizer_config.json
05/05/2022 21:22:31 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_raw_128
05/05/2022 21:22:31 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 21:22:31 - INFO - __main__ -     Num examples = 3453
05/05/2022 21:22:31 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 102.37it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:04, 102.41it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 102.02it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 101.35it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 100.82it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 100.34it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 100.03it/s]Evaluating:  20%|██        | 88/432 [00:00<00:03, 99.34it/s] Evaluating:  23%|██▎       | 98/432 [00:00<00:03, 98.70it/s]Evaluating:  25%|██▌       | 108/432 [00:01<00:03, 98.12it/s]Evaluating:  27%|██▋       | 118/432 [00:01<00:03, 97.87it/s]Evaluating:  30%|██▉       | 128/432 [00:01<00:03, 97.46it/s]Evaluating:  32%|███▏      | 138/432 [00:01<00:03, 96.72it/s]Evaluating:  34%|███▍      | 148/432 [00:01<00:02, 95.72it/s]Evaluating:  37%|███▋      | 158/432 [00:01<00:02, 94.43it/s]Evaluating:  39%|███▉      | 168/432 [00:01<00:02, 93.62it/s]Evaluating:  41%|████      | 178/432 [00:01<00:02, 92.96it/s]Evaluating:  44%|████▎     | 188/432 [00:01<00:02, 91.92it/s]Evaluating:  46%|████▌     | 198/432 [00:02<00:02, 90.87it/s]Evaluating:  48%|████▊     | 208/432 [00:02<00:02, 89.65it/s]Evaluating:  50%|█████     | 217/432 [00:02<00:02, 88.76it/s]Evaluating:  52%|█████▏    | 226/432 [00:02<00:02, 87.64it/s]Evaluating:  54%|█████▍    | 235/432 [00:02<00:02, 86.72it/s]Evaluating:  56%|█████▋    | 244/432 [00:02<00:02, 86.14it/s]Evaluating:  59%|█████▊    | 253/432 [00:02<00:02, 85.40it/s]Evaluating:  61%|██████    | 262/432 [00:02<00:02, 84.83it/s]Evaluating:  63%|██████▎   | 271/432 [00:02<00:01, 84.21it/s]Evaluating:  65%|██████▍   | 280/432 [00:03<00:01, 83.34it/s]Evaluating:  67%|██████▋   | 289/432 [00:03<00:01, 82.62it/s]Evaluating:  69%|██████▉   | 298/432 [00:03<00:01, 81.97it/s]Evaluating:  71%|███████   | 307/432 [00:03<00:01, 81.19it/s]Evaluating:  73%|███████▎  | 316/432 [00:03<00:01, 80.49it/s]Evaluating:  75%|███████▌  | 325/432 [00:03<00:01, 80.00it/s]Evaluating:  77%|███████▋  | 334/432 [00:03<00:01, 79.52it/s]Evaluating:  79%|███████▉  | 342/432 [00:03<00:01, 79.08it/s]Evaluating:  81%|████████  | 350/432 [00:03<00:01, 78.79it/s]Evaluating:  83%|████████▎ | 358/432 [00:04<00:00, 78.59it/s]Evaluating:  85%|████████▍ | 366/432 [00:04<00:00, 77.58it/s]Evaluating:  87%|████████▋ | 374/432 [00:04<00:00, 76.97it/s]Evaluating:  88%|████████▊ | 382/432 [00:04<00:00, 76.47it/s]Evaluating:  90%|█████████ | 390/432 [00:04<00:00, 76.47it/s]Evaluating:  92%|█████████▏| 398/432 [00:04<00:00, 76.07it/s]Evaluating:  94%|█████████▍| 406/432 [00:04<00:00, 75.87it/s]Evaluating:  96%|█████████▌| 414/432 [00:04<00:00, 75.50it/s]Evaluating:  98%|█████████▊| 422/432 [00:04<00:00, 75.10it/s]Evaluating: 100%|█████████▉| 430/432 [00:04<00:00, 74.65it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 86.47it/s]
05/05/2022 21:22:37 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 21:22:37 - INFO - __main__ -     f1 = 0.9030159148861339
05/05/2022 21:22:37 - INFO - __main__ -     loss = 0.14246794430623258
05/05/2022 21:22:37 - INFO - __main__ -     precision = 0.8963169837668005
05/05/2022 21:22:37 - INFO - __main__ -     recall = 0.9098157335223246
05/05/2022 21:22:37 - INFO - Distillation -   Epoch 18 finished
05/05/2022 21:22:37 - INFO - Distillation -   Epoch 19
05/05/2022 21:22:37 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 21:22:40 - INFO - Distillation -   Global step: 7917, epoch step:15
05/05/2022 21:22:43 - INFO - Distillation -   Global step: 7938, epoch step:36
05/05/2022 21:22:47 - INFO - Distillation -   Global step: 7959, epoch step:57
05/05/2022 21:22:51 - INFO - Distillation -   Global step: 7980, epoch step:78
05/05/2022 21:22:55 - INFO - Distillation -   Global step: 8001, epoch step:99
05/05/2022 21:22:59 - INFO - Distillation -   Global step: 8022, epoch step:120
05/05/2022 21:23:03 - INFO - Distillation -   Global step: 8043, epoch step:141
05/05/2022 21:23:06 - INFO - Distillation -   Global step: 8064, epoch step:162
05/05/2022 21:23:10 - INFO - Distillation -   Global step: 8085, epoch step:183
05/05/2022 21:23:14 - INFO - Distillation -   Global step: 8106, epoch step:204
05/05/2022 21:23:18 - INFO - Distillation -   Global step: 8127, epoch step:225
05/05/2022 21:23:22 - INFO - Distillation -   Global step: 8148, epoch step:246
05/05/2022 21:23:25 - INFO - Distillation -   Global step: 8169, epoch step:267
05/05/2022 21:23:29 - INFO - Distillation -   Global step: 8190, epoch step:288
05/05/2022 21:23:33 - INFO - Distillation -   Global step: 8211, epoch step:309
05/05/2022 21:23:37 - INFO - Distillation -   Global step: 8232, epoch step:330
05/05/2022 21:23:41 - INFO - Distillation -   Global step: 8253, epoch step:351
05/05/2022 21:23:45 - INFO - Distillation -   Global step: 8274, epoch step:372
05/05/2022 21:23:48 - INFO - Distillation -   Global step: 8295, epoch step:393
05/05/2022 21:23:52 - INFO - Distillation -   Global step: 8316, epoch step:414
05/05/2022 21:23:56 - INFO - Distillation -   Global step: 8337, epoch step:435
05/05/2022 21:23:57 - INFO - Distillation -   Saving at global step 8341, epoch step 439 epoch 19
05/05/2022 21:23:57 - INFO - Distillation -   Running callback function...
05/05/2022 21:23:57 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' is a path or url to a directory containing tokenizer files.
05/05/2022 21:23:57 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/vocab.txt
05/05/2022 21:23:57 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/added_tokens.json
05/05/2022 21:23:57 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/special_tokens_map.json
05/05/2022 21:23:57 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/tokenizer_config.json
05/05/2022 21:23:57 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_raw_128
05/05/2022 21:23:57 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 21:23:57 - INFO - __main__ -     Num examples = 3453
05/05/2022 21:23:57 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 102.51it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:04, 102.49it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 102.58it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 102.05it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 101.42it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 100.90it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 100.37it/s]Evaluating:  20%|██        | 88/432 [00:00<00:03, 99.62it/s] Evaluating:  23%|██▎       | 98/432 [00:00<00:03, 98.77it/s]Evaluating:  25%|██▌       | 108/432 [00:01<00:03, 98.18it/s]Evaluating:  27%|██▋       | 118/432 [00:01<00:03, 97.12it/s]Evaluating:  30%|██▉       | 128/432 [00:01<00:03, 96.48it/s]Evaluating:  32%|███▏      | 138/432 [00:01<00:03, 95.74it/s]Evaluating:  34%|███▍      | 148/432 [00:01<00:02, 94.99it/s]Evaluating:  37%|███▋      | 158/432 [00:01<00:02, 93.73it/s]Evaluating:  39%|███▉      | 168/432 [00:01<00:02, 93.02it/s]Evaluating:  41%|████      | 178/432 [00:01<00:02, 92.37it/s]Evaluating:  44%|████▎     | 188/432 [00:01<00:02, 91.15it/s]Evaluating:  46%|████▌     | 198/432 [00:02<00:02, 90.12it/s]Evaluating:  48%|████▊     | 208/432 [00:02<00:02, 88.92it/s]Evaluating:  50%|█████     | 217/432 [00:02<00:02, 87.99it/s]Evaluating:  52%|█████▏    | 226/432 [00:02<00:02, 86.86it/s]Evaluating:  54%|█████▍    | 235/432 [00:02<00:02, 85.63it/s]Evaluating:  56%|█████▋    | 244/432 [00:02<00:02, 84.80it/s]Evaluating:  59%|█████▊    | 253/432 [00:02<00:02, 83.79it/s]Evaluating:  61%|██████    | 262/432 [00:02<00:02, 82.41it/s]Evaluating:  63%|██████▎   | 271/432 [00:02<00:01, 81.27it/s]Evaluating:  65%|██████▍   | 280/432 [00:03<00:01, 80.29it/s]Evaluating:  67%|██████▋   | 289/432 [00:03<00:01, 79.61it/s]Evaluating:  69%|██████▉   | 297/432 [00:03<00:01, 79.21it/s]Evaluating:  71%|███████   | 305/432 [00:03<00:01, 78.77it/s]Evaluating:  72%|███████▏  | 313/432 [00:03<00:01, 78.24it/s]Evaluating:  74%|███████▍  | 321/432 [00:03<00:01, 77.62it/s]Evaluating:  76%|███████▌  | 329/432 [00:03<00:01, 77.05it/s]Evaluating:  78%|███████▊  | 337/432 [00:03<00:01, 76.49it/s]Evaluating:  80%|███████▉  | 345/432 [00:03<00:01, 75.92it/s]Evaluating:  82%|████████▏ | 353/432 [00:04<00:01, 75.06it/s]Evaluating:  84%|████████▎ | 361/432 [00:04<00:00, 74.77it/s]Evaluating:  85%|████████▌ | 369/432 [00:04<00:00, 74.35it/s]Evaluating:  87%|████████▋ | 377/432 [00:04<00:00, 74.18it/s]Evaluating:  89%|████████▉ | 385/432 [00:04<00:00, 73.71it/s]Evaluating:  91%|█████████ | 393/432 [00:04<00:00, 73.26it/s]Evaluating:  93%|█████████▎| 401/432 [00:04<00:00, 72.98it/s]Evaluating:  95%|█████████▍| 409/432 [00:04<00:00, 72.64it/s]Evaluating:  97%|█████████▋| 417/432 [00:04<00:00, 72.32it/s]Evaluating:  98%|█████████▊| 425/432 [00:05<00:00, 71.77it/s]Evaluating: 100%|██████████| 432/432 [00:05<00:00, 84.57it/s]
05/05/2022 21:24:03 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 21:24:03 - INFO - __main__ -     f1 = 0.9025956885173779
05/05/2022 21:24:03 - INFO - __main__ -     loss = 0.1409873275579735
05/05/2022 21:24:03 - INFO - __main__ -     precision = 0.8965215871351162
05/05/2022 21:24:03 - INFO - __main__ -     recall = 0.9087526576895819
05/05/2022 21:24:03 - INFO - Distillation -   Epoch 19 finished
05/05/2022 21:24:03 - INFO - Distillation -   Epoch 20
05/05/2022 21:24:03 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 21:24:06 - INFO - Distillation -   Global step: 8358, epoch step:17
05/05/2022 21:24:10 - INFO - Distillation -   Global step: 8379, epoch step:38
05/05/2022 21:24:14 - INFO - Distillation -   Global step: 8400, epoch step:59
05/05/2022 21:24:18 - INFO - Distillation -   Global step: 8421, epoch step:80
05/05/2022 21:24:22 - INFO - Distillation -   Global step: 8442, epoch step:101
05/05/2022 21:24:25 - INFO - Distillation -   Global step: 8463, epoch step:122
05/05/2022 21:24:29 - INFO - Distillation -   Global step: 8484, epoch step:143
05/05/2022 21:24:33 - INFO - Distillation -   Global step: 8505, epoch step:164
05/05/2022 21:24:37 - INFO - Distillation -   Global step: 8526, epoch step:185
05/05/2022 21:24:41 - INFO - Distillation -   Global step: 8547, epoch step:206
05/05/2022 21:24:45 - INFO - Distillation -   Global step: 8568, epoch step:227
05/05/2022 21:24:48 - INFO - Distillation -   Global step: 8589, epoch step:248
05/05/2022 21:24:52 - INFO - Distillation -   Global step: 8610, epoch step:269
05/05/2022 21:24:56 - INFO - Distillation -   Global step: 8631, epoch step:290
05/05/2022 21:25:00 - INFO - Distillation -   Global step: 8652, epoch step:311
05/05/2022 21:25:04 - INFO - Distillation -   Global step: 8673, epoch step:332
05/05/2022 21:25:07 - INFO - Distillation -   Global step: 8694, epoch step:353
05/05/2022 21:25:11 - INFO - Distillation -   Global step: 8715, epoch step:374
05/05/2022 21:25:15 - INFO - Distillation -   Global step: 8736, epoch step:395
05/05/2022 21:25:19 - INFO - Distillation -   Global step: 8757, epoch step:416
05/05/2022 21:25:23 - INFO - Distillation -   Global step: 8778, epoch step:437
05/05/2022 21:25:23 - INFO - Distillation -   Saving at global step 8780, epoch step 439 epoch 20
05/05/2022 21:25:23 - INFO - Distillation -   Running callback function...
05/05/2022 21:25:23 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw' is a path or url to a directory containing tokenizer files.
05/05/2022 21:25:23 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/vocab.txt
05/05/2022 21:25:23 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/added_tokens.json
05/05/2022 21:25:23 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/special_tokens_map.json
05/05/2022 21:25:23 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher_raw/tokenizer_config.json
05/05/2022 21:25:23 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_raw_128
05/05/2022 21:25:24 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 21:25:24 - INFO - __main__ -     Num examples = 3453
05/05/2022 21:25:24 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 102.59it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:04, 102.39it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 101.97it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 101.26it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 100.95it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 100.20it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 100.09it/s]Evaluating:  20%|██        | 88/432 [00:00<00:03, 99.58it/s] Evaluating:  23%|██▎       | 98/432 [00:00<00:03, 99.51it/s]Evaluating:  25%|██▌       | 108/432 [00:01<00:03, 98.45it/s]Evaluating:  27%|██▋       | 118/432 [00:01<00:03, 97.55it/s]Evaluating:  30%|██▉       | 128/432 [00:01<00:03, 96.73it/s]Evaluating:  32%|███▏      | 138/432 [00:01<00:03, 95.67it/s]Evaluating:  34%|███▍      | 148/432 [00:01<00:03, 94.36it/s]Evaluating:  37%|███▋      | 158/432 [00:01<00:02, 93.40it/s]Evaluating:  39%|███▉      | 168/432 [00:01<00:02, 92.29it/s]Evaluating:  41%|████      | 178/432 [00:01<00:02, 91.30it/s]Evaluating:  44%|████▎     | 188/432 [00:01<00:02, 90.43it/s]Evaluating:  46%|████▌     | 198/432 [00:02<00:02, 89.50it/s]Evaluating:  48%|████▊     | 207/432 [00:02<00:02, 88.58it/s]Evaluating:  50%|█████     | 216/432 [00:02<00:02, 87.72it/s]Evaluating:  52%|█████▏    | 225/432 [00:02<00:02, 86.83it/s]Evaluating:  54%|█████▍    | 234/432 [00:02<00:02, 86.01it/s]Evaluating:  56%|█████▋    | 243/432 [00:02<00:02, 85.46it/s]Evaluating:  58%|█████▊    | 252/432 [00:02<00:02, 84.85it/s]Evaluating:  60%|██████    | 261/432 [00:02<00:02, 84.16it/s]Evaluating:  62%|██████▎   | 270/432 [00:02<00:01, 83.56it/s]Evaluating:  65%|██████▍   | 279/432 [00:03<00:01, 82.99it/s]Evaluating:  67%|██████▋   | 288/432 [00:03<00:01, 82.38it/s]Evaluating:  69%|██████▉   | 297/432 [00:03<00:01, 81.55it/s]Evaluating:  71%|███████   | 306/432 [00:03<00:01, 80.79it/s]Evaluating:  73%|███████▎  | 315/432 [00:03<00:01, 80.20it/s]Evaluating:  75%|███████▌  | 324/432 [00:03<00:01, 79.63it/s]Evaluating:  77%|███████▋  | 332/432 [00:03<00:01, 78.89it/s]Evaluating:  79%|███████▊  | 340/432 [00:03<00:01, 78.95it/s]Evaluating:  81%|████████  | 348/432 [00:03<00:01, 78.96it/s]Evaluating:  82%|████████▏ | 356/432 [00:04<00:00, 78.80it/s]Evaluating:  84%|████████▍ | 364/432 [00:04<00:00, 78.53it/s]Evaluating:  86%|████████▌ | 372/432 [00:04<00:00, 77.87it/s]Evaluating:  88%|████████▊ | 380/432 [00:04<00:00, 77.35it/s]Evaluating:  90%|████████▉ | 388/432 [00:04<00:00, 76.88it/s]Evaluating:  92%|█████████▏| 396/432 [00:04<00:00, 76.42it/s]Evaluating:  94%|█████████▎| 404/432 [00:04<00:00, 75.91it/s]Evaluating:  95%|█████████▌| 412/432 [00:04<00:00, 75.17it/s]Evaluating:  97%|█████████▋| 420/432 [00:04<00:00, 74.72it/s]Evaluating:  99%|█████████▉| 428/432 [00:04<00:00, 74.34it/s]Evaluating: 100%|██████████| 432/432 [00:05<00:00, 86.06it/s]
05/05/2022 21:25:29 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 21:25:29 - INFO - __main__ -     f1 = 0.9022265246853824
05/05/2022 21:25:29 - INFO - __main__ -     loss = 0.1406874964825925
05/05/2022 21:25:29 - INFO - __main__ -     precision = 0.8963105438013639
05/05/2022 21:25:29 - INFO - __main__ -     recall = 0.9082211197732105
05/05/2022 21:25:29 - INFO - Distillation -   Epoch 20 finished
05/05/2022 21:25:29 - INFO - __main__ -   Saving model checkpoint to /home/hs3228/TextBrewer/output_model/conll_distill/student_raw_layer5
05/05/2022 21:25:29 - INFO - transformers.configuration_utils -   Configuration saved in /home/hs3228/TextBrewer/output_model/conll_distill/student_raw_layer5/config.json
05/05/2022 21:25:30 - INFO - transformers.modeling_utils -   Model weights saved in /home/hs3228/TextBrewer/output_model/conll_distill/student_raw_layer5/pytorch_model.bin
05/05/2022 21:25:30 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/student_raw_layer5' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/student_raw_layer5' is a path or url to a directory containing tokenizer files.
05/05/2022 21:25:30 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/student_raw_layer5/vocab.txt
05/05/2022 21:25:30 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/student_raw_layer5/added_tokens.json
05/05/2022 21:25:30 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/student_raw_layer5/special_tokens_map.json
05/05/2022 21:25:30 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/student_raw_layer5/tokenizer_config.json
05/05/2022 21:25:30 - INFO - __main__ -   Evaluate the following checkpoints: ['/home/hs3228/TextBrewer/output_model/conll_distill/student_raw_layer5']
05/05/2022 21:25:30 - INFO - transformers.configuration_utils -   loading configuration file /home/hs3228/TextBrewer/output_model/conll_distill/student_raw_layer5/config.json
05/05/2022 21:25:30 - INFO - transformers.configuration_utils -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "num_labels": 9,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

05/05/2022 21:25:30 - INFO - transformers.modeling_utils -   loading weights file /home/hs3228/TextBrewer/output_model/conll_distill/student_raw_layer5/pytorch_model.bin
05/05/2022 21:25:31 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_dev_teacher_raw_128
05/05/2022 21:25:32 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 21:25:32 - INFO - __main__ -     Num examples = 3250
05/05/2022 21:25:32 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/407 [00:00<?, ?it/s]Evaluating:   2%|▏         | 10/407 [00:00<00:04, 93.23it/s]Evaluating:   5%|▍         | 20/407 [00:00<00:04, 94.67it/s]Evaluating:   7%|▋         | 30/407 [00:00<00:03, 96.73it/s]Evaluating:  10%|█         | 41/407 [00:00<00:03, 98.16it/s]Evaluating:  13%|█▎        | 52/407 [00:00<00:03, 98.91it/s]Evaluating:  15%|█▌        | 62/407 [00:00<00:03, 98.98it/s]Evaluating:  18%|█▊        | 72/407 [00:00<00:03, 98.81it/s]Evaluating:  20%|██        | 82/407 [00:00<00:03, 98.82it/s]Evaluating:  23%|██▎       | 92/407 [00:00<00:03, 98.70it/s]Evaluating:  25%|██▌       | 102/407 [00:01<00:03, 98.20it/s]Evaluating:  28%|██▊       | 112/407 [00:01<00:03, 97.46it/s]Evaluating:  30%|██▉       | 122/407 [00:01<00:02, 96.56it/s]Evaluating:  32%|███▏      | 132/407 [00:01<00:02, 96.08it/s]Evaluating:  35%|███▍      | 142/407 [00:01<00:02, 95.82it/s]Evaluating:  37%|███▋      | 152/407 [00:01<00:02, 94.93it/s]Evaluating:  40%|███▉      | 162/407 [00:01<00:02, 93.63it/s]Evaluating:  42%|████▏     | 172/407 [00:01<00:02, 93.37it/s]Evaluating:  45%|████▍     | 182/407 [00:01<00:02, 92.91it/s]Evaluating:  47%|████▋     | 192/407 [00:02<00:02, 92.68it/s]Evaluating:  50%|████▉     | 202/407 [00:02<00:02, 92.08it/s]Evaluating:  52%|█████▏    | 212/407 [00:02<00:02, 90.92it/s]Evaluating:  55%|█████▍    | 222/407 [00:02<00:02, 90.13it/s]Evaluating:  57%|█████▋    | 232/407 [00:02<00:01, 89.17it/s]Evaluating:  59%|█████▉    | 241/407 [00:02<00:01, 88.28it/s]Evaluating:  61%|██████▏   | 250/407 [00:02<00:01, 87.29it/s]Evaluating:  64%|██████▎   | 259/407 [00:02<00:01, 85.61it/s]Evaluating:  66%|██████▌   | 268/407 [00:02<00:01, 85.31it/s]Evaluating:  68%|██████▊   | 277/407 [00:02<00:01, 84.77it/s]Evaluating:  70%|███████   | 286/407 [00:03<00:01, 84.22it/s]Evaluating:  72%|███████▏  | 295/407 [00:03<00:01, 83.76it/s]Evaluating:  75%|███████▍  | 304/407 [00:03<00:01, 83.14it/s]Evaluating:  77%|███████▋  | 313/407 [00:03<00:01, 82.82it/s]Evaluating:  79%|███████▉  | 322/407 [00:03<00:01, 80.89it/s]Evaluating:  81%|████████▏ | 331/407 [00:03<00:00, 80.24it/s]Evaluating:  84%|████████▎ | 340/407 [00:03<00:00, 79.53it/s]Evaluating:  86%|████████▌ | 348/407 [00:03<00:00, 79.09it/s]Evaluating:  87%|████████▋ | 356/407 [00:03<00:00, 78.49it/s]Evaluating:  89%|████████▉ | 364/407 [00:04<00:00, 78.13it/s]Evaluating:  91%|█████████▏| 372/407 [00:04<00:00, 77.59it/s]Evaluating:  93%|█████████▎| 380/407 [00:04<00:00, 77.27it/s]Evaluating:  95%|█████████▌| 388/407 [00:04<00:00, 77.06it/s]Evaluating:  97%|█████████▋| 396/407 [00:04<00:00, 76.61it/s]Evaluating:  99%|█████████▉| 404/407 [00:04<00:00, 76.14it/s]Evaluating: 100%|██████████| 407/407 [00:04<00:00, 87.58it/s]
05/05/2022 21:25:37 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 21:25:37 - INFO - __main__ -     f1 = 0.9362843729040912
05/05/2022 21:25:37 - INFO - __main__ -     loss = 0.055040152399415
05/05/2022 21:25:37 - INFO - __main__ -     precision = 0.931909212283044
05/05/2022 21:25:37 - INFO - __main__ -     recall = 0.9407008086253369
05/05/2022 21:25:37 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/student_raw_layer5' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/student_raw_layer5' is a path or url to a directory containing tokenizer files.
05/05/2022 21:25:37 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/student_raw_layer5/vocab.txt
05/05/2022 21:25:37 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/student_raw_layer5/added_tokens.json
05/05/2022 21:25:37 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/student_raw_layer5/special_tokens_map.json
05/05/2022 21:25:37 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/student_raw_layer5/tokenizer_config.json
05/05/2022 21:25:37 - INFO - transformers.configuration_utils -   loading configuration file /home/hs3228/TextBrewer/output_model/conll_distill/student_raw_layer5/config.json
05/05/2022 21:25:37 - INFO - transformers.configuration_utils -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "num_labels": 9,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

05/05/2022 21:25:37 - INFO - transformers.modeling_utils -   loading weights file /home/hs3228/TextBrewer/output_model/conll_distill/student_raw_layer5/pytorch_model.bin
05/05/2022 21:25:38 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_raw_128
05/05/2022 21:25:39 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 21:25:39 - INFO - __main__ -     Num examples = 3453
05/05/2022 21:25:39 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 103.12it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:03, 103.07it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 101.88it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 101.26it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 100.76it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 100.21it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 100.25it/s]Evaluating:  20%|██        | 88/432 [00:00<00:03, 99.63it/s] Evaluating:  23%|██▎       | 98/432 [00:00<00:03, 98.65it/s]Evaluating:  25%|██▌       | 108/432 [00:01<00:03, 97.92it/s]Evaluating:  27%|██▋       | 118/432 [00:01<00:03, 97.51it/s]Evaluating:  30%|██▉       | 128/432 [00:01<00:03, 97.20it/s]Evaluating:  32%|███▏      | 138/432 [00:01<00:03, 96.66it/s]Evaluating:  34%|███▍      | 148/432 [00:01<00:02, 95.53it/s]Evaluating:  37%|███▋      | 158/432 [00:01<00:02, 94.26it/s]Evaluating:  39%|███▉      | 168/432 [00:01<00:02, 92.89it/s]Evaluating:  41%|████      | 178/432 [00:01<00:02, 91.37it/s]Evaluating:  44%|████▎     | 188/432 [00:01<00:02, 90.35it/s]Evaluating:  46%|████▌     | 198/432 [00:02<00:02, 89.40it/s]Evaluating:  48%|████▊     | 207/432 [00:02<00:02, 88.71it/s]Evaluating:  50%|█████     | 216/432 [00:02<00:02, 88.08it/s]Evaluating:  52%|█████▏    | 225/432 [00:02<00:02, 87.35it/s]Evaluating:  54%|█████▍    | 234/432 [00:02<00:02, 86.85it/s]Evaluating:  56%|█████▋    | 243/432 [00:02<00:02, 86.37it/s]Evaluating:  58%|█████▊    | 252/432 [00:02<00:02, 85.70it/s]Evaluating:  60%|██████    | 261/432 [00:02<00:02, 85.32it/s]Evaluating:  62%|██████▎   | 270/432 [00:02<00:01, 84.99it/s]Evaluating:  65%|██████▍   | 279/432 [00:03<00:01, 84.56it/s]Evaluating:  67%|██████▋   | 288/432 [00:03<00:01, 84.05it/s]Evaluating:  69%|██████▉   | 297/432 [00:03<00:01, 83.55it/s]Evaluating:  71%|███████   | 306/432 [00:03<00:01, 83.23it/s]Evaluating:  73%|███████▎  | 315/432 [00:03<00:01, 83.19it/s]Evaluating:  75%|███████▌  | 324/432 [00:03<00:01, 82.63it/s]Evaluating:  77%|███████▋  | 333/432 [00:03<00:01, 81.68it/s]Evaluating:  79%|███████▉  | 342/432 [00:03<00:01, 80.93it/s]Evaluating:  81%|████████▏ | 351/432 [00:03<00:01, 80.17it/s]Evaluating:  83%|████████▎ | 360/432 [00:04<00:00, 79.41it/s]Evaluating:  85%|████████▌ | 368/432 [00:04<00:00, 78.72it/s]Evaluating:  87%|████████▋ | 376/432 [00:04<00:00, 78.26it/s]Evaluating:  89%|████████▉ | 384/432 [00:04<00:00, 77.81it/s]Evaluating:  91%|█████████ | 392/432 [00:04<00:00, 77.63it/s]Evaluating:  93%|█████████▎| 400/432 [00:04<00:00, 77.34it/s]Evaluating:  94%|█████████▍| 408/432 [00:04<00:00, 76.81it/s]Evaluating:  96%|█████████▋| 416/432 [00:04<00:00, 75.89it/s]Evaluating:  98%|█████████▊| 424/432 [00:04<00:00, 75.79it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 76.27it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 87.04it/s]
05/05/2022 21:25:44 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 21:25:44 - INFO - __main__ -     f1 = 0.9022265246853824
05/05/2022 21:25:44 - INFO - __main__ -     loss = 0.1406874964825925
05/05/2022 21:25:44 - INFO - __main__ -     precision = 0.8963105438013639
05/05/2022 21:25:44 - INFO - __main__ -     recall = 0.9082211197732105
05/05/2022 21:25:44 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for '.'.
05/05/2022 21:25:44 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for '2'.
05/05/2022 21:25:44 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for '-'.
05/05/2022 21:25:44 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for 'Norm'.
05/05/2022 21:25:44 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for 'Hewitt'.
05/05/2022 21:25:44 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for '('.
05/05/2022 21:25:44 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for 'New'.
05/05/2022 21:25:44 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for 'Zealand'.
05/05/2022 21:25:44 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for ')'.
05/05/2022 21:25:44 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for ','.
05/05/2022 21:25:44 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for '1'.
05/05/2022 21:25:44 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for '-'.
05/05/2022 21:25:44 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for 'Nick'.
05/05/2022 21:25:44 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for 'Popplewell'.
05/05/2022 21:25:44 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for '('.
05/05/2022 21:25:44 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for 'Ireland'.
05/05/2022 21:25:44 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for ')'.
05/05/2022 21:25:44 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for '.'.
