nohup: ignoring input
05/05/2022 19:45:47 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
05/05/2022 19:45:47 - INFO - transformers.configuration_utils -   loading configuration file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/config.json
05/05/2022 19:45:47 - INFO - transformers.configuration_utils -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 9,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

05/05/2022 19:45:47 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 19:45:47 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 19:45:47 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 19:45:47 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 19:45:47 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 19:45:47 - INFO - transformers.modeling_utils -   loading weights file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/pytorch_model.bin
05/05/2022 19:45:49 - INFO - transformers.configuration_utils -   loading configuration file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/config.json
05/05/2022 19:45:49 - INFO - transformers.configuration_utils -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 9,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

05/05/2022 19:45:49 - INFO - transformers.modeling_utils -   loading weights file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/pytorch_model.bin
05/05/2022 19:45:51 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForTokenClassification: ['bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']
05/05/2022 19:45:54 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/home/hs3228/TextBrewer/data/conll', device=device(type='cuda'), do_distill=True, do_eval=True, do_lower_case=False, do_predict=True, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, labels='', learning_rate=0.0001, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='/home/hs3228/TextBrewer/output_model/conll_distill/teacher', model_name_or_path_student=None, model_type='bert', n_gpu=1, no_cuda=False, num_hidden_layers=5, num_train_epochs=20.0, output_dir='/home/hs3228/TextBrewer/output_model/conll_distill/student_layer5', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=32, save_steps=750, seed=42, server_ip='', server_port='', tokenizer_name='', warmup_steps=0.1, weight_decay=0.0)
05/05/2022 19:45:54 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_train_teacher_128
05/05/2022 19:45:56 - INFO - __main__ -   ***** Running training *****
05/05/2022 19:45:56 - INFO - __main__ -     Num examples = 14041
05/05/2022 19:45:56 - INFO - __main__ -     Num Epochs = 20
05/05/2022 19:45:56 - INFO - __main__ -     Instantaneous batch size per GPU = 32
05/05/2022 19:45:56 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32
05/05/2022 19:45:56 - INFO - __main__ -     Gradient Accumulation steps = 1
05/05/2022 19:45:56 - INFO - __main__ -     Total optimization steps = 8780
05/05/2022 19:45:56 - INFO - Distillation -   Training steps per epoch: 439
05/05/2022 19:45:56 - INFO - Distillation -   Checkpoints(step): [0]
05/05/2022 19:45:56 - INFO - Distillation -   Epoch 1
05/05/2022 19:45:56 - INFO - Distillation -   Length of current epoch in forward batch: 439
/home/hs3228/miniconda3/envs/prac/lib/python3.6/site-packages/transformers/optimization.py:146: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
05/05/2022 19:46:00 - INFO - Distillation -   Global step: 21, epoch step:21
05/05/2022 19:46:03 - INFO - Distillation -   Global step: 42, epoch step:42
05/05/2022 19:46:07 - INFO - Distillation -   Global step: 63, epoch step:63
05/05/2022 19:46:11 - INFO - Distillation -   Global step: 84, epoch step:84
05/05/2022 19:46:15 - INFO - Distillation -   Global step: 105, epoch step:105
05/05/2022 19:46:18 - INFO - Distillation -   Global step: 126, epoch step:126
05/05/2022 19:46:22 - INFO - Distillation -   Global step: 147, epoch step:147
05/05/2022 19:46:26 - INFO - Distillation -   Global step: 168, epoch step:168
05/05/2022 19:46:30 - INFO - Distillation -   Global step: 189, epoch step:189
05/05/2022 19:46:33 - INFO - Distillation -   Global step: 210, epoch step:210
05/05/2022 19:46:37 - INFO - Distillation -   Global step: 231, epoch step:231
05/05/2022 19:46:41 - INFO - Distillation -   Global step: 252, epoch step:252
05/05/2022 19:46:45 - INFO - Distillation -   Global step: 273, epoch step:273
05/05/2022 19:46:49 - INFO - Distillation -   Global step: 294, epoch step:294
05/05/2022 19:46:52 - INFO - Distillation -   Global step: 315, epoch step:315
05/05/2022 19:46:56 - INFO - Distillation -   Global step: 336, epoch step:336
05/05/2022 19:47:00 - INFO - Distillation -   Global step: 357, epoch step:357
05/05/2022 19:47:04 - INFO - Distillation -   Global step: 378, epoch step:378
05/05/2022 19:47:07 - INFO - Distillation -   Global step: 399, epoch step:399
05/05/2022 19:47:11 - INFO - Distillation -   Global step: 420, epoch step:420
05/05/2022 19:47:15 - INFO - Distillation -   Saving at global step 439, epoch step 439 epoch 1
05/05/2022 19:47:16 - INFO - Distillation -   Running callback function...
05/05/2022 19:47:16 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 19:47:16 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 19:47:16 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 19:47:16 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 19:47:16 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 19:47:16 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 19:47:17 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 19:47:17 - INFO - __main__ -     Num examples = 3453
05/05/2022 19:47:17 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 101.16it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:04, 101.42it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 101.44it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 100.46it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 99.66it/s] Evaluating:  15%|█▌        | 65/432 [00:00<00:03, 99.56it/s]Evaluating:  17%|█▋        | 75/432 [00:00<00:03, 99.21it/s]Evaluating:  20%|█▉        | 85/432 [00:00<00:03, 98.72it/s]Evaluating:  22%|██▏       | 95/432 [00:00<00:03, 98.45it/s]Evaluating:  24%|██▍       | 105/432 [00:01<00:03, 97.58it/s]Evaluating:  27%|██▋       | 115/432 [00:01<00:03, 97.03it/s]Evaluating:  29%|██▉       | 125/432 [00:01<00:03, 95.90it/s]Evaluating:  31%|███▏      | 135/432 [00:01<00:03, 95.01it/s]Evaluating:  34%|███▎      | 145/432 [00:01<00:03, 94.21it/s]Evaluating:  36%|███▌      | 155/432 [00:01<00:02, 93.30it/s]Evaluating:  38%|███▊      | 165/432 [00:01<00:02, 92.68it/s]Evaluating:  41%|████      | 175/432 [00:01<00:02, 92.25it/s]Evaluating:  43%|████▎     | 185/432 [00:01<00:02, 91.80it/s]Evaluating:  45%|████▌     | 195/432 [00:02<00:02, 91.28it/s]Evaluating:  47%|████▋     | 205/432 [00:02<00:02, 90.84it/s]Evaluating:  50%|████▉     | 215/432 [00:02<00:02, 89.78it/s]Evaluating:  52%|█████▏    | 224/432 [00:02<00:02, 88.89it/s]Evaluating:  54%|█████▍    | 233/432 [00:02<00:02, 87.75it/s]Evaluating:  56%|█████▌    | 242/432 [00:02<00:02, 86.91it/s]Evaluating:  58%|█████▊    | 251/432 [00:02<00:02, 86.17it/s]Evaluating:  60%|██████    | 260/432 [00:02<00:02, 85.29it/s]Evaluating:  62%|██████▏   | 269/432 [00:02<00:01, 84.50it/s]Evaluating:  64%|██████▍   | 278/432 [00:03<00:01, 83.79it/s]Evaluating:  66%|██████▋   | 287/432 [00:03<00:01, 82.99it/s]Evaluating:  69%|██████▊   | 296/432 [00:03<00:01, 82.30it/s]Evaluating:  71%|███████   | 305/432 [00:03<00:01, 81.29it/s]Evaluating:  73%|███████▎  | 314/432 [00:03<00:01, 80.39it/s]Evaluating:  75%|███████▍  | 323/432 [00:03<00:01, 79.51it/s]Evaluating:  77%|███████▋  | 331/432 [00:03<00:01, 79.29it/s]Evaluating:  78%|███████▊  | 339/432 [00:03<00:01, 79.08it/s]Evaluating:  80%|████████  | 347/432 [00:03<00:01, 77.80it/s]Evaluating:  82%|████████▏ | 355/432 [00:03<00:00, 77.46it/s]Evaluating:  84%|████████▍ | 363/432 [00:04<00:00, 76.96it/s]Evaluating:  86%|████████▌ | 371/432 [00:04<00:00, 76.61it/s]Evaluating:  88%|████████▊ | 379/432 [00:04<00:00, 76.15it/s]Evaluating:  90%|████████▉ | 387/432 [00:04<00:00, 75.61it/s]Evaluating:  91%|█████████▏| 395/432 [00:04<00:00, 74.94it/s]Evaluating:  93%|█████████▎| 403/432 [00:04<00:00, 74.32it/s]Evaluating:  95%|█████████▌| 411/432 [00:04<00:00, 73.89it/s]Evaluating:  97%|█████████▋| 419/432 [00:04<00:00, 73.23it/s]Evaluating:  99%|█████████▉| 427/432 [00:04<00:00, 72.64it/s]Evaluating: 100%|██████████| 432/432 [00:05<00:00, 85.74it/s]
05/05/2022 19:47:22 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 19:47:22 - INFO - __main__ -     f1 = 0.8776876982728233
05/05/2022 19:47:22 - INFO - __main__ -     loss = 0.20583149467491543
05/05/2022 19:47:22 - INFO - __main__ -     precision = 0.8730715287517532
05/05/2022 19:47:22 - INFO - __main__ -     recall = 0.8823529411764706
05/05/2022 19:47:22 - INFO - Distillation -   Epoch 1 finished
05/05/2022 19:47:22 - INFO - Distillation -   Epoch 2
05/05/2022 19:47:22 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 19:47:23 - INFO - Distillation -   Global step: 441, epoch step:2
05/05/2022 19:47:27 - INFO - Distillation -   Global step: 462, epoch step:23
05/05/2022 19:47:30 - INFO - Distillation -   Global step: 483, epoch step:44
05/05/2022 19:47:34 - INFO - Distillation -   Global step: 504, epoch step:65
05/05/2022 19:47:38 - INFO - Distillation -   Global step: 525, epoch step:86
05/05/2022 19:47:42 - INFO - Distillation -   Global step: 546, epoch step:107
05/05/2022 19:47:46 - INFO - Distillation -   Global step: 567, epoch step:128
05/05/2022 19:47:49 - INFO - Distillation -   Global step: 588, epoch step:149
05/05/2022 19:47:53 - INFO - Distillation -   Global step: 609, epoch step:170
05/05/2022 19:47:57 - INFO - Distillation -   Global step: 630, epoch step:191
05/05/2022 19:48:01 - INFO - Distillation -   Global step: 651, epoch step:212
05/05/2022 19:48:05 - INFO - Distillation -   Global step: 672, epoch step:233
05/05/2022 19:48:08 - INFO - Distillation -   Global step: 693, epoch step:254
05/05/2022 19:48:12 - INFO - Distillation -   Global step: 714, epoch step:275
05/05/2022 19:48:16 - INFO - Distillation -   Global step: 735, epoch step:296
05/05/2022 19:48:20 - INFO - Distillation -   Global step: 756, epoch step:317
05/05/2022 19:48:24 - INFO - Distillation -   Global step: 777, epoch step:338
05/05/2022 19:48:27 - INFO - Distillation -   Global step: 798, epoch step:359
05/05/2022 19:48:31 - INFO - Distillation -   Global step: 819, epoch step:380
05/05/2022 19:48:35 - INFO - Distillation -   Global step: 840, epoch step:401
05/05/2022 19:48:39 - INFO - Distillation -   Global step: 861, epoch step:422
05/05/2022 19:48:42 - INFO - Distillation -   Saving at global step 878, epoch step 439 epoch 2
05/05/2022 19:48:44 - INFO - Distillation -   Running callback function...
05/05/2022 19:48:44 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 19:48:44 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 19:48:44 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 19:48:44 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 19:48:44 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 19:48:44 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 19:48:44 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 19:48:44 - INFO - __main__ -     Num examples = 3453
05/05/2022 19:48:44 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 102.65it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:04, 101.23it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 100.33it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 100.04it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 99.25it/s] Evaluating:  15%|█▌        | 65/432 [00:00<00:03, 99.07it/s]Evaluating:  18%|█▊        | 76/432 [00:00<00:03, 99.46it/s]Evaluating:  20%|█▉        | 86/432 [00:00<00:03, 98.87it/s]Evaluating:  22%|██▏       | 96/432 [00:00<00:03, 98.95it/s]Evaluating:  25%|██▍       | 106/432 [00:01<00:03, 98.65it/s]Evaluating:  27%|██▋       | 116/432 [00:01<00:03, 98.43it/s]Evaluating:  29%|██▉       | 126/432 [00:01<00:03, 97.66it/s]Evaluating:  31%|███▏      | 136/432 [00:01<00:03, 96.92it/s]Evaluating:  34%|███▍      | 146/432 [00:01<00:02, 95.75it/s]Evaluating:  36%|███▌      | 156/432 [00:01<00:02, 94.44it/s]Evaluating:  38%|███▊      | 166/432 [00:01<00:02, 93.41it/s]Evaluating:  41%|████      | 176/432 [00:01<00:02, 91.60it/s]Evaluating:  43%|████▎     | 186/432 [00:01<00:02, 90.12it/s]Evaluating:  45%|████▌     | 196/432 [00:02<00:02, 88.48it/s]Evaluating:  47%|████▋     | 205/432 [00:02<00:02, 87.57it/s]Evaluating:  50%|████▉     | 214/432 [00:02<00:02, 86.76it/s]Evaluating:  52%|█████▏    | 223/432 [00:02<00:02, 85.93it/s]Evaluating:  54%|█████▎    | 232/432 [00:02<00:02, 85.66it/s]Evaluating:  56%|█████▌    | 241/432 [00:02<00:02, 84.88it/s]Evaluating:  58%|█████▊    | 250/432 [00:02<00:02, 84.41it/s]Evaluating:  60%|█████▉    | 259/432 [00:02<00:02, 83.65it/s]Evaluating:  62%|██████▏   | 268/432 [00:02<00:01, 82.85it/s]Evaluating:  64%|██████▍   | 277/432 [00:03<00:01, 82.24it/s]Evaluating:  66%|██████▌   | 286/432 [00:03<00:01, 81.67it/s]Evaluating:  68%|██████▊   | 295/432 [00:03<00:01, 81.04it/s]Evaluating:  70%|███████   | 304/432 [00:03<00:01, 80.50it/s]Evaluating:  72%|███████▏  | 313/432 [00:03<00:01, 79.78it/s]Evaluating:  74%|███████▍  | 321/432 [00:03<00:01, 79.30it/s]Evaluating:  76%|███████▌  | 329/432 [00:03<00:01, 79.09it/s]Evaluating:  78%|███████▊  | 337/432 [00:03<00:01, 78.72it/s]Evaluating:  80%|███████▉  | 345/432 [00:03<00:01, 78.32it/s]Evaluating:  82%|████████▏ | 353/432 [00:03<00:01, 77.94it/s]Evaluating:  84%|████████▎ | 361/432 [00:04<00:00, 77.85it/s]Evaluating:  85%|████████▌ | 369/432 [00:04<00:00, 77.80it/s]Evaluating:  87%|████████▋ | 377/432 [00:04<00:00, 77.77it/s]Evaluating:  89%|████████▉ | 385/432 [00:04<00:00, 77.21it/s]Evaluating:  91%|█████████ | 393/432 [00:04<00:00, 76.37it/s]Evaluating:  93%|█████████▎| 401/432 [00:04<00:00, 75.30it/s]Evaluating:  95%|█████████▍| 409/432 [00:04<00:00, 74.77it/s]Evaluating:  97%|█████████▋| 417/432 [00:04<00:00, 74.21it/s]Evaluating:  98%|█████████▊| 425/432 [00:04<00:00, 73.50it/s]Evaluating: 100%|██████████| 432/432 [00:05<00:00, 85.57it/s]
05/05/2022 19:48:50 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 19:48:50 - INFO - __main__ -     f1 = 0.8787269388825741
05/05/2022 19:48:50 - INFO - __main__ -     loss = 0.2388813455337013
05/05/2022 19:48:50 - INFO - __main__ -     precision = 0.8674262040393579
05/05/2022 19:48:50 - INFO - __main__ -     recall = 0.8903260099220411
05/05/2022 19:48:50 - INFO - Distillation -   Epoch 2 finished
05/05/2022 19:48:50 - INFO - Distillation -   Epoch 3
05/05/2022 19:48:50 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 19:48:50 - INFO - Distillation -   Global step: 882, epoch step:4
05/05/2022 19:48:54 - INFO - Distillation -   Global step: 903, epoch step:25
05/05/2022 19:48:58 - INFO - Distillation -   Global step: 924, epoch step:46
05/05/2022 19:49:02 - INFO - Distillation -   Global step: 945, epoch step:67
05/05/2022 19:49:06 - INFO - Distillation -   Global step: 966, epoch step:88
05/05/2022 19:49:09 - INFO - Distillation -   Global step: 987, epoch step:109
05/05/2022 19:49:13 - INFO - Distillation -   Global step: 1008, epoch step:130
05/05/2022 19:49:17 - INFO - Distillation -   Global step: 1029, epoch step:151
05/05/2022 19:49:21 - INFO - Distillation -   Global step: 1050, epoch step:172
05/05/2022 19:49:25 - INFO - Distillation -   Global step: 1071, epoch step:193
05/05/2022 19:49:28 - INFO - Distillation -   Global step: 1092, epoch step:214
05/05/2022 19:49:32 - INFO - Distillation -   Global step: 1113, epoch step:235
05/05/2022 19:49:36 - INFO - Distillation -   Global step: 1134, epoch step:256
05/05/2022 19:49:40 - INFO - Distillation -   Global step: 1155, epoch step:277
05/05/2022 19:49:44 - INFO - Distillation -   Global step: 1176, epoch step:298
05/05/2022 19:49:47 - INFO - Distillation -   Global step: 1197, epoch step:319
05/05/2022 19:49:51 - INFO - Distillation -   Global step: 1218, epoch step:340
05/05/2022 19:49:55 - INFO - Distillation -   Global step: 1239, epoch step:361
05/05/2022 19:49:59 - INFO - Distillation -   Global step: 1260, epoch step:382
05/05/2022 19:50:03 - INFO - Distillation -   Global step: 1281, epoch step:403
05/05/2022 19:50:06 - INFO - Distillation -   Global step: 1302, epoch step:424
05/05/2022 19:50:09 - INFO - Distillation -   Saving at global step 1317, epoch step 439 epoch 3
05/05/2022 19:50:11 - INFO - Distillation -   Running callback function...
05/05/2022 19:50:11 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 19:50:11 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 19:50:11 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 19:50:11 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 19:50:11 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 19:50:11 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 19:50:11 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 19:50:11 - INFO - __main__ -     Num examples = 3453
05/05/2022 19:50:11 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 102.80it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:03, 102.54it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 102.11it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 101.25it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 100.87it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 100.29it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 100.10it/s]Evaluating:  20%|██        | 88/432 [00:00<00:03, 99.99it/s] Evaluating:  23%|██▎       | 98/432 [00:00<00:03, 99.49it/s]Evaluating:  25%|██▌       | 108/432 [00:01<00:03, 98.63it/s]Evaluating:  27%|██▋       | 118/432 [00:01<00:03, 97.89it/s]Evaluating:  30%|██▉       | 128/432 [00:01<00:03, 96.45it/s]Evaluating:  32%|███▏      | 138/432 [00:01<00:03, 95.64it/s]Evaluating:  34%|███▍      | 148/432 [00:01<00:02, 94.97it/s]Evaluating:  37%|███▋      | 158/432 [00:01<00:02, 94.06it/s]Evaluating:  39%|███▉      | 168/432 [00:01<00:02, 93.13it/s]Evaluating:  41%|████      | 178/432 [00:01<00:02, 92.74it/s]Evaluating:  44%|████▎     | 188/432 [00:01<00:02, 92.06it/s]Evaluating:  46%|████▌     | 198/432 [00:02<00:02, 91.58it/s]Evaluating:  48%|████▊     | 208/432 [00:02<00:02, 90.76it/s]Evaluating:  50%|█████     | 218/432 [00:02<00:02, 89.25it/s]Evaluating:  53%|█████▎    | 227/432 [00:02<00:02, 88.25it/s]Evaluating:  55%|█████▍    | 236/432 [00:02<00:02, 87.46it/s]Evaluating:  57%|█████▋    | 245/432 [00:02<00:02, 86.41it/s]Evaluating:  59%|█████▉    | 254/432 [00:02<00:02, 85.69it/s]Evaluating:  61%|██████    | 263/432 [00:02<00:01, 85.14it/s]Evaluating:  63%|██████▎   | 272/432 [00:02<00:01, 84.43it/s]Evaluating:  65%|██████▌   | 281/432 [00:03<00:01, 83.56it/s]Evaluating:  67%|██████▋   | 290/432 [00:03<00:01, 82.64it/s]Evaluating:  69%|██████▉   | 299/432 [00:03<00:01, 82.27it/s]Evaluating:  71%|███████▏  | 308/432 [00:03<00:01, 81.60it/s]Evaluating:  73%|███████▎  | 317/432 [00:03<00:01, 81.33it/s]Evaluating:  75%|███████▌  | 326/432 [00:03<00:01, 81.08it/s]Evaluating:  78%|███████▊  | 335/432 [00:03<00:01, 80.55it/s]Evaluating:  80%|███████▉  | 344/432 [00:03<00:01, 80.09it/s]Evaluating:  82%|████████▏ | 353/432 [00:03<00:00, 79.55it/s]Evaluating:  84%|████████▎ | 361/432 [00:04<00:00, 79.09it/s]Evaluating:  85%|████████▌ | 369/432 [00:04<00:00, 78.63it/s]Evaluating:  87%|████████▋ | 377/432 [00:04<00:00, 78.24it/s]Evaluating:  89%|████████▉ | 385/432 [00:04<00:00, 77.52it/s]Evaluating:  91%|█████████ | 393/432 [00:04<00:00, 77.08it/s]Evaluating:  93%|█████████▎| 401/432 [00:04<00:00, 76.45it/s]Evaluating:  95%|█████████▍| 409/432 [00:04<00:00, 76.03it/s]Evaluating:  97%|█████████▋| 417/432 [00:04<00:00, 75.81it/s]Evaluating:  98%|█████████▊| 425/432 [00:04<00:00, 75.41it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 86.91it/s]
05/05/2022 19:50:17 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 19:50:17 - INFO - __main__ -     f1 = 0.8832398316970548
05/05/2022 19:50:17 - INFO - __main__ -     loss = 0.23646425402618146
05/05/2022 19:50:17 - INFO - __main__ -     precision = 0.8740458015267175
05/05/2022 19:50:17 - INFO - __main__ -     recall = 0.8926293408929837
05/05/2022 19:50:17 - INFO - Distillation -   Epoch 3 finished
05/05/2022 19:50:17 - INFO - Distillation -   Epoch 4
05/05/2022 19:50:17 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 19:50:18 - INFO - Distillation -   Global step: 1323, epoch step:6
05/05/2022 19:50:22 - INFO - Distillation -   Global step: 1344, epoch step:27
05/05/2022 19:50:26 - INFO - Distillation -   Global step: 1365, epoch step:48
05/05/2022 19:50:29 - INFO - Distillation -   Global step: 1386, epoch step:69
05/05/2022 19:50:33 - INFO - Distillation -   Global step: 1407, epoch step:90
05/05/2022 19:50:37 - INFO - Distillation -   Global step: 1428, epoch step:111
05/05/2022 19:50:41 - INFO - Distillation -   Global step: 1449, epoch step:132
05/05/2022 19:50:45 - INFO - Distillation -   Global step: 1470, epoch step:153
05/05/2022 19:50:48 - INFO - Distillation -   Global step: 1491, epoch step:174
05/05/2022 19:50:52 - INFO - Distillation -   Global step: 1512, epoch step:195
05/05/2022 19:50:56 - INFO - Distillation -   Global step: 1533, epoch step:216
05/05/2022 19:51:00 - INFO - Distillation -   Global step: 1554, epoch step:237
05/05/2022 19:51:04 - INFO - Distillation -   Global step: 1575, epoch step:258
05/05/2022 19:51:07 - INFO - Distillation -   Global step: 1596, epoch step:279
05/05/2022 19:51:11 - INFO - Distillation -   Global step: 1617, epoch step:300
05/05/2022 19:51:15 - INFO - Distillation -   Global step: 1638, epoch step:321
05/05/2022 19:51:19 - INFO - Distillation -   Global step: 1659, epoch step:342
05/05/2022 19:51:23 - INFO - Distillation -   Global step: 1680, epoch step:363
05/05/2022 19:51:26 - INFO - Distillation -   Global step: 1701, epoch step:384
05/05/2022 19:51:30 - INFO - Distillation -   Global step: 1722, epoch step:405
05/05/2022 19:51:34 - INFO - Distillation -   Global step: 1743, epoch step:426
05/05/2022 19:51:36 - INFO - Distillation -   Saving at global step 1756, epoch step 439 epoch 4
05/05/2022 19:51:38 - INFO - Distillation -   Running callback function...
05/05/2022 19:51:38 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 19:51:38 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 19:51:38 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 19:51:38 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 19:51:38 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 19:51:38 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 19:51:38 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 19:51:38 - INFO - __main__ -     Num examples = 3453
05/05/2022 19:51:38 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 103.36it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:03, 103.17it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 102.52it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 101.45it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 100.79it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 99.83it/s] Evaluating:  18%|█▊        | 76/432 [00:00<00:03, 99.09it/s]Evaluating:  20%|█▉        | 86/432 [00:00<00:03, 99.20it/s]Evaluating:  22%|██▏       | 96/432 [00:00<00:03, 98.85it/s]Evaluating:  25%|██▍       | 106/432 [00:01<00:03, 98.04it/s]Evaluating:  27%|██▋       | 116/432 [00:01<00:03, 97.77it/s]Evaluating:  29%|██▉       | 126/432 [00:01<00:03, 97.39it/s]Evaluating:  31%|███▏      | 136/432 [00:01<00:03, 96.69it/s]Evaluating:  34%|███▍      | 146/432 [00:01<00:02, 96.09it/s]Evaluating:  36%|███▌      | 156/432 [00:01<00:02, 95.06it/s]Evaluating:  38%|███▊      | 166/432 [00:01<00:02, 93.77it/s]Evaluating:  41%|████      | 176/432 [00:01<00:02, 92.51it/s]Evaluating:  43%|████▎     | 186/432 [00:01<00:02, 91.16it/s]Evaluating:  45%|████▌     | 196/432 [00:02<00:02, 90.22it/s]Evaluating:  48%|████▊     | 206/432 [00:02<00:02, 89.52it/s]Evaluating:  50%|████▉     | 215/432 [00:02<00:02, 88.52it/s]Evaluating:  52%|█████▏    | 224/432 [00:02<00:02, 87.89it/s]Evaluating:  54%|█████▍    | 233/432 [00:02<00:02, 87.03it/s]Evaluating:  56%|█████▌    | 242/432 [00:02<00:02, 86.27it/s]Evaluating:  58%|█████▊    | 251/432 [00:02<00:02, 85.59it/s]Evaluating:  60%|██████    | 260/432 [00:02<00:02, 84.96it/s]Evaluating:  62%|██████▏   | 269/432 [00:02<00:01, 84.62it/s]Evaluating:  64%|██████▍   | 278/432 [00:03<00:01, 84.05it/s]Evaluating:  66%|██████▋   | 287/432 [00:03<00:01, 83.49it/s]Evaluating:  69%|██████▊   | 296/432 [00:03<00:01, 82.92it/s]Evaluating:  71%|███████   | 305/432 [00:03<00:01, 82.38it/s]Evaluating:  73%|███████▎  | 314/432 [00:03<00:01, 81.75it/s]Evaluating:  75%|███████▍  | 323/432 [00:03<00:01, 81.36it/s]Evaluating:  77%|███████▋  | 332/432 [00:03<00:01, 80.72it/s]Evaluating:  79%|███████▉  | 341/432 [00:03<00:01, 79.97it/s]Evaluating:  81%|████████  | 350/432 [00:03<00:01, 79.46it/s]Evaluating:  83%|████████▎ | 358/432 [00:04<00:00, 78.92it/s]Evaluating:  85%|████████▍ | 366/432 [00:04<00:00, 78.55it/s]Evaluating:  87%|████████▋ | 374/432 [00:04<00:00, 78.39it/s]Evaluating:  88%|████████▊ | 382/432 [00:04<00:00, 78.12it/s]Evaluating:  90%|█████████ | 390/432 [00:04<00:00, 77.91it/s]Evaluating:  92%|█████████▏| 398/432 [00:04<00:00, 77.54it/s]Evaluating:  94%|█████████▍| 406/432 [00:04<00:00, 76.90it/s]Evaluating:  96%|█████████▌| 414/432 [00:04<00:00, 76.10it/s]Evaluating:  98%|█████████▊| 422/432 [00:04<00:00, 75.40it/s]Evaluating: 100%|█████████▉| 430/432 [00:04<00:00, 75.00it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 86.83it/s]
05/05/2022 19:51:44 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 19:51:44 - INFO - __main__ -     f1 = 0.888888888888889
05/05/2022 19:51:44 - INFO - __main__ -     loss = 0.2261737513999833
05/05/2022 19:51:44 - INFO - __main__ -     precision = 0.8827538004543072
05/05/2022 19:51:44 - INFO - __main__ -     recall = 0.8951098511693835
05/05/2022 19:51:44 - INFO - Distillation -   Epoch 4 finished
05/05/2022 19:51:44 - INFO - Distillation -   Epoch 5
05/05/2022 19:51:44 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 19:51:46 - INFO - Distillation -   Global step: 1764, epoch step:8
05/05/2022 19:51:49 - INFO - Distillation -   Global step: 1785, epoch step:29
05/05/2022 19:51:53 - INFO - Distillation -   Global step: 1806, epoch step:50
05/05/2022 19:51:57 - INFO - Distillation -   Global step: 1827, epoch step:71
05/05/2022 19:52:01 - INFO - Distillation -   Global step: 1848, epoch step:92
05/05/2022 19:52:05 - INFO - Distillation -   Global step: 1869, epoch step:113
05/05/2022 19:52:08 - INFO - Distillation -   Global step: 1890, epoch step:134
05/05/2022 19:52:12 - INFO - Distillation -   Global step: 1911, epoch step:155
05/05/2022 19:52:16 - INFO - Distillation -   Global step: 1932, epoch step:176
05/05/2022 19:52:20 - INFO - Distillation -   Global step: 1953, epoch step:197
05/05/2022 19:52:24 - INFO - Distillation -   Global step: 1974, epoch step:218
05/05/2022 19:52:27 - INFO - Distillation -   Global step: 1995, epoch step:239
05/05/2022 19:52:31 - INFO - Distillation -   Global step: 2016, epoch step:260
05/05/2022 19:52:35 - INFO - Distillation -   Global step: 2037, epoch step:281
05/05/2022 19:52:39 - INFO - Distillation -   Global step: 2058, epoch step:302
05/05/2022 19:52:43 - INFO - Distillation -   Global step: 2079, epoch step:323
05/05/2022 19:52:46 - INFO - Distillation -   Global step: 2100, epoch step:344
05/05/2022 19:52:50 - INFO - Distillation -   Global step: 2121, epoch step:365
05/05/2022 19:52:54 - INFO - Distillation -   Global step: 2142, epoch step:386
05/05/2022 19:52:58 - INFO - Distillation -   Global step: 2163, epoch step:407
05/05/2022 19:53:02 - INFO - Distillation -   Global step: 2184, epoch step:428
05/05/2022 19:53:04 - INFO - Distillation -   Saving at global step 2195, epoch step 439 epoch 5
05/05/2022 19:53:05 - INFO - Distillation -   Running callback function...
05/05/2022 19:53:05 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 19:53:05 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 19:53:05 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 19:53:05 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 19:53:05 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 19:53:05 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 19:53:06 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 19:53:06 - INFO - __main__ -     Num examples = 3453
05/05/2022 19:53:06 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 100.89it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:04, 101.33it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 101.87it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 101.60it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 100.87it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 99.95it/s] Evaluating:  18%|█▊        | 76/432 [00:00<00:03, 99.35it/s]Evaluating:  20%|█▉        | 86/432 [00:00<00:03, 98.64it/s]Evaluating:  22%|██▏       | 96/432 [00:00<00:03, 98.01it/s]Evaluating:  25%|██▍       | 106/432 [00:01<00:03, 97.31it/s]Evaluating:  27%|██▋       | 116/432 [00:01<00:03, 96.39it/s]Evaluating:  29%|██▉       | 126/432 [00:01<00:03, 95.27it/s]Evaluating:  31%|███▏      | 136/432 [00:01<00:03, 94.05it/s]Evaluating:  34%|███▍      | 146/432 [00:01<00:03, 92.84it/s]Evaluating:  36%|███▌      | 156/432 [00:01<00:03, 91.89it/s]Evaluating:  38%|███▊      | 166/432 [00:01<00:02, 91.02it/s]Evaluating:  41%|████      | 176/432 [00:01<00:02, 90.13it/s]Evaluating:  43%|████▎     | 186/432 [00:01<00:02, 89.38it/s]Evaluating:  45%|████▌     | 195/432 [00:02<00:02, 88.61it/s]Evaluating:  47%|████▋     | 204/432 [00:02<00:02, 87.91it/s]Evaluating:  49%|████▉     | 213/432 [00:02<00:02, 87.11it/s]Evaluating:  51%|█████▏    | 222/432 [00:02<00:02, 86.64it/s]Evaluating:  53%|█████▎    | 231/432 [00:02<00:02, 86.03it/s]Evaluating:  56%|█████▌    | 240/432 [00:02<00:02, 85.44it/s]Evaluating:  58%|█████▊    | 249/432 [00:02<00:02, 84.60it/s]Evaluating:  60%|█████▉    | 258/432 [00:02<00:02, 83.95it/s]Evaluating:  62%|██████▏   | 267/432 [00:02<00:01, 83.17it/s]Evaluating:  64%|██████▍   | 276/432 [00:03<00:01, 82.68it/s]Evaluating:  66%|██████▌   | 285/432 [00:03<00:01, 82.07it/s]Evaluating:  68%|██████▊   | 294/432 [00:03<00:01, 81.44it/s]Evaluating:  70%|███████   | 303/432 [00:03<00:01, 80.90it/s]Evaluating:  72%|███████▏  | 312/432 [00:03<00:01, 80.40it/s]Evaluating:  74%|███████▍  | 321/432 [00:03<00:01, 79.74it/s]Evaluating:  76%|███████▌  | 329/432 [00:03<00:01, 79.21it/s]Evaluating:  78%|███████▊  | 337/432 [00:03<00:01, 78.81it/s]Evaluating:  80%|███████▉  | 345/432 [00:03<00:01, 78.40it/s]Evaluating:  82%|████████▏ | 353/432 [00:04<00:01, 78.14it/s]Evaluating:  84%|████████▎ | 361/432 [00:04<00:00, 77.83it/s]Evaluating:  85%|████████▌ | 369/432 [00:04<00:00, 77.51it/s]Evaluating:  87%|████████▋ | 377/432 [00:04<00:00, 76.82it/s]Evaluating:  89%|████████▉ | 385/432 [00:04<00:00, 76.29it/s]Evaluating:  91%|█████████ | 393/432 [00:04<00:00, 75.60it/s]Evaluating:  93%|█████████▎| 401/432 [00:04<00:00, 75.19it/s]Evaluating:  95%|█████████▍| 409/432 [00:04<00:00, 74.67it/s]Evaluating:  97%|█████████▋| 417/432 [00:04<00:00, 74.25it/s]Evaluating:  98%|█████████▊| 425/432 [00:04<00:00, 73.69it/s]Evaluating: 100%|██████████| 432/432 [00:05<00:00, 85.33it/s]
05/05/2022 19:53:12 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 19:53:12 - INFO - __main__ -     f1 = 0.8955092714649794
05/05/2022 19:53:12 - INFO - __main__ -     loss = 0.22710396573917482
05/05/2022 19:53:12 - INFO - __main__ -     precision = 0.8884045335658239
05/05/2022 19:53:12 - INFO - __main__ -     recall = 0.9027285613040397
05/05/2022 19:53:12 - INFO - Distillation -   Epoch 5 finished
05/05/2022 19:53:12 - INFO - Distillation -   Epoch 6
05/05/2022 19:53:12 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 19:53:13 - INFO - Distillation -   Global step: 2205, epoch step:10
05/05/2022 19:53:17 - INFO - Distillation -   Global step: 2226, epoch step:31
05/05/2022 19:53:21 - INFO - Distillation -   Global step: 2247, epoch step:52
05/05/2022 19:53:25 - INFO - Distillation -   Global step: 2268, epoch step:73
05/05/2022 19:53:29 - INFO - Distillation -   Global step: 2289, epoch step:94
05/05/2022 19:53:32 - INFO - Distillation -   Global step: 2310, epoch step:115
05/05/2022 19:53:36 - INFO - Distillation -   Global step: 2331, epoch step:136
05/05/2022 19:53:40 - INFO - Distillation -   Global step: 2352, epoch step:157
05/05/2022 19:53:44 - INFO - Distillation -   Global step: 2373, epoch step:178
05/05/2022 19:53:48 - INFO - Distillation -   Global step: 2394, epoch step:199
05/05/2022 19:53:51 - INFO - Distillation -   Global step: 2415, epoch step:220
05/05/2022 19:53:55 - INFO - Distillation -   Global step: 2436, epoch step:241
05/05/2022 19:53:59 - INFO - Distillation -   Global step: 2457, epoch step:262
05/05/2022 19:54:03 - INFO - Distillation -   Global step: 2478, epoch step:283
05/05/2022 19:54:07 - INFO - Distillation -   Global step: 2499, epoch step:304
05/05/2022 19:54:10 - INFO - Distillation -   Global step: 2520, epoch step:325
05/05/2022 19:54:14 - INFO - Distillation -   Global step: 2541, epoch step:346
05/05/2022 19:54:18 - INFO - Distillation -   Global step: 2562, epoch step:367
05/05/2022 19:54:22 - INFO - Distillation -   Global step: 2583, epoch step:388
05/05/2022 19:54:26 - INFO - Distillation -   Global step: 2604, epoch step:409
05/05/2022 19:54:30 - INFO - Distillation -   Global step: 2625, epoch step:430
05/05/2022 19:54:31 - INFO - Distillation -   Saving at global step 2634, epoch step 439 epoch 6
05/05/2022 19:54:33 - INFO - Distillation -   Running callback function...
05/05/2022 19:54:33 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 19:54:33 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 19:54:33 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 19:54:33 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 19:54:33 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 19:54:33 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 19:54:33 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 19:54:33 - INFO - __main__ -     Num examples = 3453
05/05/2022 19:54:33 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 103.23it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:03, 103.19it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 103.00it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 102.52it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 101.49it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 100.67it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 100.20it/s]Evaluating:  20%|██        | 88/432 [00:00<00:03, 99.63it/s] Evaluating:  23%|██▎       | 98/432 [00:00<00:03, 99.28it/s]Evaluating:  25%|██▌       | 108/432 [00:01<00:03, 98.77it/s]Evaluating:  27%|██▋       | 118/432 [00:01<00:03, 97.59it/s]Evaluating:  30%|██▉       | 128/432 [00:01<00:03, 96.79it/s]Evaluating:  32%|███▏      | 138/432 [00:01<00:03, 95.67it/s]Evaluating:  34%|███▍      | 148/432 [00:01<00:02, 94.74it/s]Evaluating:  37%|███▋      | 158/432 [00:01<00:02, 93.72it/s]Evaluating:  39%|███▉      | 168/432 [00:01<00:02, 93.27it/s]Evaluating:  41%|████      | 178/432 [00:01<00:02, 92.85it/s]Evaluating:  44%|████▎     | 188/432 [00:01<00:02, 92.14it/s]Evaluating:  46%|████▌     | 198/432 [00:02<00:02, 91.49it/s]Evaluating:  48%|████▊     | 208/432 [00:02<00:02, 90.28it/s]Evaluating:  50%|█████     | 218/432 [00:02<00:02, 88.98it/s]Evaluating:  53%|█████▎    | 227/432 [00:02<00:02, 88.10it/s]Evaluating:  55%|█████▍    | 236/432 [00:02<00:02, 87.14it/s]Evaluating:  57%|█████▋    | 245/432 [00:02<00:02, 86.39it/s]Evaluating:  59%|█████▉    | 254/432 [00:02<00:02, 85.68it/s]Evaluating:  61%|██████    | 263/432 [00:02<00:01, 85.13it/s]Evaluating:  63%|██████▎   | 272/432 [00:02<00:01, 84.64it/s]Evaluating:  65%|██████▌   | 281/432 [00:03<00:01, 83.97it/s]Evaluating:  67%|██████▋   | 290/432 [00:03<00:01, 83.58it/s]Evaluating:  69%|██████▉   | 299/432 [00:03<00:01, 82.72it/s]Evaluating:  71%|███████▏  | 308/432 [00:03<00:01, 82.25it/s]Evaluating:  73%|███████▎  | 317/432 [00:03<00:01, 81.84it/s]Evaluating:  75%|███████▌  | 326/432 [00:03<00:01, 81.53it/s]Evaluating:  78%|███████▊  | 335/432 [00:03<00:01, 81.14it/s]Evaluating:  80%|███████▉  | 344/432 [00:03<00:01, 80.58it/s]Evaluating:  82%|████████▏ | 353/432 [00:03<00:00, 80.12it/s]Evaluating:  84%|████████▍ | 362/432 [00:04<00:00, 79.54it/s]Evaluating:  86%|████████▌ | 370/432 [00:04<00:00, 79.09it/s]Evaluating:  88%|████████▊ | 378/432 [00:04<00:00, 78.39it/s]Evaluating:  89%|████████▉ | 386/432 [00:04<00:00, 77.99it/s]Evaluating:  91%|█████████ | 394/432 [00:04<00:00, 77.21it/s]Evaluating:  93%|█████████▎| 402/432 [00:04<00:00, 76.62it/s]Evaluating:  95%|█████████▍| 410/432 [00:04<00:00, 76.12it/s]Evaluating:  97%|█████████▋| 418/432 [00:04<00:00, 75.70it/s]Evaluating:  99%|█████████▊| 426/432 [00:04<00:00, 75.18it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 87.08it/s]
05/05/2022 19:54:39 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 19:54:39 - INFO - __main__ -     f1 = 0.8929572529782761
05/05/2022 19:54:39 - INFO - __main__ -     loss = 0.24185636670427008
05/05/2022 19:54:39 - INFO - __main__ -     precision = 0.8830561330561331
05/05/2022 19:54:39 - INFO - __main__ -     recall = 0.9030829199149539
05/05/2022 19:54:39 - INFO - Distillation -   Epoch 6 finished
05/05/2022 19:54:39 - INFO - Distillation -   Epoch 7
05/05/2022 19:54:39 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 19:54:41 - INFO - Distillation -   Global step: 2646, epoch step:12
05/05/2022 19:54:45 - INFO - Distillation -   Global step: 2667, epoch step:33
05/05/2022 19:54:49 - INFO - Distillation -   Global step: 2688, epoch step:54
05/05/2022 19:54:53 - INFO - Distillation -   Global step: 2709, epoch step:75
05/05/2022 19:54:56 - INFO - Distillation -   Global step: 2730, epoch step:96
05/05/2022 19:55:00 - INFO - Distillation -   Global step: 2751, epoch step:117
05/05/2022 19:55:04 - INFO - Distillation -   Global step: 2772, epoch step:138
05/05/2022 19:55:08 - INFO - Distillation -   Global step: 2793, epoch step:159
05/05/2022 19:55:12 - INFO - Distillation -   Global step: 2814, epoch step:180
05/05/2022 19:55:15 - INFO - Distillation -   Global step: 2835, epoch step:201
05/05/2022 19:55:19 - INFO - Distillation -   Global step: 2856, epoch step:222
05/05/2022 19:55:23 - INFO - Distillation -   Global step: 2877, epoch step:243
05/05/2022 19:55:27 - INFO - Distillation -   Global step: 2898, epoch step:264
05/05/2022 19:55:31 - INFO - Distillation -   Global step: 2919, epoch step:285
05/05/2022 19:55:34 - INFO - Distillation -   Global step: 2940, epoch step:306
05/05/2022 19:55:38 - INFO - Distillation -   Global step: 2961, epoch step:327
05/05/2022 19:55:42 - INFO - Distillation -   Global step: 2982, epoch step:348
05/05/2022 19:55:46 - INFO - Distillation -   Global step: 3003, epoch step:369
05/05/2022 19:55:50 - INFO - Distillation -   Global step: 3024, epoch step:390
05/05/2022 19:55:53 - INFO - Distillation -   Global step: 3045, epoch step:411
05/05/2022 19:55:57 - INFO - Distillation -   Global step: 3066, epoch step:432
05/05/2022 19:55:58 - INFO - Distillation -   Saving at global step 3073, epoch step 439 epoch 7
05/05/2022 19:56:00 - INFO - Distillation -   Running callback function...
05/05/2022 19:56:00 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 19:56:00 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 19:56:00 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 19:56:00 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 19:56:00 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 19:56:00 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 19:56:01 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 19:56:01 - INFO - __main__ -     Num examples = 3453
05/05/2022 19:56:01 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 103.42it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:03, 103.63it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 102.85it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 101.96it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 101.15it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 100.36it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 99.67it/s] Evaluating:  20%|██        | 87/432 [00:00<00:03, 99.10it/s]Evaluating:  22%|██▏       | 97/432 [00:00<00:03, 98.95it/s]Evaluating:  25%|██▍       | 107/432 [00:01<00:03, 98.43it/s]Evaluating:  27%|██▋       | 117/432 [00:01<00:03, 97.63it/s]Evaluating:  29%|██▉       | 127/432 [00:01<00:03, 96.75it/s]Evaluating:  32%|███▏      | 137/432 [00:01<00:03, 95.74it/s]Evaluating:  34%|███▍      | 147/432 [00:01<00:03, 94.91it/s]Evaluating:  36%|███▋      | 157/432 [00:01<00:02, 93.97it/s]Evaluating:  39%|███▊      | 167/432 [00:01<00:02, 93.38it/s]Evaluating:  41%|████      | 177/432 [00:01<00:02, 92.89it/s]Evaluating:  43%|████▎     | 187/432 [00:01<00:02, 92.03it/s]Evaluating:  46%|████▌     | 197/432 [00:02<00:02, 91.30it/s]Evaluating:  48%|████▊     | 207/432 [00:02<00:02, 90.16it/s]Evaluating:  50%|█████     | 217/432 [00:02<00:02, 88.96it/s]Evaluating:  52%|█████▏    | 226/432 [00:02<00:02, 88.11it/s]Evaluating:  54%|█████▍    | 235/432 [00:02<00:02, 87.21it/s]Evaluating:  56%|█████▋    | 244/432 [00:02<00:02, 86.42it/s]Evaluating:  59%|█████▊    | 253/432 [00:02<00:02, 85.82it/s]Evaluating:  61%|██████    | 262/432 [00:02<00:01, 85.00it/s]Evaluating:  63%|██████▎   | 271/432 [00:02<00:01, 84.21it/s]Evaluating:  65%|██████▍   | 280/432 [00:03<00:01, 83.71it/s]Evaluating:  67%|██████▋   | 289/432 [00:03<00:01, 83.21it/s]Evaluating:  69%|██████▉   | 298/432 [00:03<00:01, 82.78it/s]Evaluating:  71%|███████   | 307/432 [00:03<00:01, 82.35it/s]Evaluating:  73%|███████▎  | 316/432 [00:03<00:01, 81.90it/s]Evaluating:  75%|███████▌  | 325/432 [00:03<00:01, 81.59it/s]Evaluating:  77%|███████▋  | 334/432 [00:03<00:01, 80.94it/s]Evaluating:  79%|███████▉  | 343/432 [00:03<00:01, 80.27it/s]Evaluating:  81%|████████▏ | 352/432 [00:03<00:01, 79.68it/s]Evaluating:  83%|████████▎ | 360/432 [00:04<00:00, 79.35it/s]Evaluating:  85%|████████▌ | 368/432 [00:04<00:00, 79.01it/s]Evaluating:  87%|████████▋ | 376/432 [00:04<00:00, 78.82it/s]Evaluating:  89%|████████▉ | 384/432 [00:04<00:00, 78.29it/s]Evaluating:  91%|█████████ | 392/432 [00:04<00:00, 77.70it/s]Evaluating:  93%|█████████▎| 400/432 [00:04<00:00, 77.36it/s]Evaluating:  94%|█████████▍| 408/432 [00:04<00:00, 76.93it/s]Evaluating:  96%|█████████▋| 416/432 [00:04<00:00, 76.16it/s]Evaluating:  98%|█████████▊| 424/432 [00:04<00:00, 75.80it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 75.98it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 87.05it/s]
05/05/2022 19:56:06 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 19:56:06 - INFO - __main__ -     f1 = 0.8954414209467144
05/05/2022 19:56:06 - INFO - __main__ -     loss = 0.2199331427671167
05/05/2022 19:56:06 - INFO - __main__ -     precision = 0.8845289541918755
05/05/2022 19:56:06 - INFO - __main__ -     recall = 0.9066265060240963
05/05/2022 19:56:06 - INFO - Distillation -   Epoch 7 finished
05/05/2022 19:56:06 - INFO - Distillation -   Epoch 8
05/05/2022 19:56:06 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 19:56:09 - INFO - Distillation -   Global step: 3087, epoch step:14
05/05/2022 19:56:13 - INFO - Distillation -   Global step: 3108, epoch step:35
05/05/2022 19:56:16 - INFO - Distillation -   Global step: 3129, epoch step:56
05/05/2022 19:56:20 - INFO - Distillation -   Global step: 3150, epoch step:77
05/05/2022 19:56:24 - INFO - Distillation -   Global step: 3171, epoch step:98
05/05/2022 19:56:28 - INFO - Distillation -   Global step: 3192, epoch step:119
05/05/2022 19:56:32 - INFO - Distillation -   Global step: 3213, epoch step:140
05/05/2022 19:56:35 - INFO - Distillation -   Global step: 3234, epoch step:161
05/05/2022 19:56:39 - INFO - Distillation -   Global step: 3255, epoch step:182
05/05/2022 19:56:43 - INFO - Distillation -   Global step: 3276, epoch step:203
05/05/2022 19:56:47 - INFO - Distillation -   Global step: 3297, epoch step:224
05/05/2022 19:56:50 - INFO - Distillation -   Global step: 3318, epoch step:245
05/05/2022 19:56:54 - INFO - Distillation -   Global step: 3339, epoch step:266
05/05/2022 19:56:58 - INFO - Distillation -   Global step: 3360, epoch step:287
05/05/2022 19:57:02 - INFO - Distillation -   Global step: 3381, epoch step:308
05/05/2022 19:57:06 - INFO - Distillation -   Global step: 3402, epoch step:329
05/05/2022 19:57:09 - INFO - Distillation -   Global step: 3423, epoch step:350
05/05/2022 19:57:13 - INFO - Distillation -   Global step: 3444, epoch step:371
05/05/2022 19:57:17 - INFO - Distillation -   Global step: 3465, epoch step:392
05/05/2022 19:57:21 - INFO - Distillation -   Global step: 3486, epoch step:413
05/05/2022 19:57:25 - INFO - Distillation -   Global step: 3507, epoch step:434
05/05/2022 19:57:26 - INFO - Distillation -   Saving at global step 3512, epoch step 439 epoch 8
05/05/2022 19:57:27 - INFO - Distillation -   Running callback function...
05/05/2022 19:57:27 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 19:57:27 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 19:57:27 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 19:57:27 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 19:57:27 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 19:57:28 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 19:57:28 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 19:57:28 - INFO - __main__ -     Num examples = 3453
05/05/2022 19:57:28 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 103.62it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:03, 103.17it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 102.91it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 102.28it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 101.62it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 101.04it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 100.62it/s]Evaluating:  20%|██        | 88/432 [00:00<00:03, 99.78it/s] Evaluating:  23%|██▎       | 98/432 [00:00<00:03, 99.29it/s]Evaluating:  25%|██▌       | 108/432 [00:01<00:03, 98.41it/s]Evaluating:  27%|██▋       | 118/432 [00:01<00:03, 97.52it/s]Evaluating:  30%|██▉       | 128/432 [00:01<00:03, 96.40it/s]Evaluating:  32%|███▏      | 138/432 [00:01<00:03, 95.46it/s]Evaluating:  34%|███▍      | 148/432 [00:01<00:02, 94.99it/s]Evaluating:  37%|███▋      | 158/432 [00:01<00:02, 94.48it/s]Evaluating:  39%|███▉      | 168/432 [00:01<00:02, 94.31it/s]Evaluating:  41%|████      | 178/432 [00:01<00:02, 93.45it/s]Evaluating:  44%|████▎     | 188/432 [00:01<00:02, 92.29it/s]Evaluating:  46%|████▌     | 198/432 [00:02<00:02, 91.59it/s]Evaluating:  48%|████▊     | 208/432 [00:02<00:02, 90.56it/s]Evaluating:  50%|█████     | 218/432 [00:02<00:02, 89.34it/s]Evaluating:  53%|█████▎    | 227/432 [00:02<00:02, 88.29it/s]Evaluating:  55%|█████▍    | 236/432 [00:02<00:02, 87.49it/s]Evaluating:  57%|█████▋    | 245/432 [00:02<00:02, 86.63it/s]Evaluating:  59%|█████▉    | 254/432 [00:02<00:02, 85.99it/s]Evaluating:  61%|██████    | 263/432 [00:02<00:01, 85.42it/s]Evaluating:  63%|██████▎   | 272/432 [00:02<00:01, 84.58it/s]Evaluating:  65%|██████▌   | 281/432 [00:03<00:01, 84.09it/s]Evaluating:  67%|██████▋   | 290/432 [00:03<00:01, 83.72it/s]Evaluating:  69%|██████▉   | 299/432 [00:03<00:01, 83.23it/s]Evaluating:  71%|███████▏  | 308/432 [00:03<00:01, 82.62it/s]Evaluating:  73%|███████▎  | 317/432 [00:03<00:01, 81.66it/s]Evaluating:  75%|███████▌  | 326/432 [00:03<00:01, 80.81it/s]Evaluating:  78%|███████▊  | 335/432 [00:03<00:01, 80.47it/s]Evaluating:  80%|███████▉  | 344/432 [00:03<00:01, 80.28it/s]Evaluating:  82%|████████▏ | 353/432 [00:03<00:00, 79.87it/s]Evaluating:  84%|████████▎ | 361/432 [00:04<00:00, 79.59it/s]Evaluating:  85%|████████▌ | 369/432 [00:04<00:00, 79.21it/s]Evaluating:  87%|████████▋ | 377/432 [00:04<00:00, 78.70it/s]Evaluating:  89%|████████▉ | 385/432 [00:04<00:00, 78.13it/s]Evaluating:  91%|█████████ | 393/432 [00:04<00:00, 77.81it/s]Evaluating:  93%|█████████▎| 401/432 [00:04<00:00, 77.32it/s]Evaluating:  95%|█████████▍| 409/432 [00:04<00:00, 76.81it/s]Evaluating:  97%|█████████▋| 417/432 [00:04<00:00, 76.35it/s]Evaluating:  98%|█████████▊| 425/432 [00:04<00:00, 75.66it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 87.26it/s]
05/05/2022 19:57:34 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 19:57:34 - INFO - __main__ -     f1 = 0.8953916243210092
05/05/2022 19:57:34 - INFO - __main__ -     loss = 0.22384345240853434
05/05/2022 19:57:34 - INFO - __main__ -     precision = 0.8856152512998267
05/05/2022 19:57:34 - INFO - __main__ -     recall = 0.9053862508858965
05/05/2022 19:57:34 - INFO - Distillation -   Epoch 8 finished
05/05/2022 19:57:34 - INFO - Distillation -   Epoch 9
05/05/2022 19:57:34 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 19:57:36 - INFO - Distillation -   Global step: 3528, epoch step:16
05/05/2022 19:57:40 - INFO - Distillation -   Global step: 3549, epoch step:37
05/05/2022 19:57:44 - INFO - Distillation -   Global step: 3570, epoch step:58
05/05/2022 19:57:48 - INFO - Distillation -   Global step: 3591, epoch step:79
05/05/2022 19:57:52 - INFO - Distillation -   Global step: 3612, epoch step:100
05/05/2022 19:57:55 - INFO - Distillation -   Global step: 3633, epoch step:121
05/05/2022 19:57:59 - INFO - Distillation -   Global step: 3654, epoch step:142
05/05/2022 19:58:03 - INFO - Distillation -   Global step: 3675, epoch step:163
05/05/2022 19:58:07 - INFO - Distillation -   Global step: 3696, epoch step:184
05/05/2022 19:58:11 - INFO - Distillation -   Global step: 3717, epoch step:205
05/05/2022 19:58:14 - INFO - Distillation -   Global step: 3738, epoch step:226
05/05/2022 19:58:18 - INFO - Distillation -   Global step: 3759, epoch step:247
05/05/2022 19:58:22 - INFO - Distillation -   Global step: 3780, epoch step:268
05/05/2022 19:58:26 - INFO - Distillation -   Global step: 3801, epoch step:289
05/05/2022 19:58:30 - INFO - Distillation -   Global step: 3822, epoch step:310
05/05/2022 19:58:33 - INFO - Distillation -   Global step: 3843, epoch step:331
05/05/2022 19:58:37 - INFO - Distillation -   Global step: 3864, epoch step:352
05/05/2022 19:58:41 - INFO - Distillation -   Global step: 3885, epoch step:373
05/05/2022 19:58:45 - INFO - Distillation -   Global step: 3906, epoch step:394
05/05/2022 19:58:49 - INFO - Distillation -   Global step: 3927, epoch step:415
05/05/2022 19:58:52 - INFO - Distillation -   Global step: 3948, epoch step:436
05/05/2022 19:58:53 - INFO - Distillation -   Saving at global step 3951, epoch step 439 epoch 9
05/05/2022 19:58:55 - INFO - Distillation -   Running callback function...
05/05/2022 19:58:55 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 19:58:55 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 19:58:55 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 19:58:55 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 19:58:55 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 19:58:55 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 19:58:55 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 19:58:55 - INFO - __main__ -     Num examples = 3453
05/05/2022 19:58:55 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 101.53it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:04, 100.97it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 100.40it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 100.35it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 100.23it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 99.38it/s] Evaluating:  18%|█▊        | 76/432 [00:00<00:03, 98.90it/s]Evaluating:  20%|█▉        | 86/432 [00:00<00:03, 98.83it/s]Evaluating:  22%|██▏       | 96/432 [00:00<00:03, 98.51it/s]Evaluating:  25%|██▍       | 106/432 [00:01<00:03, 97.68it/s]Evaluating:  27%|██▋       | 116/432 [00:01<00:03, 96.36it/s]Evaluating:  29%|██▉       | 126/432 [00:01<00:03, 94.51it/s]Evaluating:  31%|███▏      | 136/432 [00:01<00:03, 93.71it/s]Evaluating:  34%|███▍      | 146/432 [00:01<00:03, 92.81it/s]Evaluating:  36%|███▌      | 156/432 [00:01<00:03, 91.89it/s]Evaluating:  38%|███▊      | 166/432 [00:01<00:02, 91.24it/s]Evaluating:  41%|████      | 176/432 [00:01<00:02, 90.51it/s]Evaluating:  43%|████▎     | 186/432 [00:01<00:02, 90.12it/s]Evaluating:  45%|████▌     | 196/432 [00:02<00:02, 89.81it/s]Evaluating:  47%|████▋     | 205/432 [00:02<00:02, 89.74it/s]Evaluating:  50%|████▉     | 214/432 [00:02<00:02, 88.97it/s]Evaluating:  52%|█████▏    | 223/432 [00:02<00:02, 88.12it/s]Evaluating:  54%|█████▎    | 232/432 [00:02<00:02, 87.24it/s]Evaluating:  56%|█████▌    | 241/432 [00:02<00:02, 86.61it/s]Evaluating:  58%|█████▊    | 250/432 [00:02<00:02, 86.06it/s]Evaluating:  60%|█████▉    | 259/432 [00:02<00:02, 85.22it/s]Evaluating:  62%|██████▏   | 268/432 [00:02<00:01, 84.71it/s]Evaluating:  64%|██████▍   | 277/432 [00:03<00:01, 84.09it/s]Evaluating:  66%|██████▌   | 286/432 [00:03<00:01, 83.53it/s]Evaluating:  68%|██████▊   | 295/432 [00:03<00:01, 82.95it/s]Evaluating:  70%|███████   | 304/432 [00:03<00:01, 82.52it/s]Evaluating:  72%|███████▏  | 313/432 [00:03<00:01, 81.51it/s]Evaluating:  75%|███████▍  | 322/432 [00:03<00:01, 81.22it/s]Evaluating:  77%|███████▋  | 331/432 [00:03<00:01, 80.98it/s]Evaluating:  79%|███████▊  | 340/432 [00:03<00:01, 80.42it/s]Evaluating:  81%|████████  | 349/432 [00:03<00:01, 80.16it/s]Evaluating:  83%|████████▎ | 358/432 [00:04<00:00, 79.57it/s]Evaluating:  85%|████████▍ | 366/432 [00:04<00:00, 79.10it/s]Evaluating:  87%|████████▋ | 374/432 [00:04<00:00, 78.68it/s]Evaluating:  88%|████████▊ | 382/432 [00:04<00:00, 78.25it/s]Evaluating:  90%|█████████ | 390/432 [00:04<00:00, 77.52it/s]Evaluating:  92%|█████████▏| 398/432 [00:04<00:00, 77.05it/s]Evaluating:  94%|█████████▍| 406/432 [00:04<00:00, 76.67it/s]Evaluating:  96%|█████████▌| 414/432 [00:04<00:00, 76.15it/s]Evaluating:  98%|█████████▊| 422/432 [00:04<00:00, 75.58it/s]Evaluating: 100%|█████████▉| 430/432 [00:04<00:00, 75.10it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 86.41it/s]
05/05/2022 19:59:01 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 19:59:01 - INFO - __main__ -     f1 = 0.8917286652078774
05/05/2022 19:59:01 - INFO - __main__ -     loss = 0.23113556498092777
05/05/2022 19:59:01 - INFO - __main__ -     precision = 0.881162428645563
05/05/2022 19:59:01 - INFO - __main__ -     recall = 0.9025513819985825
05/05/2022 19:59:01 - INFO - Distillation -   Epoch 9 finished
05/05/2022 19:59:01 - INFO - Distillation -   Epoch 10
05/05/2022 19:59:01 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 19:59:04 - INFO - Distillation -   Global step: 3969, epoch step:18
05/05/2022 19:59:08 - INFO - Distillation -   Global step: 3990, epoch step:39
05/05/2022 19:59:12 - INFO - Distillation -   Global step: 4011, epoch step:60
05/05/2022 19:59:15 - INFO - Distillation -   Global step: 4032, epoch step:81
05/05/2022 19:59:19 - INFO - Distillation -   Global step: 4053, epoch step:102
05/05/2022 19:59:23 - INFO - Distillation -   Global step: 4074, epoch step:123
05/05/2022 19:59:27 - INFO - Distillation -   Global step: 4095, epoch step:144
05/05/2022 19:59:31 - INFO - Distillation -   Global step: 4116, epoch step:165
05/05/2022 19:59:34 - INFO - Distillation -   Global step: 4137, epoch step:186
05/05/2022 19:59:38 - INFO - Distillation -   Global step: 4158, epoch step:207
05/05/2022 19:59:42 - INFO - Distillation -   Global step: 4179, epoch step:228
05/05/2022 19:59:46 - INFO - Distillation -   Global step: 4200, epoch step:249
05/05/2022 19:59:50 - INFO - Distillation -   Global step: 4221, epoch step:270
05/05/2022 19:59:53 - INFO - Distillation -   Global step: 4242, epoch step:291
05/05/2022 19:59:57 - INFO - Distillation -   Global step: 4263, epoch step:312
05/05/2022 20:00:01 - INFO - Distillation -   Global step: 4284, epoch step:333
05/05/2022 20:00:05 - INFO - Distillation -   Global step: 4305, epoch step:354
05/05/2022 20:00:09 - INFO - Distillation -   Global step: 4326, epoch step:375
05/05/2022 20:00:12 - INFO - Distillation -   Global step: 4347, epoch step:396
05/05/2022 20:00:16 - INFO - Distillation -   Global step: 4368, epoch step:417
05/05/2022 20:00:20 - INFO - Distillation -   Global step: 4389, epoch step:438
05/05/2022 20:00:20 - INFO - Distillation -   Saving at global step 4390, epoch step 439 epoch 10
05/05/2022 20:00:22 - INFO - Distillation -   Running callback function...
05/05/2022 20:00:22 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 20:00:22 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 20:00:22 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 20:00:22 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 20:00:22 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 20:00:22 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 20:00:22 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 20:00:22 - INFO - __main__ -     Num examples = 3453
05/05/2022 20:00:22 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 103.12it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:03, 103.05it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 102.73it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 102.38it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 101.33it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 100.69it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 100.31it/s]Evaluating:  20%|██        | 88/432 [00:00<00:03, 99.85it/s] Evaluating:  23%|██▎       | 98/432 [00:00<00:03, 99.12it/s]Evaluating:  25%|██▌       | 108/432 [00:01<00:03, 98.26it/s]Evaluating:  27%|██▋       | 118/432 [00:01<00:03, 97.56it/s]Evaluating:  30%|██▉       | 128/432 [00:01<00:03, 96.77it/s]Evaluating:  32%|███▏      | 138/432 [00:01<00:03, 96.03it/s]Evaluating:  34%|███▍      | 148/432 [00:01<00:03, 94.51it/s]Evaluating:  37%|███▋      | 158/432 [00:01<00:02, 93.77it/s]Evaluating:  39%|███▉      | 168/432 [00:01<00:02, 93.12it/s]Evaluating:  41%|████      | 178/432 [00:01<00:02, 92.46it/s]Evaluating:  44%|████▎     | 188/432 [00:01<00:02, 92.01it/s]Evaluating:  46%|████▌     | 198/432 [00:02<00:02, 91.20it/s]Evaluating:  48%|████▊     | 208/432 [00:02<00:02, 90.05it/s]Evaluating:  50%|█████     | 218/432 [00:02<00:02, 89.16it/s]Evaluating:  53%|█████▎    | 227/432 [00:02<00:02, 88.21it/s]Evaluating:  55%|█████▍    | 236/432 [00:02<00:02, 87.60it/s]Evaluating:  57%|█████▋    | 245/432 [00:02<00:02, 86.70it/s]Evaluating:  59%|█████▉    | 254/432 [00:02<00:02, 85.91it/s]Evaluating:  61%|██████    | 263/432 [00:02<00:01, 85.34it/s]Evaluating:  63%|██████▎   | 272/432 [00:02<00:01, 84.73it/s]Evaluating:  65%|██████▌   | 281/432 [00:03<00:01, 84.17it/s]Evaluating:  67%|██████▋   | 290/432 [00:03<00:01, 83.31it/s]Evaluating:  69%|██████▉   | 299/432 [00:03<00:01, 82.94it/s]Evaluating:  71%|███████▏  | 308/432 [00:03<00:01, 82.55it/s]Evaluating:  73%|███████▎  | 317/432 [00:03<00:01, 82.18it/s]Evaluating:  75%|███████▌  | 326/432 [00:03<00:01, 81.84it/s]Evaluating:  78%|███████▊  | 335/432 [00:03<00:01, 81.39it/s]Evaluating:  80%|███████▉  | 344/432 [00:03<00:01, 80.77it/s]Evaluating:  82%|████████▏ | 353/432 [00:03<00:00, 80.18it/s]Evaluating:  84%|████████▍ | 362/432 [00:04<00:00, 79.64it/s]Evaluating:  86%|████████▌ | 370/432 [00:04<00:00, 79.02it/s]Evaluating:  88%|████████▊ | 378/432 [00:04<00:00, 78.45it/s]Evaluating:  89%|████████▉ | 386/432 [00:04<00:00, 77.90it/s]Evaluating:  91%|█████████ | 394/432 [00:04<00:00, 77.45it/s]Evaluating:  93%|█████████▎| 402/432 [00:04<00:00, 76.99it/s]Evaluating:  95%|█████████▍| 410/432 [00:04<00:00, 76.63it/s]Evaluating:  97%|█████████▋| 418/432 [00:04<00:00, 75.97it/s]Evaluating:  99%|█████████▊| 426/432 [00:04<00:00, 75.58it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 87.20it/s]
05/05/2022 20:00:28 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 20:00:28 - INFO - __main__ -     f1 = 0.8937960042060988
05/05/2022 20:00:28 - INFO - __main__ -     loss = 0.2292208991707275
05/05/2022 20:00:28 - INFO - __main__ -     precision = 0.8841886269070736
05/05/2022 20:00:28 - INFO - __main__ -     recall = 0.9036144578313253
05/05/2022 20:00:28 - INFO - Distillation -   Epoch 10 finished
05/05/2022 20:00:28 - INFO - Distillation -   Epoch 11
05/05/2022 20:00:28 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 20:00:32 - INFO - Distillation -   Global step: 4410, epoch step:20
05/05/2022 20:00:35 - INFO - Distillation -   Global step: 4431, epoch step:41
05/05/2022 20:00:39 - INFO - Distillation -   Global step: 4452, epoch step:62
05/05/2022 20:00:43 - INFO - Distillation -   Global step: 4473, epoch step:83
05/05/2022 20:00:47 - INFO - Distillation -   Global step: 4494, epoch step:104
05/05/2022 20:00:51 - INFO - Distillation -   Global step: 4515, epoch step:125
05/05/2022 20:00:54 - INFO - Distillation -   Global step: 4536, epoch step:146
05/05/2022 20:00:58 - INFO - Distillation -   Global step: 4557, epoch step:167
05/05/2022 20:01:02 - INFO - Distillation -   Global step: 4578, epoch step:188
05/05/2022 20:01:06 - INFO - Distillation -   Global step: 4599, epoch step:209
05/05/2022 20:01:10 - INFO - Distillation -   Global step: 4620, epoch step:230
05/05/2022 20:01:13 - INFO - Distillation -   Global step: 4641, epoch step:251
05/05/2022 20:01:17 - INFO - Distillation -   Global step: 4662, epoch step:272
05/05/2022 20:01:21 - INFO - Distillation -   Global step: 4683, epoch step:293
05/05/2022 20:01:25 - INFO - Distillation -   Global step: 4704, epoch step:314
05/05/2022 20:01:29 - INFO - Distillation -   Global step: 4725, epoch step:335
05/05/2022 20:01:32 - INFO - Distillation -   Global step: 4746, epoch step:356
05/05/2022 20:01:36 - INFO - Distillation -   Global step: 4767, epoch step:377
05/05/2022 20:01:40 - INFO - Distillation -   Global step: 4788, epoch step:398
05/05/2022 20:01:44 - INFO - Distillation -   Global step: 4809, epoch step:419
05/05/2022 20:01:47 - INFO - Distillation -   Saving at global step 4829, epoch step 439 epoch 11
05/05/2022 20:01:49 - INFO - Distillation -   Running callback function...
05/05/2022 20:01:49 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 20:01:49 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 20:01:49 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 20:01:49 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 20:01:49 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 20:01:49 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 20:01:50 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 20:01:50 - INFO - __main__ -     Num examples = 3453
05/05/2022 20:01:50 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 103.10it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:03, 103.22it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 102.80it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 102.23it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 101.17it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 100.55it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 100.01it/s]Evaluating:  20%|██        | 88/432 [00:00<00:03, 99.56it/s] Evaluating:  23%|██▎       | 98/432 [00:00<00:03, 99.36it/s]Evaluating:  25%|██▌       | 108/432 [00:01<00:03, 98.47it/s]Evaluating:  27%|██▋       | 118/432 [00:01<00:03, 97.76it/s]Evaluating:  30%|██▉       | 128/432 [00:01<00:03, 97.04it/s]Evaluating:  32%|███▏      | 138/432 [00:01<00:03, 96.18it/s]Evaluating:  34%|███▍      | 148/432 [00:01<00:02, 95.79it/s]Evaluating:  37%|███▋      | 158/432 [00:01<00:02, 94.95it/s]Evaluating:  39%|███▉      | 168/432 [00:01<00:02, 94.12it/s]Evaluating:  41%|████      | 178/432 [00:01<00:02, 92.91it/s]Evaluating:  44%|████▎     | 188/432 [00:01<00:02, 91.98it/s]Evaluating:  46%|████▌     | 198/432 [00:02<00:02, 91.01it/s]Evaluating:  48%|████▊     | 208/432 [00:02<00:02, 90.04it/s]Evaluating:  50%|█████     | 218/432 [00:02<00:02, 89.05it/s]Evaluating:  53%|█████▎    | 227/432 [00:02<00:02, 88.23it/s]Evaluating:  55%|█████▍    | 236/432 [00:02<00:02, 87.57it/s]Evaluating:  57%|█████▋    | 245/432 [00:02<00:02, 86.64it/s]Evaluating:  59%|█████▉    | 254/432 [00:02<00:02, 85.52it/s]Evaluating:  61%|██████    | 263/432 [00:02<00:01, 85.05it/s]Evaluating:  63%|██████▎   | 272/432 [00:02<00:01, 84.43it/s]Evaluating:  65%|██████▌   | 281/432 [00:03<00:01, 83.94it/s]Evaluating:  67%|██████▋   | 290/432 [00:03<00:01, 83.20it/s]Evaluating:  69%|██████▉   | 299/432 [00:03<00:01, 82.83it/s]Evaluating:  71%|███████▏  | 308/432 [00:03<00:01, 82.24it/s]Evaluating:  73%|███████▎  | 317/432 [00:03<00:01, 81.75it/s]Evaluating:  75%|███████▌  | 326/432 [00:03<00:01, 81.11it/s]Evaluating:  78%|███████▊  | 335/432 [00:03<00:01, 80.59it/s]Evaluating:  80%|███████▉  | 344/432 [00:03<00:01, 80.13it/s]Evaluating:  82%|████████▏ | 353/432 [00:03<00:00, 79.75it/s]Evaluating:  84%|████████▎ | 361/432 [00:04<00:00, 79.41it/s]Evaluating:  85%|████████▌ | 369/432 [00:04<00:00, 78.77it/s]Evaluating:  87%|████████▋ | 377/432 [00:04<00:00, 78.24it/s]Evaluating:  89%|████████▉ | 385/432 [00:04<00:00, 77.83it/s]Evaluating:  91%|█████████ | 393/432 [00:04<00:00, 77.58it/s]Evaluating:  93%|█████████▎| 401/432 [00:04<00:00, 76.90it/s]Evaluating:  95%|█████████▍| 409/432 [00:04<00:00, 76.35it/s]Evaluating:  97%|█████████▋| 417/432 [00:04<00:00, 76.08it/s]Evaluating:  98%|█████████▊| 425/432 [00:04<00:00, 75.70it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 87.14it/s]
05/05/2022 20:01:55 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 20:01:55 - INFO - __main__ -     f1 = 0.8982665032393626
05/05/2022 20:01:55 - INFO - __main__ -     loss = 0.2227877118894336
05/05/2022 20:01:55 - INFO - __main__ -     precision = 0.8878504672897196
05/05/2022 20:01:55 - INFO - __main__ -     recall = 0.9089298369950389
05/05/2022 20:01:55 - INFO - Distillation -   Epoch 11 finished
05/05/2022 20:01:55 - INFO - Distillation -   Epoch 12
05/05/2022 20:01:55 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 20:01:56 - INFO - Distillation -   Global step: 4830, epoch step:1
05/05/2022 20:01:59 - INFO - Distillation -   Global step: 4851, epoch step:22
05/05/2022 20:02:03 - INFO - Distillation -   Global step: 4872, epoch step:43
05/05/2022 20:02:07 - INFO - Distillation -   Global step: 4893, epoch step:64
05/05/2022 20:02:11 - INFO - Distillation -   Global step: 4914, epoch step:85
05/05/2022 20:02:15 - INFO - Distillation -   Global step: 4935, epoch step:106
05/05/2022 20:02:18 - INFO - Distillation -   Global step: 4956, epoch step:127
05/05/2022 20:02:22 - INFO - Distillation -   Global step: 4977, epoch step:148
05/05/2022 20:02:26 - INFO - Distillation -   Global step: 4998, epoch step:169
05/05/2022 20:02:30 - INFO - Distillation -   Global step: 5019, epoch step:190
05/05/2022 20:02:34 - INFO - Distillation -   Global step: 5040, epoch step:211
05/05/2022 20:02:37 - INFO - Distillation -   Global step: 5061, epoch step:232
05/05/2022 20:02:41 - INFO - Distillation -   Global step: 5082, epoch step:253
05/05/2022 20:02:45 - INFO - Distillation -   Global step: 5103, epoch step:274
05/05/2022 20:02:49 - INFO - Distillation -   Global step: 5124, epoch step:295
05/05/2022 20:02:53 - INFO - Distillation -   Global step: 5145, epoch step:316
05/05/2022 20:02:56 - INFO - Distillation -   Global step: 5166, epoch step:337
05/05/2022 20:03:00 - INFO - Distillation -   Global step: 5187, epoch step:358
05/05/2022 20:03:04 - INFO - Distillation -   Global step: 5208, epoch step:379
05/05/2022 20:03:08 - INFO - Distillation -   Global step: 5229, epoch step:400
05/05/2022 20:03:12 - INFO - Distillation -   Global step: 5250, epoch step:421
05/05/2022 20:03:15 - INFO - Distillation -   Saving at global step 5268, epoch step 439 epoch 12
05/05/2022 20:03:17 - INFO - Distillation -   Running callback function...
05/05/2022 20:03:17 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 20:03:17 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 20:03:17 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 20:03:17 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 20:03:17 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 20:03:17 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 20:03:17 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 20:03:17 - INFO - __main__ -     Num examples = 3453
05/05/2022 20:03:17 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 103.21it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:03, 102.89it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 102.27it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 101.45it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 101.02it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 100.42it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 99.89it/s] Evaluating:  20%|██        | 87/432 [00:00<00:03, 99.61it/s]Evaluating:  22%|██▏       | 97/432 [00:00<00:03, 99.05it/s]Evaluating:  25%|██▍       | 107/432 [00:01<00:03, 97.87it/s]Evaluating:  27%|██▋       | 117/432 [00:01<00:03, 96.76it/s]Evaluating:  29%|██▉       | 127/432 [00:01<00:03, 96.07it/s]Evaluating:  32%|███▏      | 137/432 [00:01<00:03, 95.29it/s]Evaluating:  34%|███▍      | 147/432 [00:01<00:03, 94.48it/s]Evaluating:  36%|███▋      | 157/432 [00:01<00:02, 94.00it/s]Evaluating:  39%|███▊      | 167/432 [00:01<00:02, 93.39it/s]Evaluating:  41%|████      | 177/432 [00:01<00:02, 92.72it/s]Evaluating:  43%|████▎     | 187/432 [00:01<00:02, 92.01it/s]Evaluating:  46%|████▌     | 197/432 [00:02<00:02, 90.91it/s]Evaluating:  48%|████▊     | 207/432 [00:02<00:02, 89.80it/s]Evaluating:  50%|█████     | 216/432 [00:02<00:02, 88.73it/s]Evaluating:  52%|█████▏    | 225/432 [00:02<00:02, 87.91it/s]Evaluating:  54%|█████▍    | 234/432 [00:02<00:02, 87.22it/s]Evaluating:  56%|█████▋    | 243/432 [00:02<00:02, 86.45it/s]Evaluating:  58%|█████▊    | 252/432 [00:02<00:02, 85.84it/s]Evaluating:  60%|██████    | 261/432 [00:02<00:02, 85.22it/s]Evaluating:  62%|██████▎   | 270/432 [00:02<00:01, 84.66it/s]Evaluating:  65%|██████▍   | 279/432 [00:03<00:01, 83.87it/s]Evaluating:  67%|██████▋   | 288/432 [00:03<00:01, 83.16it/s]Evaluating:  69%|██████▉   | 297/432 [00:03<00:01, 82.54it/s]Evaluating:  71%|███████   | 306/432 [00:03<00:01, 82.04it/s]Evaluating:  73%|███████▎  | 315/432 [00:03<00:01, 81.73it/s]Evaluating:  75%|███████▌  | 324/432 [00:03<00:01, 81.36it/s]Evaluating:  77%|███████▋  | 333/432 [00:03<00:01, 80.82it/s]Evaluating:  79%|███████▉  | 342/432 [00:03<00:01, 80.23it/s]Evaluating:  81%|████████▏ | 351/432 [00:03<00:01, 79.78it/s]Evaluating:  83%|████████▎ | 359/432 [00:04<00:00, 79.29it/s]Evaluating:  85%|████████▍ | 367/432 [00:04<00:00, 78.65it/s]Evaluating:  87%|████████▋ | 375/432 [00:04<00:00, 77.99it/s]Evaluating:  89%|████████▊ | 383/432 [00:04<00:00, 77.40it/s]Evaluating:  91%|█████████ | 391/432 [00:04<00:00, 76.96it/s]Evaluating:  92%|█████████▏| 399/432 [00:04<00:00, 76.59it/s]Evaluating:  94%|█████████▍| 407/432 [00:04<00:00, 76.14it/s]Evaluating:  96%|█████████▌| 415/432 [00:04<00:00, 75.72it/s]Evaluating:  98%|█████████▊| 423/432 [00:04<00:00, 75.35it/s]Evaluating: 100%|█████████▉| 431/432 [00:04<00:00, 75.07it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 86.77it/s]
05/05/2022 20:03:23 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 20:03:23 - INFO - __main__ -     f1 = 0.899612539626629
05/05/2022 20:03:23 - INFO - __main__ -     loss = 0.21333172479617174
05/05/2022 20:03:23 - INFO - __main__ -     precision = 0.8942577030812325
05/05/2022 20:03:23 - INFO - __main__ -     recall = 0.9050318922749823
05/05/2022 20:03:23 - INFO - Distillation -   Epoch 12 finished
05/05/2022 20:03:23 - INFO - Distillation -   Epoch 13
05/05/2022 20:03:23 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 20:03:23 - INFO - Distillation -   Global step: 5271, epoch step:3
05/05/2022 20:03:27 - INFO - Distillation -   Global step: 5292, epoch step:24
05/05/2022 20:03:31 - INFO - Distillation -   Global step: 5313, epoch step:45
05/05/2022 20:03:35 - INFO - Distillation -   Global step: 5334, epoch step:66
05/05/2022 20:03:39 - INFO - Distillation -   Global step: 5355, epoch step:87
05/05/2022 20:03:42 - INFO - Distillation -   Global step: 5376, epoch step:108
05/05/2022 20:03:46 - INFO - Distillation -   Global step: 5397, epoch step:129
05/05/2022 20:03:50 - INFO - Distillation -   Global step: 5418, epoch step:150
05/05/2022 20:03:54 - INFO - Distillation -   Global step: 5439, epoch step:171
05/05/2022 20:03:58 - INFO - Distillation -   Global step: 5460, epoch step:192
05/05/2022 20:04:02 - INFO - Distillation -   Global step: 5481, epoch step:213
05/05/2022 20:04:05 - INFO - Distillation -   Global step: 5502, epoch step:234
05/05/2022 20:04:09 - INFO - Distillation -   Global step: 5523, epoch step:255
05/05/2022 20:04:13 - INFO - Distillation -   Global step: 5544, epoch step:276
05/05/2022 20:04:17 - INFO - Distillation -   Global step: 5565, epoch step:297
05/05/2022 20:04:21 - INFO - Distillation -   Global step: 5586, epoch step:318
05/05/2022 20:04:24 - INFO - Distillation -   Global step: 5607, epoch step:339
05/05/2022 20:04:28 - INFO - Distillation -   Global step: 5628, epoch step:360
05/05/2022 20:04:32 - INFO - Distillation -   Global step: 5649, epoch step:381
05/05/2022 20:04:36 - INFO - Distillation -   Global step: 5670, epoch step:402
05/05/2022 20:04:40 - INFO - Distillation -   Global step: 5691, epoch step:423
05/05/2022 20:04:42 - INFO - Distillation -   Saving at global step 5707, epoch step 439 epoch 13
05/05/2022 20:04:44 - INFO - Distillation -   Running callback function...
05/05/2022 20:04:44 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 20:04:44 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 20:04:44 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 20:04:44 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 20:04:44 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 20:04:44 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 20:04:45 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 20:04:45 - INFO - __main__ -     Num examples = 3453
05/05/2022 20:04:45 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 103.28it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:03, 103.22it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 103.20it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 102.59it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 101.42it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 100.59it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 100.33it/s]Evaluating:  20%|██        | 88/432 [00:00<00:03, 99.82it/s] Evaluating:  23%|██▎       | 98/432 [00:00<00:03, 99.56it/s]Evaluating:  25%|██▌       | 108/432 [00:01<00:03, 98.51it/s]Evaluating:  27%|██▋       | 118/432 [00:01<00:03, 97.54it/s]Evaluating:  30%|██▉       | 128/432 [00:01<00:03, 96.60it/s]Evaluating:  32%|███▏      | 138/432 [00:01<00:03, 95.10it/s]Evaluating:  34%|███▍      | 148/432 [00:01<00:03, 94.15it/s]Evaluating:  37%|███▋      | 158/432 [00:01<00:02, 93.46it/s]Evaluating:  39%|███▉      | 168/432 [00:01<00:02, 93.04it/s]Evaluating:  41%|████      | 178/432 [00:01<00:02, 92.08it/s]Evaluating:  44%|████▎     | 188/432 [00:01<00:02, 91.29it/s]Evaluating:  46%|████▌     | 198/432 [00:02<00:02, 90.53it/s]Evaluating:  48%|████▊     | 208/432 [00:02<00:02, 89.44it/s]Evaluating:  50%|█████     | 217/432 [00:02<00:02, 88.49it/s]Evaluating:  52%|█████▏    | 226/432 [00:02<00:02, 87.62it/s]Evaluating:  54%|█████▍    | 235/432 [00:02<00:02, 86.91it/s]Evaluating:  56%|█████▋    | 244/432 [00:02<00:02, 86.18it/s]Evaluating:  59%|█████▊    | 253/432 [00:02<00:02, 85.37it/s]Evaluating:  61%|██████    | 262/432 [00:02<00:02, 84.51it/s]Evaluating:  63%|██████▎   | 271/432 [00:02<00:01, 83.67it/s]Evaluating:  65%|██████▍   | 280/432 [00:03<00:01, 83.16it/s]Evaluating:  67%|██████▋   | 289/432 [00:03<00:01, 82.59it/s]Evaluating:  69%|██████▉   | 298/432 [00:03<00:01, 82.04it/s]Evaluating:  71%|███████   | 307/432 [00:03<00:01, 81.88it/s]Evaluating:  73%|███████▎  | 316/432 [00:03<00:01, 81.59it/s]Evaluating:  75%|███████▌  | 325/432 [00:03<00:01, 81.28it/s]Evaluating:  77%|███████▋  | 334/432 [00:03<00:01, 80.92it/s]Evaluating:  79%|███████▉  | 343/432 [00:03<00:01, 80.31it/s]Evaluating:  81%|████████▏ | 352/432 [00:03<00:01, 79.74it/s]Evaluating:  83%|████████▎ | 360/432 [00:04<00:00, 79.37it/s]Evaluating:  85%|████████▌ | 368/432 [00:04<00:00, 78.81it/s]Evaluating:  87%|████████▋ | 376/432 [00:04<00:00, 78.09it/s]Evaluating:  89%|████████▉ | 384/432 [00:04<00:00, 77.68it/s]Evaluating:  91%|█████████ | 392/432 [00:04<00:00, 77.24it/s]Evaluating:  93%|█████████▎| 400/432 [00:04<00:00, 76.73it/s]Evaluating:  94%|█████████▍| 408/432 [00:04<00:00, 76.31it/s]Evaluating:  96%|█████████▋| 416/432 [00:04<00:00, 75.94it/s]Evaluating:  98%|█████████▊| 424/432 [00:04<00:00, 75.26it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 75.52it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 86.74it/s]
05/05/2022 20:04:50 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 20:04:50 - INFO - __main__ -     f1 = 0.8981962164540255
05/05/2022 20:04:50 - INFO - __main__ -     loss = 0.22383490845332213
05/05/2022 20:04:50 - INFO - __main__ -     precision = 0.8921517217269708
05/05/2022 20:04:50 - INFO - __main__ -     recall = 0.9043231750531537
05/05/2022 20:04:50 - INFO - Distillation -   Epoch 13 finished
05/05/2022 20:04:50 - INFO - Distillation -   Epoch 14
05/05/2022 20:04:50 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 20:04:51 - INFO - Distillation -   Global step: 5712, epoch step:5
05/05/2022 20:04:55 - INFO - Distillation -   Global step: 5733, epoch step:26
05/05/2022 20:04:59 - INFO - Distillation -   Global step: 5754, epoch step:47
05/05/2022 20:05:03 - INFO - Distillation -   Global step: 5775, epoch step:68
05/05/2022 20:05:06 - INFO - Distillation -   Global step: 5796, epoch step:89
05/05/2022 20:05:10 - INFO - Distillation -   Global step: 5817, epoch step:110
05/05/2022 20:05:14 - INFO - Distillation -   Global step: 5838, epoch step:131
05/05/2022 20:05:18 - INFO - Distillation -   Global step: 5859, epoch step:152
05/05/2022 20:05:22 - INFO - Distillation -   Global step: 5880, epoch step:173
05/05/2022 20:05:25 - INFO - Distillation -   Global step: 5901, epoch step:194
05/05/2022 20:05:29 - INFO - Distillation -   Global step: 5922, epoch step:215
05/05/2022 20:05:33 - INFO - Distillation -   Global step: 5943, epoch step:236
05/05/2022 20:05:37 - INFO - Distillation -   Global step: 5964, epoch step:257
05/05/2022 20:05:41 - INFO - Distillation -   Global step: 5985, epoch step:278
05/05/2022 20:05:44 - INFO - Distillation -   Global step: 6006, epoch step:299
05/05/2022 20:05:48 - INFO - Distillation -   Global step: 6027, epoch step:320
05/05/2022 20:05:52 - INFO - Distillation -   Global step: 6048, epoch step:341
05/05/2022 20:05:56 - INFO - Distillation -   Global step: 6069, epoch step:362
05/05/2022 20:06:00 - INFO - Distillation -   Global step: 6090, epoch step:383
05/05/2022 20:06:04 - INFO - Distillation -   Global step: 6111, epoch step:404
05/05/2022 20:06:07 - INFO - Distillation -   Global step: 6132, epoch step:425
05/05/2022 20:06:10 - INFO - Distillation -   Saving at global step 6146, epoch step 439 epoch 14
05/05/2022 20:06:12 - INFO - Distillation -   Running callback function...
05/05/2022 20:06:12 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 20:06:12 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 20:06:12 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 20:06:12 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 20:06:12 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 20:06:12 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 20:06:12 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 20:06:12 - INFO - __main__ -     Num examples = 3453
05/05/2022 20:06:12 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 102.02it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:04, 102.38it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 102.03it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 101.38it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 100.88it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 100.12it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 99.59it/s] Evaluating:  20%|██        | 87/432 [00:00<00:03, 98.95it/s]Evaluating:  22%|██▏       | 97/432 [00:00<00:03, 98.08it/s]Evaluating:  25%|██▍       | 107/432 [00:01<00:03, 97.23it/s]Evaluating:  27%|██▋       | 117/432 [00:01<00:03, 96.44it/s]Evaluating:  29%|██▉       | 127/432 [00:01<00:03, 95.41it/s]Evaluating:  32%|███▏      | 137/432 [00:01<00:03, 94.61it/s]Evaluating:  34%|███▍      | 147/432 [00:01<00:03, 93.69it/s]Evaluating:  36%|███▋      | 157/432 [00:01<00:02, 92.43it/s]Evaluating:  39%|███▊      | 167/432 [00:01<00:02, 91.45it/s]Evaluating:  41%|████      | 177/432 [00:01<00:02, 90.78it/s]Evaluating:  43%|████▎     | 187/432 [00:01<00:02, 90.06it/s]Evaluating:  46%|████▌     | 197/432 [00:02<00:02, 89.37it/s]Evaluating:  48%|████▊     | 206/432 [00:02<00:02, 88.74it/s]Evaluating:  50%|████▉     | 215/432 [00:02<00:02, 88.01it/s]Evaluating:  52%|█████▏    | 224/432 [00:02<00:02, 87.36it/s]Evaluating:  54%|█████▍    | 233/432 [00:02<00:02, 86.79it/s]Evaluating:  56%|█████▌    | 242/432 [00:02<00:02, 86.01it/s]Evaluating:  58%|█████▊    | 251/432 [00:02<00:02, 85.09it/s]Evaluating:  60%|██████    | 260/432 [00:02<00:02, 84.41it/s]Evaluating:  62%|██████▏   | 269/432 [00:02<00:01, 83.73it/s]Evaluating:  64%|██████▍   | 278/432 [00:03<00:01, 83.10it/s]Evaluating:  66%|██████▋   | 287/432 [00:03<00:01, 82.55it/s]Evaluating:  69%|██████▊   | 296/432 [00:03<00:01, 81.96it/s]Evaluating:  71%|███████   | 305/432 [00:03<00:01, 81.18it/s]Evaluating:  73%|███████▎  | 314/432 [00:03<00:01, 80.45it/s]Evaluating:  75%|███████▍  | 323/432 [00:03<00:01, 79.94it/s]Evaluating:  77%|███████▋  | 331/432 [00:03<00:01, 79.53it/s]Evaluating:  78%|███████▊  | 339/432 [00:03<00:01, 79.26it/s]Evaluating:  80%|████████  | 347/432 [00:03<00:01, 78.87it/s]Evaluating:  82%|████████▏ | 355/432 [00:04<00:00, 78.60it/s]Evaluating:  84%|████████▍ | 363/432 [00:04<00:00, 78.21it/s]Evaluating:  86%|████████▌ | 371/432 [00:04<00:00, 77.61it/s]Evaluating:  88%|████████▊ | 379/432 [00:04<00:00, 77.09it/s]Evaluating:  90%|████████▉ | 387/432 [00:04<00:00, 76.55it/s]Evaluating:  91%|█████████▏| 395/432 [00:04<00:00, 76.01it/s]Evaluating:  93%|█████████▎| 403/432 [00:04<00:00, 75.70it/s]Evaluating:  95%|█████████▌| 411/432 [00:04<00:00, 75.24it/s]Evaluating:  97%|█████████▋| 419/432 [00:04<00:00, 74.80it/s]Evaluating:  99%|█████████▉| 427/432 [00:04<00:00, 74.30it/s]Evaluating: 100%|██████████| 432/432 [00:05<00:00, 85.88it/s]
05/05/2022 20:06:18 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 20:06:18 - INFO - __main__ -     f1 = 0.896872803935348
05/05/2022 20:06:18 - INFO - __main__ -     loss = 0.21682441853708193
05/05/2022 20:06:18 - INFO - __main__ -     precision = 0.8893728222996515
05/05/2022 20:06:18 - INFO - __main__ -     recall = 0.9045003543586109
05/05/2022 20:06:18 - INFO - Distillation -   Epoch 14 finished
05/05/2022 20:06:18 - INFO - Distillation -   Epoch 15
05/05/2022 20:06:18 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 20:06:19 - INFO - Distillation -   Global step: 6153, epoch step:7
05/05/2022 20:06:23 - INFO - Distillation -   Global step: 6174, epoch step:28
05/05/2022 20:06:27 - INFO - Distillation -   Global step: 6195, epoch step:49
05/05/2022 20:06:31 - INFO - Distillation -   Global step: 6216, epoch step:70
05/05/2022 20:06:34 - INFO - Distillation -   Global step: 6237, epoch step:91
05/05/2022 20:06:38 - INFO - Distillation -   Global step: 6258, epoch step:112
05/05/2022 20:06:42 - INFO - Distillation -   Global step: 6279, epoch step:133
05/05/2022 20:06:46 - INFO - Distillation -   Global step: 6300, epoch step:154
05/05/2022 20:06:50 - INFO - Distillation -   Global step: 6321, epoch step:175
05/05/2022 20:06:53 - INFO - Distillation -   Global step: 6342, epoch step:196
05/05/2022 20:06:57 - INFO - Distillation -   Global step: 6363, epoch step:217
05/05/2022 20:07:01 - INFO - Distillation -   Global step: 6384, epoch step:238
05/05/2022 20:07:05 - INFO - Distillation -   Global step: 6405, epoch step:259
05/05/2022 20:07:09 - INFO - Distillation -   Global step: 6426, epoch step:280
05/05/2022 20:07:13 - INFO - Distillation -   Global step: 6447, epoch step:301
05/05/2022 20:07:16 - INFO - Distillation -   Global step: 6468, epoch step:322
05/05/2022 20:07:20 - INFO - Distillation -   Global step: 6489, epoch step:343
05/05/2022 20:07:24 - INFO - Distillation -   Global step: 6510, epoch step:364
05/05/2022 20:07:28 - INFO - Distillation -   Global step: 6531, epoch step:385
05/05/2022 20:07:32 - INFO - Distillation -   Global step: 6552, epoch step:406
05/05/2022 20:07:35 - INFO - Distillation -   Global step: 6573, epoch step:427
05/05/2022 20:07:38 - INFO - Distillation -   Saving at global step 6585, epoch step 439 epoch 15
05/05/2022 20:07:39 - INFO - Distillation -   Running callback function...
05/05/2022 20:07:39 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 20:07:39 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 20:07:39 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 20:07:39 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 20:07:39 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 20:07:40 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 20:07:40 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 20:07:40 - INFO - __main__ -     Num examples = 3453
05/05/2022 20:07:40 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 102.86it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:03, 102.95it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 101.84it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 101.11it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 100.40it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 100.06it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 99.54it/s] Evaluating:  20%|██        | 87/432 [00:00<00:03, 99.03it/s]Evaluating:  22%|██▏       | 97/432 [00:00<00:03, 98.81it/s]Evaluating:  25%|██▍       | 107/432 [00:01<00:03, 98.24it/s]Evaluating:  27%|██▋       | 117/432 [00:01<00:03, 97.73it/s]Evaluating:  29%|██▉       | 127/432 [00:01<00:03, 96.90it/s]Evaluating:  32%|███▏      | 137/432 [00:01<00:03, 95.46it/s]Evaluating:  34%|███▍      | 147/432 [00:01<00:03, 94.41it/s]Evaluating:  36%|███▋      | 157/432 [00:01<00:02, 93.14it/s]Evaluating:  39%|███▊      | 167/432 [00:01<00:02, 92.16it/s]Evaluating:  41%|████      | 177/432 [00:01<00:02, 91.21it/s]Evaluating:  43%|████▎     | 187/432 [00:01<00:02, 90.34it/s]Evaluating:  46%|████▌     | 197/432 [00:02<00:02, 89.42it/s]Evaluating:  48%|████▊     | 206/432 [00:02<00:02, 88.68it/s]Evaluating:  50%|████▉     | 215/432 [00:02<00:02, 87.52it/s]Evaluating:  52%|█████▏    | 224/432 [00:02<00:02, 86.76it/s]Evaluating:  54%|█████▍    | 233/432 [00:02<00:02, 86.18it/s]Evaluating:  56%|█████▌    | 242/432 [00:02<00:02, 85.51it/s]Evaluating:  58%|█████▊    | 251/432 [00:02<00:02, 85.28it/s]Evaluating:  60%|██████    | 260/432 [00:02<00:02, 84.85it/s]Evaluating:  62%|██████▏   | 269/432 [00:02<00:01, 84.14it/s]Evaluating:  64%|██████▍   | 278/432 [00:03<00:01, 83.42it/s]Evaluating:  66%|██████▋   | 287/432 [00:03<00:01, 82.98it/s]Evaluating:  69%|██████▊   | 296/432 [00:03<00:01, 82.41it/s]Evaluating:  71%|███████   | 305/432 [00:03<00:01, 81.80it/s]Evaluating:  73%|███████▎  | 314/432 [00:03<00:01, 81.15it/s]Evaluating:  75%|███████▍  | 323/432 [00:03<00:01, 80.50it/s]Evaluating:  77%|███████▋  | 332/432 [00:03<00:01, 80.09it/s]Evaluating:  79%|███████▉  | 341/432 [00:03<00:01, 79.62it/s]Evaluating:  81%|████████  | 349/432 [00:03<00:01, 79.34it/s]Evaluating:  83%|████████▎ | 357/432 [00:04<00:00, 79.19it/s]Evaluating:  84%|████████▍ | 365/432 [00:04<00:00, 78.61it/s]Evaluating:  86%|████████▋ | 373/432 [00:04<00:00, 78.13it/s]Evaluating:  88%|████████▊ | 381/432 [00:04<00:00, 77.55it/s]Evaluating:  90%|█████████ | 389/432 [00:04<00:00, 76.88it/s]Evaluating:  92%|█████████▏| 397/432 [00:04<00:00, 76.26it/s]Evaluating:  94%|█████████▍| 405/432 [00:04<00:00, 75.60it/s]Evaluating:  96%|█████████▌| 413/432 [00:04<00:00, 75.21it/s]Evaluating:  97%|█████████▋| 421/432 [00:04<00:00, 74.69it/s]Evaluating:  99%|█████████▉| 429/432 [00:04<00:00, 74.34it/s]Evaluating: 100%|██████████| 432/432 [00:05<00:00, 86.21it/s]
05/05/2022 20:07:46 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 20:07:46 - INFO - __main__ -     f1 = 0.8959943780744904
05/05/2022 20:07:46 - INFO - __main__ -     loss = 0.21822071801305912
05/05/2022 20:07:46 - INFO - __main__ -     precision = 0.8885017421602788
05/05/2022 20:07:46 - INFO - __main__ -     recall = 0.9036144578313253
05/05/2022 20:07:46 - INFO - Distillation -   Epoch 15 finished
05/05/2022 20:07:46 - INFO - Distillation -   Epoch 16
05/05/2022 20:07:46 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 20:07:47 - INFO - Distillation -   Global step: 6594, epoch step:9
05/05/2022 20:07:51 - INFO - Distillation -   Global step: 6615, epoch step:30
05/05/2022 20:07:55 - INFO - Distillation -   Global step: 6636, epoch step:51
05/05/2022 20:07:59 - INFO - Distillation -   Global step: 6657, epoch step:72
05/05/2022 20:08:02 - INFO - Distillation -   Global step: 6678, epoch step:93
05/05/2022 20:08:06 - INFO - Distillation -   Global step: 6699, epoch step:114
05/05/2022 20:08:10 - INFO - Distillation -   Global step: 6720, epoch step:135
05/05/2022 20:08:14 - INFO - Distillation -   Global step: 6741, epoch step:156
05/05/2022 20:08:18 - INFO - Distillation -   Global step: 6762, epoch step:177
05/05/2022 20:08:22 - INFO - Distillation -   Global step: 6783, epoch step:198
05/05/2022 20:08:25 - INFO - Distillation -   Global step: 6804, epoch step:219
05/05/2022 20:08:29 - INFO - Distillation -   Global step: 6825, epoch step:240
05/05/2022 20:08:33 - INFO - Distillation -   Global step: 6846, epoch step:261
05/05/2022 20:08:37 - INFO - Distillation -   Global step: 6867, epoch step:282
05/05/2022 20:08:41 - INFO - Distillation -   Global step: 6888, epoch step:303
05/05/2022 20:08:44 - INFO - Distillation -   Global step: 6909, epoch step:324
05/05/2022 20:08:48 - INFO - Distillation -   Global step: 6930, epoch step:345
05/05/2022 20:08:52 - INFO - Distillation -   Global step: 6951, epoch step:366
05/05/2022 20:08:56 - INFO - Distillation -   Global step: 6972, epoch step:387
05/05/2022 20:09:00 - INFO - Distillation -   Global step: 6993, epoch step:408
05/05/2022 20:09:04 - INFO - Distillation -   Global step: 7014, epoch step:429
05/05/2022 20:09:05 - INFO - Distillation -   Saving at global step 7024, epoch step 439 epoch 16
05/05/2022 20:09:07 - INFO - Distillation -   Running callback function...
05/05/2022 20:09:07 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 20:09:07 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 20:09:07 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 20:09:07 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 20:09:07 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 20:09:07 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 20:09:08 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 20:09:08 - INFO - __main__ -     Num examples = 3453
05/05/2022 20:09:08 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 102.71it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:03, 102.93it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 102.52it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 101.97it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 101.14it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 99.64it/s] Evaluating:  18%|█▊        | 76/432 [00:00<00:03, 98.80it/s]Evaluating:  20%|█▉        | 86/432 [00:00<00:03, 98.17it/s]Evaluating:  22%|██▏       | 96/432 [00:00<00:03, 97.61it/s]Evaluating:  25%|██▍       | 106/432 [00:01<00:03, 96.91it/s]Evaluating:  27%|██▋       | 116/432 [00:01<00:03, 95.83it/s]Evaluating:  29%|██▉       | 126/432 [00:01<00:03, 94.22it/s]Evaluating:  31%|███▏      | 136/432 [00:01<00:03, 92.75it/s]Evaluating:  34%|███▍      | 146/432 [00:01<00:03, 91.62it/s]Evaluating:  36%|███▌      | 156/432 [00:01<00:03, 90.79it/s]Evaluating:  38%|███▊      | 166/432 [00:01<00:02, 90.08it/s]Evaluating:  41%|████      | 176/432 [00:01<00:02, 89.65it/s]Evaluating:  43%|████▎     | 185/432 [00:01<00:02, 88.88it/s]Evaluating:  45%|████▍     | 194/432 [00:02<00:02, 88.16it/s]Evaluating:  47%|████▋     | 203/432 [00:02<00:02, 87.62it/s]Evaluating:  49%|████▉     | 212/432 [00:02<00:02, 86.84it/s]Evaluating:  51%|█████     | 221/432 [00:02<00:02, 86.31it/s]Evaluating:  53%|█████▎    | 230/432 [00:02<00:02, 85.70it/s]Evaluating:  55%|█████▌    | 239/432 [00:02<00:02, 85.06it/s]Evaluating:  57%|█████▋    | 248/432 [00:02<00:02, 84.58it/s]Evaluating:  59%|█████▉    | 257/432 [00:02<00:02, 84.21it/s]Evaluating:  62%|██████▏   | 266/432 [00:02<00:01, 83.37it/s]Evaluating:  64%|██████▎   | 275/432 [00:03<00:01, 82.65it/s]Evaluating:  66%|██████▌   | 284/432 [00:03<00:01, 82.27it/s]Evaluating:  68%|██████▊   | 293/432 [00:03<00:01, 81.83it/s]Evaluating:  70%|██████▉   | 302/432 [00:03<00:01, 81.31it/s]Evaluating:  72%|███████▏  | 311/432 [00:03<00:01, 80.66it/s]Evaluating:  74%|███████▍  | 320/432 [00:03<00:01, 79.98it/s]Evaluating:  76%|███████▌  | 329/432 [00:03<00:01, 79.61it/s]Evaluating:  78%|███████▊  | 337/432 [00:03<00:01, 79.07it/s]Evaluating:  80%|███████▉  | 345/432 [00:03<00:01, 78.52it/s]Evaluating:  82%|████████▏ | 353/432 [00:04<00:01, 77.86it/s]Evaluating:  84%|████████▎ | 361/432 [00:04<00:00, 77.65it/s]Evaluating:  85%|████████▌ | 369/432 [00:04<00:00, 77.40it/s]Evaluating:  87%|████████▋ | 377/432 [00:04<00:00, 76.96it/s]Evaluating:  89%|████████▉ | 385/432 [00:04<00:00, 76.30it/s]Evaluating:  91%|█████████ | 393/432 [00:04<00:00, 75.83it/s]Evaluating:  93%|█████████▎| 401/432 [00:04<00:00, 75.54it/s]Evaluating:  95%|█████████▍| 409/432 [00:04<00:00, 75.28it/s]Evaluating:  97%|█████████▋| 417/432 [00:04<00:00, 74.76it/s]Evaluating:  98%|█████████▊| 425/432 [00:04<00:00, 74.17it/s]Evaluating: 100%|██████████| 432/432 [00:05<00:00, 85.28it/s]
05/05/2022 20:09:13 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 20:09:13 - INFO - __main__ -     f1 = 0.8981026001405481
05/05/2022 20:09:13 - INFO - __main__ -     loss = 0.22172889191289483
05/05/2022 20:09:13 - INFO - __main__ -     precision = 0.8905923344947735
05/05/2022 20:09:13 - INFO - __main__ -     recall = 0.9057406094968108
05/05/2022 20:09:13 - INFO - Distillation -   Epoch 16 finished
05/05/2022 20:09:13 - INFO - Distillation -   Epoch 17
05/05/2022 20:09:13 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 20:09:15 - INFO - Distillation -   Global step: 7035, epoch step:11
05/05/2022 20:09:19 - INFO - Distillation -   Global step: 7056, epoch step:32
05/05/2022 20:09:23 - INFO - Distillation -   Global step: 7077, epoch step:53
05/05/2022 20:09:27 - INFO - Distillation -   Global step: 7098, epoch step:74
05/05/2022 20:09:31 - INFO - Distillation -   Global step: 7119, epoch step:95
05/05/2022 20:09:34 - INFO - Distillation -   Global step: 7140, epoch step:116
05/05/2022 20:09:38 - INFO - Distillation -   Global step: 7161, epoch step:137
05/05/2022 20:09:42 - INFO - Distillation -   Global step: 7182, epoch step:158
05/05/2022 20:09:46 - INFO - Distillation -   Global step: 7203, epoch step:179
05/05/2022 20:09:50 - INFO - Distillation -   Global step: 7224, epoch step:200
05/05/2022 20:09:54 - INFO - Distillation -   Global step: 7245, epoch step:221
05/05/2022 20:09:57 - INFO - Distillation -   Global step: 7266, epoch step:242
05/05/2022 20:10:01 - INFO - Distillation -   Global step: 7287, epoch step:263
05/05/2022 20:10:05 - INFO - Distillation -   Global step: 7308, epoch step:284
05/05/2022 20:10:09 - INFO - Distillation -   Global step: 7329, epoch step:305
05/05/2022 20:10:13 - INFO - Distillation -   Global step: 7350, epoch step:326
05/05/2022 20:10:16 - INFO - Distillation -   Global step: 7371, epoch step:347
05/05/2022 20:10:20 - INFO - Distillation -   Global step: 7392, epoch step:368
05/05/2022 20:10:24 - INFO - Distillation -   Global step: 7413, epoch step:389
05/05/2022 20:10:28 - INFO - Distillation -   Global step: 7434, epoch step:410
05/05/2022 20:10:32 - INFO - Distillation -   Global step: 7455, epoch step:431
05/05/2022 20:10:33 - INFO - Distillation -   Saving at global step 7463, epoch step 439 epoch 17
05/05/2022 20:10:35 - INFO - Distillation -   Running callback function...
05/05/2022 20:10:35 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 20:10:35 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 20:10:35 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 20:10:35 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 20:10:35 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 20:10:35 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 20:10:35 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 20:10:35 - INFO - __main__ -     Num examples = 3453
05/05/2022 20:10:35 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 102.61it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:04, 102.00it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 101.72it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 100.98it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 99.79it/s] Evaluating:  15%|█▌        | 65/432 [00:00<00:03, 99.46it/s]Evaluating:  17%|█▋        | 75/432 [00:00<00:03, 99.34it/s]Evaluating:  20%|█▉        | 85/432 [00:00<00:03, 99.52it/s]Evaluating:  22%|██▏       | 95/432 [00:00<00:03, 99.14it/s]Evaluating:  24%|██▍       | 105/432 [00:01<00:03, 98.58it/s]Evaluating:  27%|██▋       | 115/432 [00:01<00:03, 97.61it/s]Evaluating:  29%|██▉       | 125/432 [00:01<00:03, 96.67it/s]Evaluating:  31%|███▏      | 135/432 [00:01<00:03, 95.68it/s]Evaluating:  34%|███▎      | 145/432 [00:01<00:03, 94.48it/s]Evaluating:  36%|███▌      | 155/432 [00:01<00:02, 93.48it/s]Evaluating:  38%|███▊      | 165/432 [00:01<00:02, 92.64it/s]Evaluating:  41%|████      | 175/432 [00:01<00:02, 91.63it/s]Evaluating:  43%|████▎     | 185/432 [00:01<00:02, 90.73it/s]Evaluating:  45%|████▌     | 195/432 [00:02<00:02, 89.74it/s]Evaluating:  47%|████▋     | 204/432 [00:02<00:02, 88.86it/s]Evaluating:  49%|████▉     | 213/432 [00:02<00:02, 88.15it/s]Evaluating:  51%|█████▏    | 222/432 [00:02<00:02, 87.47it/s]Evaluating:  53%|█████▎    | 231/432 [00:02<00:02, 86.53it/s]Evaluating:  56%|█████▌    | 240/432 [00:02<00:02, 85.48it/s]Evaluating:  58%|█████▊    | 249/432 [00:02<00:02, 84.97it/s]Evaluating:  60%|█████▉    | 258/432 [00:02<00:02, 84.54it/s]Evaluating:  62%|██████▏   | 267/432 [00:02<00:01, 83.94it/s]Evaluating:  64%|██████▍   | 276/432 [00:02<00:01, 83.52it/s]Evaluating:  66%|██████▌   | 285/432 [00:03<00:01, 83.00it/s]Evaluating:  68%|██████▊   | 294/432 [00:03<00:01, 82.49it/s]Evaluating:  70%|███████   | 303/432 [00:03<00:01, 82.04it/s]Evaluating:  72%|███████▏  | 312/432 [00:03<00:01, 81.57it/s]Evaluating:  74%|███████▍  | 321/432 [00:03<00:01, 80.72it/s]Evaluating:  76%|███████▋  | 330/432 [00:03<00:01, 80.35it/s]Evaluating:  78%|███████▊  | 339/432 [00:03<00:01, 80.07it/s]Evaluating:  81%|████████  | 348/432 [00:03<00:01, 79.71it/s]Evaluating:  82%|████████▏ | 356/432 [00:04<00:00, 79.30it/s]Evaluating:  84%|████████▍ | 364/432 [00:04<00:00, 78.48it/s]Evaluating:  86%|████████▌ | 372/432 [00:04<00:00, 77.86it/s]Evaluating:  88%|████████▊ | 380/432 [00:04<00:00, 77.54it/s]Evaluating:  90%|████████▉ | 388/432 [00:04<00:00, 77.20it/s]Evaluating:  92%|█████████▏| 396/432 [00:04<00:00, 76.76it/s]Evaluating:  94%|█████████▎| 404/432 [00:04<00:00, 76.05it/s]Evaluating:  95%|█████████▌| 412/432 [00:04<00:00, 75.71it/s]Evaluating:  97%|█████████▋| 420/432 [00:04<00:00, 75.28it/s]Evaluating:  99%|█████████▉| 428/432 [00:04<00:00, 74.87it/s]Evaluating: 100%|██████████| 432/432 [00:05<00:00, 86.29it/s]
05/05/2022 20:10:41 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 20:10:41 - INFO - __main__ -     f1 = 0.8976474719101124
05/05/2022 20:10:41 - INFO - __main__ -     loss = 0.21822676197868524
05/05/2022 20:10:41 - INFO - __main__ -     precision = 0.889526791927627
05/05/2022 20:10:41 - INFO - __main__ -     recall = 0.9059177888022679
05/05/2022 20:10:41 - INFO - Distillation -   Epoch 17 finished
05/05/2022 20:10:41 - INFO - Distillation -   Epoch 18
05/05/2022 20:10:41 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 20:10:43 - INFO - Distillation -   Global step: 7476, epoch step:13
05/05/2022 20:10:47 - INFO - Distillation -   Global step: 7497, epoch step:34
05/05/2022 20:10:51 - INFO - Distillation -   Global step: 7518, epoch step:55
05/05/2022 20:10:55 - INFO - Distillation -   Global step: 7539, epoch step:76
05/05/2022 20:10:59 - INFO - Distillation -   Global step: 7560, epoch step:97
05/05/2022 20:11:02 - INFO - Distillation -   Global step: 7581, epoch step:118
05/05/2022 20:11:06 - INFO - Distillation -   Global step: 7602, epoch step:139
05/05/2022 20:11:10 - INFO - Distillation -   Global step: 7623, epoch step:160
05/05/2022 20:11:14 - INFO - Distillation -   Global step: 7644, epoch step:181
05/05/2022 20:11:18 - INFO - Distillation -   Global step: 7665, epoch step:202
05/05/2022 20:11:22 - INFO - Distillation -   Global step: 7686, epoch step:223
05/05/2022 20:11:25 - INFO - Distillation -   Global step: 7707, epoch step:244
05/05/2022 20:11:29 - INFO - Distillation -   Global step: 7728, epoch step:265
05/05/2022 20:11:33 - INFO - Distillation -   Global step: 7749, epoch step:286
05/05/2022 20:11:37 - INFO - Distillation -   Global step: 7770, epoch step:307
05/05/2022 20:11:41 - INFO - Distillation -   Global step: 7791, epoch step:328
05/05/2022 20:11:44 - INFO - Distillation -   Global step: 7812, epoch step:349
05/05/2022 20:11:48 - INFO - Distillation -   Global step: 7833, epoch step:370
05/05/2022 20:11:52 - INFO - Distillation -   Global step: 7854, epoch step:391
05/05/2022 20:11:56 - INFO - Distillation -   Global step: 7875, epoch step:412
05/05/2022 20:12:00 - INFO - Distillation -   Global step: 7896, epoch step:433
05/05/2022 20:12:01 - INFO - Distillation -   Saving at global step 7902, epoch step 439 epoch 18
05/05/2022 20:12:03 - INFO - Distillation -   Running callback function...
05/05/2022 20:12:03 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 20:12:03 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 20:12:03 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 20:12:03 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 20:12:03 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 20:12:03 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 20:12:03 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 20:12:03 - INFO - __main__ -     Num examples = 3453
05/05/2022 20:12:03 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 103.39it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:03, 102.93it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 102.33it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 101.75it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 101.21it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 100.54it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 100.12it/s]Evaluating:  20%|██        | 88/432 [00:00<00:03, 99.42it/s] Evaluating:  23%|██▎       | 98/432 [00:00<00:03, 98.87it/s]Evaluating:  25%|██▌       | 108/432 [00:01<00:03, 98.03it/s]Evaluating:  27%|██▋       | 118/432 [00:01<00:03, 97.39it/s]Evaluating:  30%|██▉       | 128/432 [00:01<00:03, 96.09it/s]Evaluating:  32%|███▏      | 138/432 [00:01<00:03, 94.86it/s]Evaluating:  34%|███▍      | 148/432 [00:01<00:03, 93.36it/s]Evaluating:  37%|███▋      | 158/432 [00:01<00:02, 92.80it/s]Evaluating:  39%|███▉      | 168/432 [00:01<00:02, 92.36it/s]Evaluating:  41%|████      | 178/432 [00:01<00:02, 91.98it/s]Evaluating:  44%|████▎     | 188/432 [00:01<00:02, 91.37it/s]Evaluating:  46%|████▌     | 198/432 [00:02<00:02, 90.61it/s]Evaluating:  48%|████▊     | 208/432 [00:02<00:02, 89.67it/s]Evaluating:  50%|█████     | 217/432 [00:02<00:02, 88.81it/s]Evaluating:  52%|█████▏    | 226/432 [00:02<00:02, 88.01it/s]Evaluating:  54%|█████▍    | 235/432 [00:02<00:02, 87.24it/s]Evaluating:  56%|█████▋    | 244/432 [00:02<00:02, 86.40it/s]Evaluating:  59%|█████▊    | 253/432 [00:02<00:02, 85.57it/s]Evaluating:  61%|██████    | 262/432 [00:02<00:02, 84.67it/s]Evaluating:  63%|██████▎   | 271/432 [00:02<00:01, 84.06it/s]Evaluating:  65%|██████▍   | 280/432 [00:03<00:01, 83.79it/s]Evaluating:  67%|██████▋   | 289/432 [00:03<00:01, 83.40it/s]Evaluating:  69%|██████▉   | 298/432 [00:03<00:01, 82.90it/s]Evaluating:  71%|███████   | 307/432 [00:03<00:01, 82.27it/s]Evaluating:  73%|███████▎  | 316/432 [00:03<00:01, 81.66it/s]Evaluating:  75%|███████▌  | 325/432 [00:03<00:01, 81.04it/s]Evaluating:  77%|███████▋  | 334/432 [00:03<00:01, 80.26it/s]Evaluating:  79%|███████▉  | 343/432 [00:03<00:01, 79.88it/s]Evaluating:  81%|████████▏ | 351/432 [00:03<00:01, 79.24it/s]Evaluating:  83%|████████▎ | 359/432 [00:04<00:00, 78.85it/s]Evaluating:  85%|████████▍ | 367/432 [00:04<00:00, 78.48it/s]Evaluating:  87%|████████▋ | 375/432 [00:04<00:00, 77.97it/s]Evaluating:  89%|████████▊ | 383/432 [00:04<00:00, 77.45it/s]Evaluating:  91%|█████████ | 391/432 [00:04<00:00, 77.18it/s]Evaluating:  92%|█████████▏| 399/432 [00:04<00:00, 77.02it/s]Evaluating:  94%|█████████▍| 407/432 [00:04<00:00, 76.68it/s]Evaluating:  96%|█████████▌| 415/432 [00:04<00:00, 76.06it/s]Evaluating:  98%|█████████▊| 423/432 [00:04<00:00, 75.59it/s]Evaluating: 100%|█████████▉| 431/432 [00:04<00:00, 75.13it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 86.70it/s]
05/05/2022 20:12:09 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 20:12:09 - INFO - __main__ -     f1 = 0.9009500351864884
05/05/2022 20:12:09 - INFO - __main__ -     loss = 0.21486168861246016
05/05/2022 20:12:09 - INFO - __main__ -     precision = 0.8946540880503144
05/05/2022 20:12:09 - INFO - __main__ -     recall = 0.9073352232459249
05/05/2022 20:12:09 - INFO - Distillation -   Epoch 18 finished
05/05/2022 20:12:09 - INFO - Distillation -   Epoch 19
05/05/2022 20:12:09 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 20:12:11 - INFO - Distillation -   Global step: 7917, epoch step:15
05/05/2022 20:12:15 - INFO - Distillation -   Global step: 7938, epoch step:36
05/05/2022 20:12:19 - INFO - Distillation -   Global step: 7959, epoch step:57
05/05/2022 20:12:23 - INFO - Distillation -   Global step: 7980, epoch step:78
05/05/2022 20:12:27 - INFO - Distillation -   Global step: 8001, epoch step:99
05/05/2022 20:12:30 - INFO - Distillation -   Global step: 8022, epoch step:120
05/05/2022 20:12:34 - INFO - Distillation -   Global step: 8043, epoch step:141
05/05/2022 20:12:38 - INFO - Distillation -   Global step: 8064, epoch step:162
05/05/2022 20:12:42 - INFO - Distillation -   Global step: 8085, epoch step:183
05/05/2022 20:12:46 - INFO - Distillation -   Global step: 8106, epoch step:204
05/05/2022 20:12:49 - INFO - Distillation -   Global step: 8127, epoch step:225
05/05/2022 20:12:53 - INFO - Distillation -   Global step: 8148, epoch step:246
05/05/2022 20:12:57 - INFO - Distillation -   Global step: 8169, epoch step:267
05/05/2022 20:13:01 - INFO - Distillation -   Global step: 8190, epoch step:288
05/05/2022 20:13:05 - INFO - Distillation -   Global step: 8211, epoch step:309
05/05/2022 20:13:08 - INFO - Distillation -   Global step: 8232, epoch step:330
05/05/2022 20:13:12 - INFO - Distillation -   Global step: 8253, epoch step:351
05/05/2022 20:13:16 - INFO - Distillation -   Global step: 8274, epoch step:372
05/05/2022 20:13:20 - INFO - Distillation -   Global step: 8295, epoch step:393
05/05/2022 20:13:24 - INFO - Distillation -   Global step: 8316, epoch step:414
05/05/2022 20:13:27 - INFO - Distillation -   Global step: 8337, epoch step:435
05/05/2022 20:13:28 - INFO - Distillation -   Saving at global step 8341, epoch step 439 epoch 19
05/05/2022 20:13:30 - INFO - Distillation -   Running callback function...
05/05/2022 20:13:30 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 20:13:30 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 20:13:30 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 20:13:30 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 20:13:30 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 20:13:30 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 20:13:30 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 20:13:30 - INFO - __main__ -     Num examples = 3453
05/05/2022 20:13:30 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 103.56it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:03, 103.16it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 102.82it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 102.38it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 101.26it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 100.59it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 99.84it/s] Evaluating:  20%|██        | 87/432 [00:00<00:03, 99.00it/s]Evaluating:  22%|██▏       | 97/432 [00:00<00:03, 98.52it/s]Evaluating:  25%|██▍       | 107/432 [00:01<00:03, 97.95it/s]Evaluating:  27%|██▋       | 117/432 [00:01<00:03, 97.13it/s]Evaluating:  29%|██▉       | 127/432 [00:01<00:03, 96.64it/s]Evaluating:  32%|███▏      | 137/432 [00:01<00:03, 95.68it/s]Evaluating:  34%|███▍      | 147/432 [00:01<00:03, 94.82it/s]Evaluating:  36%|███▋      | 157/432 [00:01<00:02, 93.90it/s]Evaluating:  39%|███▊      | 167/432 [00:01<00:02, 92.93it/s]Evaluating:  41%|████      | 177/432 [00:01<00:02, 92.15it/s]Evaluating:  43%|████▎     | 187/432 [00:01<00:02, 91.31it/s]Evaluating:  46%|████▌     | 197/432 [00:02<00:02, 90.55it/s]Evaluating:  48%|████▊     | 207/432 [00:02<00:02, 89.48it/s]Evaluating:  50%|█████     | 216/432 [00:02<00:02, 88.10it/s]Evaluating:  52%|█████▏    | 225/432 [00:02<00:02, 86.77it/s]Evaluating:  54%|█████▍    | 234/432 [00:02<00:02, 85.87it/s]Evaluating:  56%|█████▋    | 243/432 [00:02<00:02, 85.49it/s]Evaluating:  58%|█████▊    | 252/432 [00:02<00:02, 85.05it/s]Evaluating:  60%|██████    | 261/432 [00:02<00:02, 84.47it/s]Evaluating:  62%|██████▎   | 270/432 [00:02<00:01, 83.83it/s]Evaluating:  65%|██████▍   | 279/432 [00:03<00:01, 83.29it/s]Evaluating:  67%|██████▋   | 288/432 [00:03<00:01, 82.72it/s]Evaluating:  69%|██████▉   | 297/432 [00:03<00:01, 82.27it/s]Evaluating:  71%|███████   | 306/432 [00:03<00:01, 82.12it/s]Evaluating:  73%|███████▎  | 315/432 [00:03<00:01, 81.86it/s]Evaluating:  75%|███████▌  | 324/432 [00:03<00:01, 81.52it/s]Evaluating:  77%|███████▋  | 333/432 [00:03<00:01, 81.05it/s]Evaluating:  79%|███████▉  | 342/432 [00:03<00:01, 80.41it/s]Evaluating:  81%|████████▏ | 351/432 [00:03<00:01, 79.37it/s]Evaluating:  83%|████████▎ | 359/432 [00:04<00:00, 78.68it/s]Evaluating:  85%|████████▍ | 367/432 [00:04<00:00, 78.41it/s]Evaluating:  87%|████████▋ | 375/432 [00:04<00:00, 78.05it/s]Evaluating:  89%|████████▊ | 383/432 [00:04<00:00, 77.84it/s]Evaluating:  91%|█████████ | 391/432 [00:04<00:00, 77.35it/s]Evaluating:  92%|█████████▏| 399/432 [00:04<00:00, 76.96it/s]Evaluating:  94%|█████████▍| 407/432 [00:04<00:00, 76.51it/s]Evaluating:  96%|█████████▌| 415/432 [00:04<00:00, 76.20it/s]Evaluating:  98%|█████████▊| 423/432 [00:04<00:00, 75.94it/s]Evaluating: 100%|█████████▉| 431/432 [00:04<00:00, 75.36it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 86.66it/s]
05/05/2022 20:13:36 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 20:13:36 - INFO - __main__ -     f1 = 0.9003255036509193
05/05/2022 20:13:36 - INFO - __main__ -     loss = 0.21237732202337997
05/05/2022 20:13:36 - INFO - __main__ -     precision = 0.8941114799930107
05/05/2022 20:13:36 - INFO - __main__ -     recall = 0.9066265060240963
05/05/2022 20:13:36 - INFO - Distillation -   Epoch 19 finished
05/05/2022 20:13:36 - INFO - Distillation -   Epoch 20
05/05/2022 20:13:36 - INFO - Distillation -   Length of current epoch in forward batch: 439
05/05/2022 20:13:39 - INFO - Distillation -   Global step: 8358, epoch step:17
05/05/2022 20:13:43 - INFO - Distillation -   Global step: 8379, epoch step:38
05/05/2022 20:13:47 - INFO - Distillation -   Global step: 8400, epoch step:59
05/05/2022 20:13:51 - INFO - Distillation -   Global step: 8421, epoch step:80
05/05/2022 20:13:54 - INFO - Distillation -   Global step: 8442, epoch step:101
05/05/2022 20:13:58 - INFO - Distillation -   Global step: 8463, epoch step:122
05/05/2022 20:14:02 - INFO - Distillation -   Global step: 8484, epoch step:143
05/05/2022 20:14:06 - INFO - Distillation -   Global step: 8505, epoch step:164
05/05/2022 20:14:10 - INFO - Distillation -   Global step: 8526, epoch step:185
05/05/2022 20:14:13 - INFO - Distillation -   Global step: 8547, epoch step:206
05/05/2022 20:14:17 - INFO - Distillation -   Global step: 8568, epoch step:227
05/05/2022 20:14:21 - INFO - Distillation -   Global step: 8589, epoch step:248
05/05/2022 20:14:25 - INFO - Distillation -   Global step: 8610, epoch step:269
05/05/2022 20:14:29 - INFO - Distillation -   Global step: 8631, epoch step:290
05/05/2022 20:14:32 - INFO - Distillation -   Global step: 8652, epoch step:311
05/05/2022 20:14:36 - INFO - Distillation -   Global step: 8673, epoch step:332
05/05/2022 20:14:40 - INFO - Distillation -   Global step: 8694, epoch step:353
05/05/2022 20:14:44 - INFO - Distillation -   Global step: 8715, epoch step:374
05/05/2022 20:14:48 - INFO - Distillation -   Global step: 8736, epoch step:395
05/05/2022 20:14:52 - INFO - Distillation -   Global step: 8757, epoch step:416
05/05/2022 20:14:55 - INFO - Distillation -   Global step: 8778, epoch step:437
05/05/2022 20:14:56 - INFO - Distillation -   Saving at global step 8780, epoch step 439 epoch 20
05/05/2022 20:14:58 - INFO - Distillation -   Running callback function...
05/05/2022 20:14:58 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/teacher' is a path or url to a directory containing tokenizer files.
05/05/2022 20:14:58 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/vocab.txt
05/05/2022 20:14:58 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/added_tokens.json
05/05/2022 20:14:58 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/special_tokens_map.json
05/05/2022 20:14:58 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/teacher/tokenizer_config.json
05/05/2022 20:14:58 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 20:14:58 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 20:14:58 - INFO - __main__ -     Num examples = 3453
05/05/2022 20:14:58 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 102.52it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:04, 102.46it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 102.52it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 102.19it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 101.83it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 101.32it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 100.72it/s]Evaluating:  20%|██        | 88/432 [00:00<00:03, 100.32it/s]Evaluating:  23%|██▎       | 99/432 [00:00<00:03, 99.71it/s] Evaluating:  25%|██▌       | 109/432 [00:01<00:03, 98.80it/s]Evaluating:  28%|██▊       | 119/432 [00:01<00:03, 97.43it/s]Evaluating:  30%|██▉       | 129/432 [00:01<00:03, 96.67it/s]Evaluating:  32%|███▏      | 139/432 [00:01<00:03, 95.71it/s]Evaluating:  34%|███▍      | 149/432 [00:01<00:02, 94.86it/s]Evaluating:  37%|███▋      | 159/432 [00:01<00:02, 93.81it/s]Evaluating:  39%|███▉      | 169/432 [00:01<00:02, 93.10it/s]Evaluating:  41%|████▏     | 179/432 [00:01<00:02, 92.15it/s]Evaluating:  44%|████▍     | 189/432 [00:01<00:02, 91.52it/s]Evaluating:  46%|████▌     | 199/432 [00:02<00:02, 90.52it/s]Evaluating:  48%|████▊     | 209/432 [00:02<00:02, 89.36it/s]Evaluating:  50%|█████     | 218/432 [00:02<00:02, 88.80it/s]Evaluating:  53%|█████▎    | 227/432 [00:02<00:02, 88.12it/s]Evaluating:  55%|█████▍    | 236/432 [00:02<00:02, 87.41it/s]Evaluating:  57%|█████▋    | 245/432 [00:02<00:02, 86.46it/s]Evaluating:  59%|█████▉    | 254/432 [00:02<00:02, 85.62it/s]Evaluating:  61%|██████    | 263/432 [00:02<00:01, 84.91it/s]Evaluating:  63%|██████▎   | 272/432 [00:02<00:01, 84.43it/s]Evaluating:  65%|██████▌   | 281/432 [00:03<00:01, 83.71it/s]Evaluating:  67%|██████▋   | 290/432 [00:03<00:01, 82.99it/s]Evaluating:  69%|██████▉   | 299/432 [00:03<00:01, 82.30it/s]Evaluating:  71%|███████▏  | 308/432 [00:03<00:01, 81.81it/s]Evaluating:  73%|███████▎  | 317/432 [00:03<00:01, 81.15it/s]Evaluating:  75%|███████▌  | 326/432 [00:03<00:01, 80.90it/s]Evaluating:  78%|███████▊  | 335/432 [00:03<00:01, 80.49it/s]Evaluating:  80%|███████▉  | 344/432 [00:03<00:01, 80.34it/s]Evaluating:  82%|████████▏ | 353/432 [00:03<00:00, 80.16it/s]Evaluating:  84%|████████▍ | 362/432 [00:04<00:00, 79.74it/s]Evaluating:  86%|████████▌ | 370/432 [00:04<00:00, 79.35it/s]Evaluating:  88%|████████▊ | 378/432 [00:04<00:00, 78.76it/s]Evaluating:  89%|████████▉ | 386/432 [00:04<00:00, 78.32it/s]Evaluating:  91%|█████████ | 394/432 [00:04<00:00, 77.89it/s]Evaluating:  93%|█████████▎| 402/432 [00:04<00:00, 77.24it/s]Evaluating:  95%|█████████▍| 410/432 [00:04<00:00, 76.60it/s]Evaluating:  97%|█████████▋| 418/432 [00:04<00:00, 76.21it/s]Evaluating:  99%|█████████▊| 426/432 [00:04<00:00, 75.83it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 87.09it/s]
05/05/2022 20:15:04 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 20:15:04 - INFO - __main__ -     f1 = 0.9002022333597117
05/05/2022 20:15:04 - INFO - __main__ -     loss = 0.2127702559990915
05/05/2022 20:15:04 - INFO - __main__ -     precision = 0.8935241752487345
05/05/2022 20:15:04 - INFO - __main__ -     recall = 0.9069808646350106
05/05/2022 20:15:04 - INFO - Distillation -   Epoch 20 finished
05/05/2022 20:15:04 - INFO - __main__ -   Saving model checkpoint to /home/hs3228/TextBrewer/output_model/conll_distill/student_layer5
05/05/2022 20:15:04 - INFO - transformers.configuration_utils -   Configuration saved in /home/hs3228/TextBrewer/output_model/conll_distill/student_layer5/config.json
05/05/2022 20:15:05 - INFO - transformers.modeling_utils -   Model weights saved in /home/hs3228/TextBrewer/output_model/conll_distill/student_layer5/pytorch_model.bin
05/05/2022 20:15:06 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/student_layer5' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/student_layer5' is a path or url to a directory containing tokenizer files.
05/05/2022 20:15:06 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/student_layer5/vocab.txt
05/05/2022 20:15:06 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/student_layer5/added_tokens.json
05/05/2022 20:15:06 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/student_layer5/special_tokens_map.json
05/05/2022 20:15:06 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/student_layer5/tokenizer_config.json
05/05/2022 20:15:06 - INFO - __main__ -   Evaluate the following checkpoints: ['/home/hs3228/TextBrewer/output_model/conll_distill/student_layer5']
05/05/2022 20:15:06 - INFO - transformers.configuration_utils -   loading configuration file /home/hs3228/TextBrewer/output_model/conll_distill/student_layer5/config.json
05/05/2022 20:15:06 - INFO - transformers.configuration_utils -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "num_labels": 9,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

05/05/2022 20:15:06 - INFO - transformers.modeling_utils -   loading weights file /home/hs3228/TextBrewer/output_model/conll_distill/student_layer5/pytorch_model.bin
05/05/2022 20:15:07 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_dev_teacher_128
05/05/2022 20:15:07 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 20:15:07 - INFO - __main__ -     Num examples = 3250
05/05/2022 20:15:07 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/407 [00:00<?, ?it/s]Evaluating:   2%|▏         | 10/407 [00:00<00:04, 93.58it/s]Evaluating:   5%|▍         | 20/407 [00:00<00:04, 96.28it/s]Evaluating:   8%|▊         | 31/407 [00:00<00:03, 98.18it/s]Evaluating:  10%|█         | 42/407 [00:00<00:03, 99.43it/s]Evaluating:  13%|█▎        | 53/407 [00:00<00:03, 99.86it/s]Evaluating:  16%|█▌        | 64/407 [00:00<00:03, 99.95it/s]Evaluating:  18%|█▊        | 74/407 [00:00<00:03, 99.64it/s]Evaluating:  21%|██        | 84/407 [00:00<00:03, 99.01it/s]Evaluating:  23%|██▎       | 94/407 [00:00<00:03, 98.49it/s]Evaluating:  26%|██▌       | 104/407 [00:01<00:03, 98.50it/s]Evaluating:  28%|██▊       | 114/407 [00:01<00:02, 98.08it/s]Evaluating:  30%|███       | 124/407 [00:01<00:02, 97.95it/s]Evaluating:  33%|███▎      | 134/407 [00:01<00:02, 97.28it/s]Evaluating:  35%|███▌      | 144/407 [00:01<00:02, 96.44it/s]Evaluating:  38%|███▊      | 154/407 [00:01<00:02, 95.69it/s]Evaluating:  40%|████      | 164/407 [00:01<00:02, 94.86it/s]Evaluating:  43%|████▎     | 174/407 [00:01<00:02, 94.25it/s]Evaluating:  45%|████▌     | 184/407 [00:01<00:02, 93.54it/s]Evaluating:  48%|████▊     | 194/407 [00:02<00:02, 92.51it/s]Evaluating:  50%|█████     | 204/407 [00:02<00:02, 91.86it/s]Evaluating:  53%|█████▎    | 214/407 [00:02<00:02, 90.99it/s]Evaluating:  55%|█████▌    | 224/407 [00:02<00:02, 90.14it/s]Evaluating:  57%|█████▋    | 234/407 [00:02<00:01, 88.84it/s]Evaluating:  60%|█████▉    | 243/407 [00:02<00:01, 88.06it/s]Evaluating:  62%|██████▏   | 252/407 [00:02<00:01, 87.56it/s]Evaluating:  64%|██████▍   | 261/407 [00:02<00:01, 86.75it/s]Evaluating:  66%|██████▋   | 270/407 [00:02<00:01, 85.96it/s]Evaluating:  69%|██████▊   | 279/407 [00:02<00:01, 85.22it/s]Evaluating:  71%|███████   | 288/407 [00:03<00:01, 84.29it/s]Evaluating:  73%|███████▎  | 297/407 [00:03<00:01, 83.58it/s]Evaluating:  75%|███████▌  | 306/407 [00:03<00:01, 83.18it/s]Evaluating:  77%|███████▋  | 315/407 [00:03<00:01, 82.62it/s]Evaluating:  80%|███████▉  | 324/407 [00:03<00:01, 79.86it/s]Evaluating:  82%|████████▏ | 333/407 [00:03<00:00, 79.89it/s]Evaluating:  84%|████████▍ | 342/407 [00:03<00:00, 79.73it/s]Evaluating:  86%|████████▌ | 350/407 [00:03<00:00, 79.47it/s]Evaluating:  88%|████████▊ | 358/407 [00:03<00:00, 79.13it/s]Evaluating:  90%|████████▉ | 366/407 [00:04<00:00, 78.61it/s]Evaluating:  92%|█████████▏| 374/407 [00:04<00:00, 78.22it/s]Evaluating:  94%|█████████▍| 382/407 [00:04<00:00, 78.16it/s]Evaluating:  96%|█████████▌| 390/407 [00:04<00:00, 77.60it/s]Evaluating:  98%|█████████▊| 398/407 [00:04<00:00, 76.48it/s]Evaluating: 100%|█████████▉| 406/407 [00:04<00:00, 76.21it/s]Evaluating: 100%|██████████| 407/407 [00:04<00:00, 88.07it/s]
05/05/2022 20:15:13 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 20:15:13 - INFO - __main__ -     f1 = 0.9395567494963063
05/05/2022 20:15:13 - INFO - __main__ -     loss = 0.086715015073916
05/05/2022 20:15:13 - INFO - __main__ -     precision = 0.9364123159303882
05/05/2022 20:15:13 - INFO - __main__ -     recall = 0.942722371967655
05/05/2022 20:15:13 - INFO - transformers.tokenization_utils -   Model name '/home/hs3228/TextBrewer/output_model/conll_distill/student_layer5' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/home/hs3228/TextBrewer/output_model/conll_distill/student_layer5' is a path or url to a directory containing tokenizer files.
05/05/2022 20:15:13 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/student_layer5/vocab.txt
05/05/2022 20:15:13 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/student_layer5/added_tokens.json
05/05/2022 20:15:13 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/student_layer5/special_tokens_map.json
05/05/2022 20:15:13 - INFO - transformers.tokenization_utils -   loading file /home/hs3228/TextBrewer/output_model/conll_distill/student_layer5/tokenizer_config.json
05/05/2022 20:15:13 - INFO - transformers.configuration_utils -   loading configuration file /home/hs3228/TextBrewer/output_model/conll_distill/student_layer5/config.json
05/05/2022 20:15:13 - INFO - transformers.configuration_utils -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "num_labels": 9,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

05/05/2022 20:15:13 - INFO - transformers.modeling_utils -   loading weights file /home/hs3228/TextBrewer/output_model/conll_distill/student_layer5/pytorch_model.bin
05/05/2022 20:15:14 - INFO - __main__ -   Loading features from cached file /home/hs3228/TextBrewer/data/conll/cached_test_teacher_128
05/05/2022 20:15:15 - INFO - __main__ -   ***** Running evaluation  *****
05/05/2022 20:15:15 - INFO - __main__ -     Num examples = 3453
05/05/2022 20:15:15 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/432 [00:00<?, ?it/s]Evaluating:   3%|▎         | 11/432 [00:00<00:04, 104.00it/s]Evaluating:   5%|▌         | 22/432 [00:00<00:03, 102.96it/s]Evaluating:   8%|▊         | 33/432 [00:00<00:03, 102.09it/s]Evaluating:  10%|█         | 44/432 [00:00<00:03, 101.45it/s]Evaluating:  13%|█▎        | 55/432 [00:00<00:03, 101.51it/s]Evaluating:  15%|█▌        | 66/432 [00:00<00:03, 101.06it/s]Evaluating:  18%|█▊        | 77/432 [00:00<00:03, 100.59it/s]Evaluating:  20%|██        | 88/432 [00:00<00:03, 100.06it/s]Evaluating:  23%|██▎       | 99/432 [00:00<00:03, 98.73it/s] Evaluating:  25%|██▌       | 109/432 [00:01<00:03, 98.47it/s]Evaluating:  28%|██▊       | 119/432 [00:01<00:03, 98.20it/s]Evaluating:  30%|██▉       | 129/432 [00:01<00:03, 97.66it/s]Evaluating:  32%|███▏      | 139/432 [00:01<00:03, 96.87it/s]Evaluating:  34%|███▍      | 149/432 [00:01<00:02, 95.70it/s]Evaluating:  37%|███▋      | 159/432 [00:01<00:02, 94.70it/s]Evaluating:  39%|███▉      | 169/432 [00:01<00:02, 93.57it/s]Evaluating:  41%|████▏     | 179/432 [00:01<00:02, 92.96it/s]Evaluating:  44%|████▍     | 189/432 [00:01<00:02, 91.75it/s]Evaluating:  46%|████▌     | 199/432 [00:02<00:02, 91.22it/s]Evaluating:  48%|████▊     | 209/432 [00:02<00:02, 91.16it/s]Evaluating:  51%|█████     | 219/432 [00:02<00:02, 90.85it/s]Evaluating:  53%|█████▎    | 229/432 [00:02<00:02, 89.87it/s]Evaluating:  55%|█████▌    | 238/432 [00:02<00:02, 88.81it/s]Evaluating:  57%|█████▋    | 247/432 [00:02<00:02, 87.85it/s]Evaluating:  59%|█████▉    | 256/432 [00:02<00:02, 87.04it/s]Evaluating:  61%|██████▏   | 265/432 [00:02<00:01, 85.68it/s]Evaluating:  63%|██████▎   | 274/432 [00:02<00:01, 84.30it/s]Evaluating:  66%|██████▌   | 283/432 [00:03<00:01, 83.57it/s]Evaluating:  68%|██████▊   | 292/432 [00:03<00:01, 82.94it/s]Evaluating:  70%|██████▉   | 301/432 [00:03<00:01, 82.44it/s]Evaluating:  72%|███████▏  | 310/432 [00:03<00:01, 81.96it/s]Evaluating:  74%|███████▍  | 319/432 [00:03<00:01, 81.30it/s]Evaluating:  76%|███████▌  | 328/432 [00:03<00:01, 80.83it/s]Evaluating:  78%|███████▊  | 337/432 [00:03<00:01, 80.40it/s]Evaluating:  80%|████████  | 346/432 [00:03<00:01, 79.88it/s]Evaluating:  82%|████████▏ | 354/432 [00:03<00:00, 79.39it/s]Evaluating:  84%|████████▍ | 362/432 [00:04<00:00, 78.74it/s]Evaluating:  86%|████████▌ | 370/432 [00:04<00:00, 78.45it/s]Evaluating:  88%|████████▊ | 378/432 [00:04<00:00, 78.04it/s]Evaluating:  89%|████████▉ | 386/432 [00:04<00:00, 77.41it/s]Evaluating:  91%|█████████ | 394/432 [00:04<00:00, 77.21it/s]Evaluating:  93%|█████████▎| 402/432 [00:04<00:00, 76.98it/s]Evaluating:  95%|█████████▍| 410/432 [00:04<00:00, 76.57it/s]Evaluating:  97%|█████████▋| 418/432 [00:04<00:00, 75.64it/s]Evaluating:  99%|█████████▊| 426/432 [00:04<00:00, 74.50it/s]Evaluating: 100%|██████████| 432/432 [00:04<00:00, 87.16it/s]
05/05/2022 20:15:20 - INFO - __main__ -   ***** Eval results  *****
05/05/2022 20:15:20 - INFO - __main__ -     f1 = 0.9002022333597117
05/05/2022 20:15:20 - INFO - __main__ -     loss = 0.2127702559990915
05/05/2022 20:15:20 - INFO - __main__ -     precision = 0.8935241752487345
05/05/2022 20:15:20 - INFO - __main__ -     recall = 0.9069808646350106
05/05/2022 20:15:20 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for '.'.
05/05/2022 20:15:20 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for '2'.
05/05/2022 20:15:20 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for '-'.
05/05/2022 20:15:20 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for 'Norm'.
05/05/2022 20:15:20 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for 'Hewitt'.
05/05/2022 20:15:20 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for '('.
05/05/2022 20:15:20 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for 'New'.
05/05/2022 20:15:20 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for 'Zealand'.
05/05/2022 20:15:20 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for ')'.
05/05/2022 20:15:20 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for ','.
05/05/2022 20:15:20 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for '1'.
05/05/2022 20:15:20 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for '-'.
05/05/2022 20:15:20 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for 'Nick'.
05/05/2022 20:15:20 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for 'Popplewell'.
05/05/2022 20:15:20 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for '('.
05/05/2022 20:15:20 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for 'Ireland'.
05/05/2022 20:15:20 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for ')'.
05/05/2022 20:15:20 - WARNING - __main__ -   Maximum sequence length exceeded: No prediction for '.'.
